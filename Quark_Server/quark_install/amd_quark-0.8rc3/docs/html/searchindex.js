Search.setIndex({"alltitles": {"1. Compare Scores from Step 4 and Step 5": [[311, "compare-scores-from-step-4-and-step-5"]], "1. Image Classification Task PTQ/QAT Results": [[318, "image-classification-task-ptq-qat-results"]], "1. Install AMD Quark": [[301, "install-amd-quark"]], "1. Install Quark:": [[337, "install-quark"]], "1. PPL in LM-Evaluation-Harness vs PPL Feature in Table": [[309, "ppl-in-lm-evaluation-harness-vs-ppl-feature-in-table"]], "1. Quantizing to Other Precisions": [[300, "quantizing-to-other-precisions"]], "1. Retrieve dataset from LM-Eval-Harness": [[311, "retrieve-dataset-from-lm-eval-harness"]], "1.1 Quantizing Float32 Models to Int16 or Int32": [[300, "quantizing-float32-models-to-int16-or-int32"]], "1.2 Quantizing Float32 Models to Float16 or BFloat16": [[300, "quantizing-float32-models-to-float16-or-bfloat16"]], "1.3 Quantizing Float32 Models to BFP16": [[300, "quantizing-float32-models-to-bfp16"]], "1.4 Quantizing Float32 Models to Mixed Data Formats": [[300, "quantizing-float32-models-to-mixed-data-formats"]], "2. Export pre-trained Model-Of-Interest to ONNX": [[311, "export-pre-trained-model-of-interest-to-onnx"]], "2. ONNX Exported Models": [[309, "onnx-exported-models"]], "2. Object Detection Task PTQ/QAT Results": [[318, "object-detection-task-ptq-qat-results"]], "2. Quantizing Float16 Models": [[300, "quantizing-float16-models"]], "2. Set the model": [[301, "set-the-model"]], "2. Set the model:": [[337, "set-the-model"]], "3. Retrieve OGA references for pre-trained ONNX Model": [[311, "retrieve-oga-references-for-pre-trained-onnx-model"]], "3. Set the quantization configuration": [[301, "set-the-quantization-configuration"]], "3. Set the quantization configuration:": [[337, "set-the-quantization-configuration"]], "3. Support for VLMs": [[309, "support-for-vlms"]], "4. Do quantization": [[337, "do-quantization"]], "4. Get Baseline Evaluation Scores on pre-trained ONNX Model": [[311, "get-baseline-evaluation-scores-on-pre-trained-onnx-model"]], "4. LLM_Eval_Harness vs LLM_Eval_Haness_Offline": [[309, "llm-eval-harness-vs-llm-eval-haness-offline"]], "4. Set up the calibration data (this is required for weight only and dynamic quantization as well)": [[301, "set-up-the-calibration-data-this-is-required-for-weight-only-and-dynamic-quantization-as-well"]], "5. Apply the quantization": [[301, "apply-the-quantization"]], "5. Evaluate an optimized ONNX Model": [[311, "evaluate-an-optimized-onnx-model"]], "5. Export Evaluation Results": [[309, "export-evaluation-results"]], "AMD Quark APIs for ONNX": [[287, null]], "AMD Quark for ONNX": [[258, null], [289, "amd-quark-for-onnx"]], "AMD Quark for ONNX: Streamlined Quantization for ONNX models": [[249, "amd-quark-for-onnx-streamlined-quantization-for-onnx-models"]], "AMD Quark for PyTorch": [[303, null]], "AMD Quark for PyTorch: Flexible and Efficient Quantization for PyTorch Models": [[249, "amd-quark-for-pytorch-flexible-and-efficient-quantization-for-pytorch-models"]], "AMD Quark for Pytorch": [[330, "amd-quark-for-pytorch"]], "Accelerate with GPUs": [[284, null]], "Accessing ONNX Examples": [[288, null]], "Accessing PyTorch Examples": [[329, null]], "Accuracy Improvement Algorithms": [[256, null]], "Activation/weight smoothing (SmoothQuant)": [[336, null]], "AdaQuant": [[252, "adaquant"], [252, "id2"]], "AdaRound": [[252, "adaround"], [252, "id1"]], "Adding Calibration Datasets": [[259, null], [304, null]], "Apply Quantization Algorithms": [[335, "apply-quantization-algorithms"]], "Arguments": [[252, "arguments"], [253, "arguments"], [254, "arguments"], [255, "arguments"]], "Attributes": [[41, "attributes"], [228, "attributes"]], "Auto Search for Best Practice of RyzenAI ONNX Model Quantization": [[279, null]], "Auto Search for RyzenAI ONNX Model Quantization \u2013 yolo_nas_s": [[278, null]], "Auto search for RyzenAI quantization": [[283, "auto-search-for-ryzenai-quantization"]], "Automatic Mixed Precision based on Sensitivity Analysis": [[297, "automatic-mixed-precision-based-on-sensitivity-analysis"]], "Automatic Search for Model Quantization": [[298, null]], "BF16 quantization in AMD Quark for ONNX": [[293, "bf16-quantization-in-amd-quark-for-onnx"]], "BFP16 (Block floating point) Quantization": [[294, null], [337, null]], "BFP16 Models Inference": [[284, "bfp16-models-inference"]], "BFP16 Quantization": [[265, "bfp16-quantization"]], "BFP16 Quantization with ADAQUANT": [[265, "bfp16-quantization-with-adaquant"]], "Basic Example": [[258, "basic-example"], [303, "basic-example"]], "Basic Usage": [[248, null]], "Benchmark": [[308, "benchmark"]], "Benefits of AdaRound and AdaQuant": [[252, "benefits-of-adaround-and-adaquant"]], "Benefits of BFP16 Quantization": [[294, "benefits-of-bfp16-quantization"]], "Benefits of Mixed Precision Quantization": [[297, "benefits-of-mixed-precision-quantization"]], "Best Practice for Ryzen AI in AMD Quark ONNX": [[291, null]], "Best Practice for Ryzen AI in Quark ONNX": [[283, null], [285, null], [286, null]], "Best Practices for Post-Training Quantization (PTQ)": [[335, null]], "Block Floating Point (BFP) Example": [[265, null]], "Brevitas Integration": [[307, null], [307, "id1"]], "Bridge from Quark to llama.cpp": [[319, null]], "Bug Fixes and Enhancements": [[341, "bug-fixes-and-enhancements"]], "C++ Compilation Issues": [[330, "c-compilation-issues"]], "Calibration": [[284, "calibration"]], "Calibration Data Path for AMD Quark Quantizer": [[259, "calibration-data-path-for-amd-quark-quantizer"]], "Calibration Methods": [[260, null], [305, null]], "Calibration and Export": [[308, "calibration-and-export"]], "Call the Auto Search Process": [[278, "call-the-auto-search-process"], [279, "call-the-auto-search-process"]], "Class DataReader for AMD Quark Quantizer": [[259, "class-datareader-for-amd-quark-quantizer"]], "Classes": [[1, "classes"], [3, "classes"], [4, "classes"], [5, "classes"], [6, "classes"], [7, "classes"], [8, "classes"], [13, "classes"], [14, "classes"], [15, "classes"], [16, "classes"], [17, "classes"], [21, "classes"], [22, "classes"], [25, "classes"], [26, "classes"], [27, "classes"], [31, "classes"], [32, "classes"], [33, "classes"], [34, "classes"], [39, "classes"], [50, "classes"], [51, "classes"], [53, "classes"], [54, "classes"], [55, "classes"], [56, "classes"], [57, "classes"], [59, "classes"], [63, "classes"], [66, "classes"], [71, "classes"], [76, "classes"], [111, "classes"], [118, "classes"], [121, "classes"], [122, "classes"], [123, "classes"], [127, "classes"], [128, "classes"], [130, "classes"], [131, "classes"], [132, "classes"], [134, "classes"], [135, "classes"], [136, "classes"], [139, "classes"], [140, "classes"], [143, "classes"], [144, "classes"], [150, "classes"], [151, "classes"], [156, "classes"], [159, "classes"], [160, "classes"], [162, "classes"], [178, "classes"], [181, "classes"], [182, "classes"], [183, "classes"], [186, "classes"], [188, "classes"], [193, "classes"], [194, "classes"], [195, "classes"], [198, "classes"], [199, "classes"], [202, "classes"], [206, "classes"], [209, "classes"], [213, "classes"], [215, "classes"], [217, "classes"], [222, "classes"], [229, "classes"], [232, "classes"], [234, "classes"], [235, "classes"], [236, "classes"], [237, "classes"], [240, "classes"], [241, "classes"], [242, "classes"], [243, "classes"]], "Components": [[298, "components"]], "Conclusion": [[298, "conclusion"], [301, "conclusion"], [307, "conclusion"]], "Configuring ONNX Quantization": [[299, null]], "Configuring PyTorch Quantization": [[340, null]], "Considerations for Quantization": [[251, "considerations-for-quantization"]], "Context: Uniform Integer Quantization": [[302, "context-uniform-integer-quantization"]], "Convert a A8W8 NPU Model to a A8W8 CPU Model": [[292, "convert-a-a8w8-npu-model-to-a-a8w8-cpu-model"]], "Convert a Float16 Model to a BFP16 Model": [[292, "convert-a-float16-model-to-a-bfp16-model"]], "Convert a Float16 Model to a BFloat16 Model": [[292, "convert-a-float16-model-to-a-bfloat16-model"]], "Convert a Float16 Model to a Float32 Model": [[292, "convert-a-float16-model-to-a-float32-model"]], "Convert a Float32 Model to a BFP16 Model": [[292, "convert-a-float32-model-to-a-bfp16-model"]], "Convert a Float32 Model to a BFloat16 Model": [[292, "convert-a-float32-model-to-a-bfloat16-model"]], "Convert a NCHW input Model to a NHWC Model": [[292, "convert-a-nchw-input-model-to-a-nhwc-model"]], "Convert a U16U8 Quantized Model to a U8U8 Model": [[292, "convert-a-u16u8-quantized-model-to-a-u8u8-model"]], "Customized Configurations": [[299, "customized-configurations"]], "Dataloader with Dataset as torch.Tensor": [[304, "dataloader-with-dataset-as-torch-tensor"]], "Dataloader with Dict[str, torch.Tensor]": [[304, "dataloader-with-dict-str-torch-tensor"]], "Dataloader with List[Dict[str, torch.Tensor]] or List[torch.Tensor]": [[304, "dataloader-with-list-dict-str-torch-tensor-or-list-torch-tensor"]], "Dataset Files": [[308, "dataset-files"]], "Debugging quantization degradation in AMD Quark": [[306, null]], "Detailed Code": [[264, "detailed-code"]], "Diffusion Model Quantization using Quark": [[308, null]], "Dumping the Simulation Results": [[290, "dumping-the-simulation-results"]], "Dynamic Quantization": [[288, null]], "Dynamic Quantization for Llama-2-7b": [[271, null]], "Dynamic Quantization for OPT-125M": [[272, null]], "Entropy Calibration Method": [[260, "entropy-calibration-method"]], "Environment Issues": [[330, "environment-issues"]], "Environment Setup": [[284, "environment-setup"]], "Evaluate Accuracy Between Two Image Folders": [[292, "evaluate-accuracy-between-two-image-folders"]], "Evaluating the Quantized Model": [[290, "evaluating-the-quantized-model"]], "Evaluation": [[265, "evaluation"], [266, "evaluation"], [267, "evaluation"], [268, "evaluation"], [269, "evaluation"], [270, "evaluation"], [271, "evaluation"], [272, "evaluation"], [273, "evaluation"], [274, "evaluation"], [275, "evaluation"], [276, "evaluation"], [277, "evaluation"], [280, "evaluation"], [281, "evaluation"], [282, "evaluation"]], "Example": [[253, "example"], [254, "example"], [255, "example"], [294, "example"], [296, "example"]], "Example 1": [[317, "example-1"]], "Example 2": [[317, "example-2"]], "Example 3": [[317, "example-3"]], "Example Code:": [[259, "example-code"], [259, "id1"]], "Example of GGUF Exporting": [[321, "example-of-gguf-exporting"]], "Example of HF Format Exporting": [[322, "example-of-hf-format-exporting"]], "Example of HF Format Importing": [[322, "example-of-hf-format-importing"]], "Example of Loading in Eager Mode": [[334, "example-of-loading-in-eager-mode"]], "Example of Loading in FX-graph Mode": [[334, "example-of-loading-in-fx-graph-mode"]], "Example of Onnx Exporting": [[324, "example-of-onnx-exporting"]], "Example of Quark Format Exporting": [[325, "example-of-quark-format-exporting"]], "Example of Quark Format Importing": [[325, "example-of-quark-format-importing"]], "Example of Saving in Eager Mode": [[334, "example-of-saving-in-eager-mode"]], "Example of Saving in FX-graph Mode": [[334, "example-of-saving-in-fx-graph-mode"]], "Examples": [[252, "examples"], [295, "examples"]], "Exclude Outlier Layers": [[335, "exclude-outlier-layers"]], "Expanding with more models": [[339, "expanding-with-more-models"]], "Experiment Results": [[318, "experiment-results"]], "Experiment results": [[319, "id4"]], "Experiments": [[319, "experiments"]], "Exporting PyTorch Models to ONNX": [[290, "exporting-pytorch-models-to-onnx"]], "Exporting Quantized Models": [[320, null]], "Exporting Using ONNX Runtime Gen AI Model Builder": [[323, null], [323, "id1"]], "Exporting to HuggingFace Format": [[322, "exporting-to-huggingface-format"]], "Extension Feature Design": [[307, "extension-feature-design"]], "Extensions for PyTorch": [[326, null]], "Fake Quantization": [[251, "fake-quantization"]], "Fast Finetune": [[252, "fast-finetune"], [284, "fast-finetune"]], "Fine-Grained User Guide": [[318, "fine-grained-user-guide"]], "Flow Diagram": [[298, "flow-diagram"]], "For Multi-Input Models": [[264, "for-multi-input-models"]], "For Multi-Input Models:": [[259, "for-multi-input-models"]], "For Single-Input Models": [[264, "for-single-input-models"]], "For Single-Input Models:": [[259, "for-single-input-models"]], "Frequently Asked Questions (FAQ)": [[289, null], [330, null]], "Full List of Quantization Configuration Features": [[257, null]], "Functions": [[1, "functions"], [3, "functions"], [5, "functions"], [9, "functions"], [11, "functions"], [18, "functions"], [20, "functions"], [22, "functions"], [32, "functions"], [36, "functions"], [41, "functions"], [53, "functions"], [55, "functions"], [59, "functions"], [61, "functions"], [72, "functions"], [74, "functions"], [78, "functions"], [86, "functions"], [91, "functions"], [92, "functions"], [93, "functions"], [99, "functions"], [106, "functions"], [107, "functions"], [108, "functions"], [110, "functions"], [114, "functions"], [119, "functions"], [120, "functions"], [138, "functions"], [140, "functions"], [141, "functions"], [144, "functions"], [147, "functions"], [150, "functions"], [154, "functions"], [169, "functions"], [180, "functions"], [188, "functions"], [190, "functions"], [198, "functions"], [199, "functions"], [203, "functions"], [205, "functions"], [211, "functions"], [212, "functions"], [218, "functions"], [219, "functions"], [220, "functions"], [221, "functions"], [225, "functions"], [226, "functions"], [228, "functions"], [229, "functions"], [230, "functions"], [235, "functions"], [238, "functions"], [244, "functions"]], "GGUF Exporting": [[321, null]], "Get example code and script": [[318, "get-example-code-and-script"]], "Getting Started": [[309, "getting-started"]], "Guidelines": [[277, "guidelines"]], "Hardware Mapping": [[302, "hardware-mapping"]], "How BFP16 works in Quark": [[337, "how-bfp16-works-in-quark"]], "How Does It Work": [[319, "how-does-it-work"]], "How Does Quark Do Quantization": [[319, "how-does-quark-do-quantization"]], "How are These Two-Level Scales Obtained?": [[302, "how-are-these-two-level-scales-obtained"]], "How are the scales calculated?": [[301, "how-are-the-scales-calculated"]], "How are the scales used?": [[301, "how-are-the-scales-used"]], "How does SmoothQuant work?": [[336, "how-does-smoothquant-work"]], "How is the tensor turned into blocks?": [[301, "how-is-the-tensor-turned-into-blocks"]], "How to Enable AdaRound / AdaQuant in AMD Quark?": [[252, "how-to-enable-adaround-adaquant-in-amd-quark"]], "How to Enable MX Quantization in AMD Quark for ONNX?": [[296, "how-to-enable-mx-quantization-in-amd-quark-for-onnx"]], "How to Enable Mixed Precision in AMD Quark for ONNX?": [[297, "how-to-enable-mixed-precision-in-amd-quark-for-onnx"]], "How to Further Improve the Accuracy for BF16 Quantization?": [[293, "how-to-further-improve-the-accuracy-for-bf16-quantization"]], "How to Further Improve the Accuracy of a MX Quantized Model?": [[296, "how-to-further-improve-the-accuracy-of-a-mx-quantized-model"]], "How to Further Improve the Accuracy of a MX9 Quantized Model?": [[295, "how-to-further-improve-the-accuracy-of-a-mx9-quantized-model"]], "How to Use GGUF Export in Quark": [[319, "how-to-use-gguf-export-in-quark"]], "How to enable BFP16 quantization in AMD Quark for ONNX?": [[294, "how-to-enable-bfp16-quantization-in-amd-quark-for-onnx"]], "How to enable MX9 quantization in AMD Quark for ONNX?": [[295, "how-to-enable-mx9-quantization-in-amd-quark-for-onnx"]], "How to further improve the accuracy of a BFP16 quantized model in AMD Quark for ONNX?": [[294, "how-to-further-improve-the-accuracy-of-a-bfp16-quantized-model-in-amd-quark-for-onnx"]], "How to use BFP16 in Quark": [[337, "how-to-use-bfp16-in-quark"]], "How to use MX in AMD Quark": [[301, "how-to-use-mx-in-amd-quark"]], "HuggingFace Format": [[322, null]], "Image Classification": [[288, null]], "Important Details": [[309, "important-details"]], "Importing HuggingFace Format": [[322, "importing-huggingface-format"]], "Improving Model Accuracy": [[288, null]], "Inference": [[286, "inference"]], "Install from ZIP": [[250, "install-from-zip"]], "Installation": [[250, "installation"], [315, "installation"]], "Installation Guide": [[250, null]], "Installation Verification": [[250, "installation-verification"]], "Integer Quantization": [[251, "integer-quantization"]], "Integration with AMD Pytorch-light (APL)": [[317, null]], "Introduction": [[277, "introduction"], [293, null], [295, null], [301, "introduction"], [317, "introduction"], [319, "introduction"], [337, "introduction"], [339, "introduction"]], "Key Concepts": [[294, "key-concepts"]], "Key Features": [[249, "key-features"], [249, "id1"]], "LM-Evaluation Harness (Offline)": [[311, null]], "LM-Evaluation Harness Evaluations": [[310, null]], "LM-Evaluation-Harness on ONNX Models": [[310, "lm-evaluation-harness-on-onnx-models"]], "LM-Evaluation-Harness on Torch Models": [[310, "lm-evaluation-harness-on-torch-models"]], "Language Model Evaluations in Quark": [[309, null]], "Language Model Optimization": [[327, null]], "Language Model Post Training Quantization (PTQ) Using Quark": [[315, null]], "Language Model QAT Using Quark": [[316, null]], "Language Models": [[288, null]], "License": [[269, "license"], [275, "license"], [277, "license"], [293, "license"], [294, "license"], [295, "license"], [296, "license"], [297, "license"], [298, "license"]], "Load SafeTensor and Run with a prompt": [[308, "load-safetensor-and-run-with-a-prompt"]], "Load SafeTensor and Test": [[308, "load-safetensor-and-test"]], "Load and Run": [[308, "load-and-run"]], "Load and Test": [[308, "load-and-test"]], "Loading": [[334, "loading"]], "MSE Calibration Method": [[260, "mse-calibration-method"]], "Microscaling (MX)": [[296, null]], "Microscaling (MX) Example": [[266, null]], "MinMax Calibration Method": [[260, "minmax-calibration-method"]], "Mixed Precision": [[297, null]], "Mixed Precision Quantization in AMD Quark for ONNX": [[297, "mixed-precision-quantization-in-amd-quark-for-onnx"]], "Model Issues": [[289, "model-issues"]], "Model Preparation": [[323, "model-preparation"]], "Model Quantization": [[251, "model-quantization"], [274, "model-quantization"]], "Model Quantization Preparation": [[279, "model-quantization-preparation"]], "ModelQuantizer": [[307, "modelquantizer"]], "Module Contents": [[1, "module-contents"], [3, "module-contents"], [4, "module-contents"], [5, "module-contents"], [6, "module-contents"], [7, "module-contents"], [8, "module-contents"], [9, "module-contents"], [11, "module-contents"], [13, "module-contents"], [14, "module-contents"], [15, "module-contents"], [16, "module-contents"], [17, "module-contents"], [18, "module-contents"], [20, "module-contents"], [21, "module-contents"], [22, "module-contents"], [25, "module-contents"], [26, "module-contents"], [27, "module-contents"], [31, "module-contents"], [32, "module-contents"], [33, "module-contents"], [34, "module-contents"], [36, "module-contents"], [39, "module-contents"], [50, "module-contents"], [51, "module-contents"], [53, "module-contents"], [54, "module-contents"], [55, "module-contents"], [56, "module-contents"], [57, "module-contents"], [61, "module-contents"], [63, "module-contents"], [66, "module-contents"], [71, "module-contents"], [72, "module-contents"], [74, "module-contents"], [76, "module-contents"], [78, "module-contents"], [86, "module-contents"], [91, "module-contents"], [92, "module-contents"], [93, "module-contents"], [99, "module-contents"], [106, "module-contents"], [107, "module-contents"], [108, "module-contents"], [110, "module-contents"], [111, "module-contents"], [114, "module-contents"], [118, "module-contents"], [119, "module-contents"], [120, "module-contents"], [121, "module-contents"], [122, "module-contents"], [127, "module-contents"], [128, "module-contents"], [131, "module-contents"], [135, "module-contents"], [136, "module-contents"], [138, "module-contents"], [139, "module-contents"], [140, "module-contents"], [141, "module-contents"], [143, "module-contents"], [144, "module-contents"], [147, "module-contents"], [150, "module-contents"], [151, "module-contents"], [154, "module-contents"], [156, "module-contents"], [159, "module-contents"], [162, "module-contents"], [169, "module-contents"], [178, "module-contents"], [180, "module-contents"], [181, "module-contents"], [182, "module-contents"], [183, "module-contents"], [186, "module-contents"], [190, "module-contents"], [193, "module-contents"], [194, "module-contents"], [198, "module-contents"], [199, "module-contents"], [202, "module-contents"], [203, "module-contents"], [205, "module-contents"], [206, "module-contents"], [209, "module-contents"], [211, "module-contents"], [212, "module-contents"], [213, "module-contents"], [215, "module-contents"], [217, "module-contents"], [218, "module-contents"], [219, "module-contents"], [220, "module-contents"], [221, "module-contents"], [222, "module-contents"], [225, "module-contents"], [226, "module-contents"], [228, "module-contents"], [230, "module-contents"], [234, "module-contents"], [235, "module-contents"], [236, "module-contents"], [237, "module-contents"], [238, "module-contents"], [240, "module-contents"], [241, "module-contents"], [242, "module-contents"], [243, "module-contents"], [244, "module-contents"]], "More Quantization Default Configurations": [[299, "more-quantization-default-configurations"]], "New Features": [[341, "new-features"]], "New Features (Version 0.1.0)": [[341, "new-features-version-0-1-0"]], "New Features (Version 0.2.0)": [[341, "new-features-version-0-2-0"]], "New Features (Version 0.5.0)": [[341, "new-features-version-0-5-0"]], "New Features (Version 0.5.1)": [[341, "new-features-version-0-5-1"]], "New Features (Version 0.6.0)": [[341, "new-features-version-0-6-0"]], "New Features (Version 0.8)": [[341, "new-features-version-0-8"]], "NonOverflow Calibration Method": [[260, "nonoverflow-calibration-method"]], "Notes": [[302, "notes"]], "ONNX Examples in AMD Quark for This Release": [[288, "onnx-examples-in-amd-quark-for-this-release"]], "ONNX Exporting": [[324, null]], "ONNX Runtime Gen AI (OGA) Installation": [[323, "onnx-runtime-gen-ai-oga-installation"]], "Older Versions": [[250, "older-versions"]], "Optional Utilities": [[290, null]], "Other Arguments": [[310, "other-arguments"], [312, "other-arguments"], [313, "other-arguments"]], "Overview": [[298, "overview"], [307, "overview"]], "PPL on Torch Models": [[312, "ppl-on-torch-models"]], "PTQ": [[318, "ptq"]], "Package Contents": [[41, "package-contents"], [59, "package-contents"], [123, "package-contents"], [130, "package-contents"], [132, "package-contents"], [134, "package-contents"], [160, "package-contents"], [188, "package-contents"], [195, "package-contents"], [229, "package-contents"], [232, "package-contents"]], "Parameter Explanation": [[340, "id1"], [340, "id2"], [340, "id3"], [340, "id4"], [340, "id5"], [340, "id6"]], "Percentile Calibration Method": [[260, "percentile-calibration-method"]], "Perplexity Evaluations": [[312, null]], "Perplexity on ONNX Models": [[312, "perplexity-on-onnx-models"]], "Pip Requirements": [[268, "pip-requirements"], [270, "pip-requirements"], [273, "pip-requirements"], [280, "pip-requirements"], [283, "pip-requirements"], [285, "pip-requirements"], [286, "pip-requirements"], [291, "pip-requirements"]], "Pip requirements": [[265, "pip-requirements"], [266, "pip-requirements"], [267, "pip-requirements"], [269, "pip-requirements"], [271, "pip-requirements"], [272, "pip-requirements"], [274, "pip-requirements"], [275, "pip-requirements"], [276, "pip-requirements"], [277, "pip-requirements"], [278, "pip-requirements"], [281, "pip-requirements"], [282, "pip-requirements"]], "Pre-processing on the Float Model": [[290, "pre-processing-on-the-float-model"]], "Preparation": [[314, "preparation"], [315, "preparation"], [316, "preparation"], [323, "preparation"]], "Prepare Calibration Data": [[283, "prepare-calibration-data"], [285, "prepare-calibration-data"], [286, "prepare-calibration-data"], [291, "prepare-calibration-data"]], "Prepare Data": [[268, "prepare-data"], [270, "prepare-data"]], "Prepare Model": [[268, "prepare-model"], [270, "prepare-model"], [273, "prepare-model"], [280, "prepare-model"]], "Prepare data": [[265, "prepare-data"], [266, "prepare-data"], [267, "prepare-data"], [269, "prepare-data"], [276, "prepare-data"]], "Prepare data and model": [[274, "prepare-data-and-model"]], "Prepare model": [[265, "prepare-model"], [266, "prepare-model"], [267, "prepare-model"], [269, "prepare-model"], [271, "prepare-model"], [272, "prepare-model"], [275, "prepare-model"], [276, "prepare-model"], [277, "prepare-model"], [278, "prepare-model"], [281, "prepare-model"], [282, "prepare-model"], [283, "prepare-model"], [285, "prepare-model"], [286, "prepare-model"], [291, "prepare-model"]], "Prepare model from Torch to ONNX (Optional)": [[286, "prepare-model-from-torch-to-onnx-optional"]], "Prerequisites": [[250, "prerequisites"]], "Print Names and Quantity of A16W8 and A8W8 Conv for Mixed-Precision Models": [[292, "print-names-and-quantity-of-a16w8-and-a8w8-conv-for-mixed-precision-models"]], "Pruning": [[314, null]], "Pruning Scripts": [[314, "pruning-scripts"]], "PyTorch Examples in Quark for This Release": [[329, null]], "QAT": [[318, "qat"]], "QAT Scripts": [[316, "qat-scripts"]], "QuaRot": [[254, null]], "Quantizating Llama-2-7b model using MatMulNBits quantizer": [[281, null]], "Quantizating a model with GPTQ": [[273, null]], "Quantization": [[271, "quantization"], [272, "quantization"], [275, "quantization"], [281, "quantization"], [282, "quantization"], [285, "quantization"], [286, "quantization"], [291, "quantization"]], "Quantization & Export Scripts": [[308, "quantization-export-scripts"], [315, "quantization-export-scripts"]], "Quantization Issues": [[289, "quantization-issues"]], "Quantization Preparation": [[278, "quantization-preparation"]], "Quantization Results": [[265, "quantization-results"]], "Quantization Schemes": [[261, null], [331, null]], "Quantization Strategies": [[262, null], [332, null]], "Quantization Symmetry": [[263, null], [333, null]], "Quantization Using AdaQuant and AdaRound": [[252, null]], "Quantization With CLE": [[270, "quantization-with-cle"]], "Quantization With GPTQ": [[273, "quantization-with-gptq"]], "Quantization With Mixed Precision": [[276, "quantization-with-mixed-precision"]], "Quantization With Smooth Quant": [[280, "quantization-with-smooth-quant"]], "Quantization Without CLE": [[270, "quantization-without-cle"]], "Quantization Without GPTQ": [[273, "quantization-without-gptq"]], "Quantization Without Mixed Precision": [[276, "quantization-without-mixed-precision"]], "Quantization Without Smooth Quant": [[280, "quantization-without-smooth-quant"]], "Quantization using Mixed Precision": [[276, null]], "Quantization using SmoothQuant": [[280, null]], "Quantization with ADAQUANT": [[267, "quantization-with-adaquant"]], "Quantization with ADAROUND": [[268, "quantization-with-adaround"]], "Quantization with AMD Quark": [[251, null]], "Quantization with HQQ": [[281, "quantization-with-hqq"]], "Quantization with MX Formats": [[266, "quantization-with-mx-formats"]], "Quantization with QuaRot": [[277, "quantization-with-quarot"]], "Quantization with QuaRot and SmoothQuant": [[277, "quantization-with-quarot-and-smoothquant"]], "Quantization with auto_search": [[269, "quantization-with-auto-search"]], "Quantization without ADAQUANT": [[267, "quantization-without-adaquant"]], "Quantization without ADAROUND": [[268, "quantization-without-adaround"]], "Quantization without QuaRot": [[277, "quantization-without-quarot"]], "QuantizationConfig": [[307, "quantizationconfig"]], "Quantize Controlnet and Export SafeTensors (unet-only)": [[308, "quantize-controlnet-and-export-safetensors-unet-only"]], "Quantize Diffusion and Export ONNX (entire pipeline)": [[308, "quantize-diffusion-and-export-onnx-entire-pipeline"]], "Quantize a ONNX Model Using Random Input": [[292, "quantize-a-onnx-model-using-random-input"]], "Quantizing Using CrossLayerEqualization (CLE)": [[253, null]], "Quantizing a ResNet50-v1-12 Model": [[274, null]], "Quantizing an OPT-125M Model": [[275, null]], "Quantizing with Rotation and SmoothQuant": [[339, null]], "Quark APIs for PyTorch": [[328, null]], "Quark Format": [[325, null]], "Quark Format Exporting": [[325, "quark-format-exporting"]], "Quark Format Importing": [[325, "quark-format-importing"]], "Quark ONNX Example for CrossLayerEqualization (CLE)": [[270, null]], "Quark ONNX Quantization Example": [[267, null], [268, null], [269, null], [277, null], [282, null]], "Quark UINT4 Quantization with AWQ": [[323, "quark-uint4-quantization-with-awq"]], "Quick Start": [[318, "quick-start"]], "Recipe 1: Evaluation of Llama Float16 Model without Quantization": [[315, "recipe-1-evaluation-of-llama-float16-model-without-quantization"]], "Recipe 1: Evaluation of Llama2 Float16 Model without Pruning": [[314, "recipe-1-evaluation-of-llama2-float16-model-without-pruning"]], "Recipe 1: Evaluation of Original LLM": [[316, "recipe-1-evaluation-of-original-llm"]], "Recipe 2: FP8 (OCP fp8_e4m3) Quantization & Json_SafeTensors_Export with KV Cache": [[315, "recipe-2-fp8-ocp-fp8-e4m3-quantization-json-safetensors-export-with-kv-cache"]], "Recipe 2: Pruning Model and Saved to Safetensors": [[314, "recipe-2-pruning-model-and-saved-to-safetensors"]], "Recipe 2: QAT Finetuning and Export to Safetensors": [[316, "recipe-2-qat-finetuning-and-export-to-safetensors"]], "Recipe 3: INT Weight-Only Quantization & Json_SafeTensors_Export with AWQ": [[315, "recipe-3-int-weight-only-quantization-json-safetensors-export-with-awq"]], "Recipe 3: Reload and Evaluate Finetuned Model": [[316, "recipe-3-reload-and-evaluate-finetuned-model"]], "Recipe 4: INT Static Quantization & Json_SafeTensors_Export (on CPU)": [[315, "recipe-4-int-static-quantization-json-safetensors-export-on-cpu"]], "Recipe 5: Quantization & GGUF_Export with AWQ (W_uint4 A_float16 per_group asymmetric)": [[315, "recipe-5-quantization-gguf-export-with-awq-w-uint4-a-float16-per-group-asymmetric"]], "Recipe 6: MX Quantization": [[315, "recipe-6-mx-quantization"]], "Recipe 7: BFP16 Quantization": [[315, "recipe-7-bfp16-quantization"]], "Recipe 8: MX6 Quantization": [[315, "recipe-8-mx6-quantization"]], "Recipes": [[310, "recipes"], [312, "recipes"], [313, "recipes"]], "Release 0.7": [[341, "release-0-7"]], "Release Notes": [[341, null]], "Replace inf and -inf Values in ONNX Model Weights": [[292, "replace-inf-and-inf-values-in-onnx-model-weights"]], "Results": [[339, "results"]], "Rotation-based quantization with QuaRot": [[338, null]], "Rouge & Meteor Evaluations": [[313, null]], "Rouge/Meteor on ONNX Models": [[313, "rouge-meteor-on-onnx-models"]], "Rouge/Meteor on Torch Models": [[313, "rouge-meteor-on-torch-models"]], "Run Diffusion Model Without Quantization": [[308, "run-diffusion-model-without-quantization"]], "Ryzen AI Quantization": [[288, null]], "Save & Load Quantized Models": [[334, null]], "Saving": [[334, "saving"]], "Search Config Settings": [[278, "search-config-settings"], [279, "search-config-settings"]], "SmoothQuant (SQ)": [[255, null]], "Some of GGUF dtypes and their corresponding quant schemes": [[319, "id3"]], "Step 1: Configuring QuantizationSpec for torch.Tensors": [[340, "step-1-configuring-quantizationspec-for-torch-tensors"]], "Step 1: Quantize Your Model": [[319, "step-1-quantize-your-model"]], "Step 2: Establishing QuantizationConfig for nn.Module": [[340, "step-2-establishing-quantizationconfig-for-nn-module"]], "Step 2: Export to GGUF": [[319, "step-2-export-to-gguf"]], "Step 3: Run with llama.cpp": [[319, "step-3-run-with-llama-cpp"]], "Step 3: [Optional] Setting AlgoConfig for the model": [[340, "step-3-optional-setting-algoconfig-for-the-model"]], "Step 4: Setting up the overall Config for the model.": [[340, "step-4-setting-up-the-overall-config-for-the-model"]], "Step-by-Step Integration": [[307, "step-by-step-integration"]], "Step-by-Step Process": [[311, "step-by-step-process"]], "Submodules": [[30, "submodules"], [35, "submodules"], [52, "submodules"], [59, "submodules"], [60, "submodules"], [100, "submodules"], [113, "submodules"], [133, "submodules"], [152, "submodules"], [160, "submodules"], [184, "submodules"], [195, "submodules"], [201, "submodules"], [229, "submodules"]], "Summary Table": [[300, "summary-table"], [300, "summary-table-1"]], "Supported Data Types": [[300, "supported-data-types"]], "Supported Data and Op Types": [[300, null]], "Supported Features": [[258, "supported-features"], [303, "supported-features"]], "Supported Models": [[314, "supported-models"], [315, "supported-models"], [315, "id1"], [316, "supported-models"]], "Supported Op Type": [[300, "supported-op-type"]], "Supported Tasks": [[311, "supported-tasks"]], "TQT": [[318, "tqt"]], "Third-party Dependencies": [[308, "third-party-dependencies"]], "Tips:": [[290, "tips"]], "Tools": [[292, null]], "Try Different Quantization Schemes": [[335, "try-different-quantization-schemes"]], "Try QAT": [[335, "try-qat"]], "Tutorial: Generating AWQ Configuration Automatically (Experimental)": [[315, "tutorial-generating-awq-configuration-automatically-experimental"]], "Tutorial: Running a Model Not on the Supported List": [[315, "tutorial-running-a-model-not-on-the-supported-list"]], "Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents)": [[302, null]], "Two-level Quantization: MX6 and MX9 Data Types": [[302, "two-level-quantization-mx6-and-mx9-data-types"]], "Upgrades of AdaRound / AdaQuant in AMD Quark for ONNX": [[252, "upgrades-of-adaround-adaquant-in-amd-quark-for-onnx"]], "Usage": [[298, "usage"], [315, "usage"]], "User Guide": [[309, "user-guide"]], "Using MX (Microscaling)": [[301, null]], "Using ONNX Model Inference and Saving Input Data in NPY Format": [[264, null]], "Using Random Data for AMD Quark Quantizer": [[259, "using-random-data-for-amd-quark-quantizer"]], "Using SmoothQuant in quark.torch": [[336, "using-smoothquant-in-quark-torch"]], "Vision Model Quantization Using Quark FX Graph Mode": [[318, null]], "Weights-Only Quantization": [[288, null]], "Welcome to AMD Quark Documentation!": [[249, null]], "What Does This Mean?": [[251, "what-does-this-mean"]], "What Happens Internally in Quark When We Quantize Something?": [[251, "what-happens-internally-in-quark-when-we-quantize-something"]], "What Is GGUF": [[319, "what-is-gguf"]], "What Is Quantization?": [[251, "what-is-quantization"]], "What is MX Quantization?": [[296, "what-is-mx-quantization"]], "What is Microexponents Quantization?": [[295, "what-is-microexponents-quantization"]], "What is Mixed Precision Quantization?": [[297, "what-is-mixed-precision-quantization"]], "When Are Values Actually Converted into Their Quantized Data Types?": [[251, "when-are-values-actually-converted-into-their-quantized-data-types"]], "quark": [[0, null]], "quark.onnx": [[35, null]], "quark.onnx.auto_search": [[1, null]], "quark.onnx.bias_correction": [[2, null]], "quark.onnx.calibrate": [[3, null]], "quark.onnx.cpu_quantizer": [[4, null]], "quark.onnx.equalization": [[5, null]], "quark.onnx.finetuning": [[19, null]], "quark.onnx.finetuning.create_torch": [[12, null]], "quark.onnx.finetuning.create_torch.base_fn_quantizers": [[6, null]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers": [[7, null]], "quark.onnx.finetuning.create_torch.create_model": [[8, null]], "quark.onnx.finetuning.create_torch.create_model_ops": [[9, null]], "quark.onnx.finetuning.create_torch.create_model_test": [[10, null]], "quark.onnx.finetuning.create_torch.create_model_utils": [[11, null]], "quark.onnx.finetuning.create_torch.quant_base_ops": [[13, null]], "quark.onnx.finetuning.create_torch.quant_conv_ops": [[14, null]], "quark.onnx.finetuning.create_torch.quant_gemm_ops": [[15, null]], "quark.onnx.finetuning.create_torch.quant_matmul_ops": [[16, null]], "quark.onnx.finetuning.create_torch.quant_norm_ops": [[17, null]], "quark.onnx.finetuning.fast_finetune": [[18, null]], "quark.onnx.finetuning.onnx_evaluate": [[20, null]], "quark.onnx.finetuning.onnx_subgraph": [[21, null]], "quark.onnx.finetuning.torch_utils": [[22, null]], "quark.onnx.finetuning.torch_utils_test": [[23, null]], "quark.onnx.finetuning.train_torch": [[24, null]], "quark.onnx.finetuning.train_torch.train_model": [[25, null]], "quark.onnx.finetuning.train_torch.train_model_loss": [[26, null]], "quark.onnx.finetuning.train_torch.train_model_param": [[27, null]], "quark.onnx.gptq": [[29, null]], "quark.onnx.gptq.gptq": [[28, null]], "quark.onnx.graph_transformations": [[30, null]], "quark.onnx.graph_transformations.model_transformer": [[31, null]], "quark.onnx.graph_transformations.model_transformer_test": [[32, null]], "quark.onnx.graph_transformations.transforms": [[33, null]], "quark.onnx.graph_transformations.transforms_pipeline": [[34, null]], "quark.onnx.mprecision": [[37, null]], "quark.onnx.mprecision.auto_mixprecision": [[36, null]], "quark.onnx.mprecision.mixed_bfp": [[38, null]], "quark.onnx.onnx_quantizer": [[39, null]], "quark.onnx.operators": [[42, null]], "quark.onnx.operators.custom_ops": [[41, null]], "quark.onnx.operators.custom_ops.build_vai_custom_op": [[40, null]], "quark.onnx.operators.vai_ops": [[45, null]], "quark.onnx.operators.vai_ops.concat": [[43, null]], "quark.onnx.operators.vai_ops.hardsigmoid": [[44, null]], "quark.onnx.operators.vai_ops.layernorm": [[46, null]], "quark.onnx.operators.vai_ops.prelu": [[47, null]], "quark.onnx.operators.vai_ops.qdq_ops": [[48, null]], "quark.onnx.operators.vai_ops.softmax": [[49, null]], "quark.onnx.optimizations": [[52, null]], "quark.onnx.optimizations.convert_transforms": [[50, null]], "quark.onnx.optimizations.convert_transforms_pipeline": [[51, null]], "quark.onnx.optimize": [[53, null]], "quark.onnx.qdq_quantizer": [[54, null]], "quark.onnx.quant_utils": [[55, null]], "quark.onnx.quantization": [[60, null]], "quark.onnx.quantization.api": [[56, null]], "quark.onnx.quantization.config": [[59, null]], "quark.onnx.quantization.config.config": [[57, null]], "quark.onnx.quantization.config.custom_config": [[58, null]], "quark.onnx.quantize": [[61, null]], "quark.onnx.quantizers": [[65, null]], "quark.onnx.quantizers.bfp_quantizer": [[62, null]], "quark.onnx.quantizers.cpu_quantizer": [[63, null]], "quark.onnx.quantizers.extended_quantizer": [[64, null]], "quark.onnx.quantizers.matmul_nbits_quantizer": [[66, null]], "quark.onnx.quantizers.npu_cnn_quantizer": [[67, null]], "quark.onnx.quantizers.npu_transformer_quantizer": [[68, null]], "quark.onnx.quantizers.onnx_quantizer": [[69, null]], "quark.onnx.quantizers.qdq_quantizer": [[70, null]], "quark.onnx.quarot": [[71, null]], "quark.onnx.refine": [[72, null]], "quark.onnx.registry": [[73, null]], "quark.onnx.simulate_dpu": [[74, null]], "quark.onnx.simulate_dpu_softmax": [[75, null]], "quark.onnx.smooth_quant": [[76, null]], "quark.onnx.tools": [[100, null]], "quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu": [[77, null]], "quark.onnx.tools.convert_customqdq_to_qdq": [[78, null]], "quark.onnx.tools.convert_dynamic_to_fixed": [[79, null]], "quark.onnx.tools.convert_fp16_to_bf16": [[80, null]], "quark.onnx.tools.convert_fp16_to_bfp16": [[81, null]], "quark.onnx.tools.convert_fp16_to_fp32": [[82, null]], "quark.onnx.tools.convert_fp32_to_bf16": [[83, null]], "quark.onnx.tools.convert_fp32_to_bfp16": [[84, null]], "quark.onnx.tools.convert_fp32_to_fp16": [[85, null]], "quark.onnx.tools.convert_lstm_to_customlstm": [[86, null]], "quark.onnx.tools.convert_nchw_to_nhwc": [[87, null]], "quark.onnx.tools.convert_onnx_to_onnxtxt": [[88, null]], "quark.onnx.tools.convert_onnxtxt_to_onnx": [[89, null]], "quark.onnx.tools.convert_opset_version": [[90, null]], "quark.onnx.tools.convert_qdq_to_qop": [[91, null]], "quark.onnx.tools.convert_quant_to_float": [[92, null]], "quark.onnx.tools.convert_resize_fs_to_pof2s": [[93, null]], "quark.onnx.tools.convert_s8s8_to_u8s8": [[94, null]], "quark.onnx.tools.convert_shared_initializer_to_unique": [[95, null]], "quark.onnx.tools.convert_u16s8_to_s16s8": [[96, null]], "quark.onnx.tools.convert_u16u8_to_u8u8": [[97, null]], "quark.onnx.tools.evaluate": [[98, null]], "quark.onnx.tools.float16": [[99, null]], "quark.onnx.tools.insert_clip_bfloat16_qdq": [[101, null]], "quark.onnx.tools.print_a16w8_a8w8_nodes": [[102, null]], "quark.onnx.tools.random_quantize": [[103, null]], "quark.onnx.tools.remove_bf16_cast": [[104, null]], "quark.onnx.tools.remove_initializer_from_input": [[105, null]], "quark.onnx.tools.remove_qdq": [[106, null]], "quark.onnx.tools.remove_qdq_between_ops": [[107, null]], "quark.onnx.tools.remove_qdq_mul_add": [[108, null]], "quark.onnx.tools.replace_bfloat16_qdq_cast": [[109, null]], "quark.onnx.tools.replace_inf_weights": [[110, null]], "quark.onnx.tools.save_tensor_hist": [[111, null]], "quark.onnx.tools.save_weights_hist": [[112, null]], "quark.onnx.utils": [[113, null]], "quark.onnx.utils.model_utils": [[114, null]], "quark.shares": [[115, null]], "quark.shares.utils": [[117, null]], "quark.shares.utils.import_utils": [[116, null]], "quark.shares.utils.log": [[118, null]], "quark.shares.utils.testing_utils": [[119, null]], "quark.torch": [[188, null]], "quark.torch.algorithm": [[133, null]], "quark.torch.algorithm.api": [[120, null]], "quark.torch.algorithm.awq": [[123, null]], "quark.torch.algorithm.awq.auto_smooth": [[121, null]], "quark.torch.algorithm.awq.awq": [[122, null]], "quark.torch.algorithm.awq.modules": [[125, null]], "quark.torch.algorithm.awq.modules.act": [[124, null]], "quark.torch.algorithm.awq.scale": [[126, null]], "quark.torch.algorithm.awq.smooth": [[127, null]], "quark.torch.algorithm.blockwise_tuning": [[130, null]], "quark.torch.algorithm.blockwise_tuning.blockwise_tuning": [[128, null]], "quark.torch.algorithm.blockwise_tuning.blockwise_utils": [[129, null]], "quark.torch.algorithm.gptq": [[132, null]], "quark.torch.algorithm.gptq.gptq": [[131, null]], "quark.torch.algorithm.osscar": [[134, null]], "quark.torch.algorithm.osscar.osscar": [[135, null]], "quark.torch.algorithm.processor": [[136, null]], "quark.torch.algorithm.quarot": [[137, null]], "quark.torch.algorithm.quarot.monkeypatch": [[138, null]], "quark.torch.algorithm.quarot.quarot": [[139, null]], "quark.torch.algorithm.quarot.utils": [[140, null]], "quark.torch.algorithm.rotation": [[142, null]], "quark.torch.algorithm.rotation.hadamard": [[141, null]], "quark.torch.algorithm.rotation.rotation": [[143, null]], "quark.torch.algorithm.rotation.rotation_utils": [[144, null]], "quark.torch.algorithm.utils": [[146, null]], "quark.torch.algorithm.utils.auto_config": [[145, null]], "quark.torch.algorithm.utils.module": [[147, null]], "quark.torch.algorithm.utils.prepare": [[148, null]], "quark.torch.algorithm.utils.utils": [[149, null]], "quark.torch.export": [[160, null]], "quark.torch.export.api": [[150, null]], "quark.torch.export.config": [[152, null]], "quark.torch.export.config.config": [[151, null]], "quark.torch.export.constants": [[153, null]], "quark.torch.export.gguf_export": [[157, null]], "quark.torch.export.gguf_export.api": [[154, null]], "quark.torch.export.gguf_export.gguf_model_converter": [[155, null]], "quark.torch.export.gguf_export.gguf_model_writer": [[156, null]], "quark.torch.export.gguf_export.tensor_convert": [[158, null]], "quark.torch.export.gguf_export.utils": [[159, null]], "quark.torch.export.json_export": [[167, null]], "quark.torch.export.json_export.builder": [[161, null]], "quark.torch.export.json_export.builder.llm_info": [[162, null]], "quark.torch.export.json_export.builder.llm_info_builder": [[163, null]], "quark.torch.export.json_export.builder.native_model_info_builder": [[164, null]], "quark.torch.export.json_export.converter": [[165, null]], "quark.torch.export.json_export.converter.llm_info_converter": [[166, null]], "quark.torch.export.json_export.utils": [[168, null]], "quark.torch.export.json_export.utils.utils": [[169, null]], "quark.torch.export.main_export": [[170, null]], "quark.torch.export.main_export.model_post_process": [[171, null]], "quark.torch.export.main_export.quant_config_parser": [[172, null]], "quark.torch.export.main_import": [[173, null]], "quark.torch.export.main_import.pretrained_config": [[174, null]], "quark.torch.export.nn": [[175, null]], "quark.torch.export.nn.modules": [[176, null]], "quark.torch.export.nn.modules.qparamslinear": [[177, null]], "quark.torch.export.nn.modules.realquantizer": [[178, null]], "quark.torch.export.onnx": [[179, null]], "quark.torch.export.utils": [[180, null]], "quark.torch.extensions": [[187, null]], "quark.torch.extensions.brevitas": [[184, null]], "quark.torch.extensions.brevitas.algos": [[181, null]], "quark.torch.extensions.brevitas.api": [[182, null]], "quark.torch.extensions.brevitas.config": [[183, null]], "quark.torch.extensions.brevitas.mapping": [[185, null]], "quark.torch.extensions.brevitas.verification": [[186, null]], "quark.torch.kernel": [[192, null]], "quark.torch.kernel.hw_emulation": [[191, null]], "quark.torch.kernel.hw_emulation.extensions": [[189, null]], "quark.torch.kernel.hw_emulation.hw_emulation_interface": [[190, null]], "quark.torch.pruning": [[195, null]], "quark.torch.pruning.api": [[193, null]], "quark.torch.pruning.config": [[194, null]], "quark.torch.pruning.model_transformation": [[196, null]], "quark.torch.pruning.utils": [[197, null]], "quark.torch.quantization": [[229, null]], "quark.torch.quantization.api": [[198, null]], "quark.torch.quantization.config": [[201, null]], "quark.torch.quantization.config.config": [[199, null]], "quark.torch.quantization.config.config_verification": [[200, null]], "quark.torch.quantization.config.type": [[202, null]], "quark.torch.quantization.config.utils": [[203, null]], "quark.torch.quantization.constants": [[204, null]], "quark.torch.quantization.debug": [[205, null]], "quark.torch.quantization.graph": [[208, null]], "quark.torch.quantization.graph.fx": [[207, null]], "quark.torch.quantization.graph.fx.base": [[206, null]], "quark.torch.quantization.graph.optimization": [[210, null]], "quark.torch.quantization.graph.optimization.activate_dropout": [[209, null]], "quark.torch.quantization.graph.optimization.model_optimization": [[211, null]], "quark.torch.quantization.graph.optimization.modify_reshape_param": [[212, null]], "quark.torch.quantization.graph.optimization.opt_pass_manager": [[213, null]], "quark.torch.quantization.graph.optimization.post_quant": [[214, null]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant": [[215, null]], "quark.torch.quantization.graph.optimization.pre_quant": [[216, null]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant": [[217, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d": [[218, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model": [[219, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d": [[220, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear": [[221, null]], "quark.torch.quantization.graph.optimization.remove_dropout_node": [[222, null]], "quark.torch.quantization.graph.optimization.utils": [[223, null]], "quark.torch.quantization.graph.processor": [[224, null]], "quark.torch.quantization.graph.processor.insert_quantizer": [[225, null]], "quark.torch.quantization.graph.processor.processor": [[226, null]], "quark.torch.quantization.graph.processor.processor_utils": [[227, null]], "quark.torch.quantization.graph.torch_utils": [[228, null]], "quark.torch.quantization.model_transformation": [[230, null]], "quark.torch.quantization.nn": [[231, null]], "quark.torch.quantization.nn.modules": [[232, null]], "quark.torch.quantization.nn.modules.mixin": [[233, null]], "quark.torch.quantization.nn.modules.quantize_conv": [[234, null]], "quark.torch.quantization.nn.modules.quantize_conv_bn_fused": [[235, null]], "quark.torch.quantization.nn.modules.quantize_embed": [[236, null]], "quark.torch.quantization.nn.modules.quantize_linear": [[237, null]], "quark.torch.quantization.nn.utils": [[238, null]], "quark.torch.quantization.observer": [[239, null]], "quark.torch.quantization.observer.lsq_observer": [[240, null]], "quark.torch.quantization.observer.observer": [[241, null]], "quark.torch.quantization.observer.tqt_observer": [[242, null]], "quark.torch.quantization.tensor_quantize": [[243, null]], "quark.torch.quantization.utils": [[244, null]], "quark.torch.utils": [[245, null]], "quark.torch.utils.pack": [[246, null]], "quark.version": [[247, null]]}, "docnames": ["autoapi/quark/index", "autoapi/quark/onnx/auto_search/index", "autoapi/quark/onnx/bias_correction/index", "autoapi/quark/onnx/calibrate/index", "autoapi/quark/onnx/cpu_quantizer/index", "autoapi/quark/onnx/equalization/index", "autoapi/quark/onnx/finetuning/create_torch/base_fn_quantizers/index", "autoapi/quark/onnx/finetuning/create_torch/base_qdq_quantizers/index", "autoapi/quark/onnx/finetuning/create_torch/create_model/index", "autoapi/quark/onnx/finetuning/create_torch/create_model_ops/index", "autoapi/quark/onnx/finetuning/create_torch/create_model_test/index", "autoapi/quark/onnx/finetuning/create_torch/create_model_utils/index", "autoapi/quark/onnx/finetuning/create_torch/index", "autoapi/quark/onnx/finetuning/create_torch/quant_base_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_conv_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_gemm_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_matmul_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_norm_ops/index", "autoapi/quark/onnx/finetuning/fast_finetune/index", "autoapi/quark/onnx/finetuning/index", "autoapi/quark/onnx/finetuning/onnx_evaluate/index", "autoapi/quark/onnx/finetuning/onnx_subgraph/index", "autoapi/quark/onnx/finetuning/torch_utils/index", "autoapi/quark/onnx/finetuning/torch_utils_test/index", "autoapi/quark/onnx/finetuning/train_torch/index", "autoapi/quark/onnx/finetuning/train_torch/train_model/index", "autoapi/quark/onnx/finetuning/train_torch/train_model_loss/index", "autoapi/quark/onnx/finetuning/train_torch/train_model_param/index", "autoapi/quark/onnx/gptq/gptq/index", "autoapi/quark/onnx/gptq/index", "autoapi/quark/onnx/graph_transformations/index", "autoapi/quark/onnx/graph_transformations/model_transformer/index", "autoapi/quark/onnx/graph_transformations/model_transformer_test/index", "autoapi/quark/onnx/graph_transformations/transforms/index", "autoapi/quark/onnx/graph_transformations/transforms_pipeline/index", "autoapi/quark/onnx/index", "autoapi/quark/onnx/mprecision/auto_mixprecision/index", "autoapi/quark/onnx/mprecision/index", "autoapi/quark/onnx/mprecision/mixed_bfp/index", "autoapi/quark/onnx/onnx_quantizer/index", "autoapi/quark/onnx/operators/custom_ops/build_vai_custom_op/index", "autoapi/quark/onnx/operators/custom_ops/index", "autoapi/quark/onnx/operators/index", "autoapi/quark/onnx/operators/vai_ops/concat/index", "autoapi/quark/onnx/operators/vai_ops/hardsigmoid/index", "autoapi/quark/onnx/operators/vai_ops/index", "autoapi/quark/onnx/operators/vai_ops/layernorm/index", "autoapi/quark/onnx/operators/vai_ops/prelu/index", "autoapi/quark/onnx/operators/vai_ops/qdq_ops/index", "autoapi/quark/onnx/operators/vai_ops/softmax/index", "autoapi/quark/onnx/optimizations/convert_transforms/index", "autoapi/quark/onnx/optimizations/convert_transforms_pipeline/index", "autoapi/quark/onnx/optimizations/index", "autoapi/quark/onnx/optimize/index", "autoapi/quark/onnx/qdq_quantizer/index", "autoapi/quark/onnx/quant_utils/index", "autoapi/quark/onnx/quantization/api/index", "autoapi/quark/onnx/quantization/config/config/index", "autoapi/quark/onnx/quantization/config/custom_config/index", "autoapi/quark/onnx/quantization/config/index", "autoapi/quark/onnx/quantization/index", "autoapi/quark/onnx/quantize/index", "autoapi/quark/onnx/quantizers/bfp_quantizer/index", "autoapi/quark/onnx/quantizers/cpu_quantizer/index", "autoapi/quark/onnx/quantizers/extended_quantizer/index", "autoapi/quark/onnx/quantizers/index", "autoapi/quark/onnx/quantizers/matmul_nbits_quantizer/index", "autoapi/quark/onnx/quantizers/npu_cnn_quantizer/index", "autoapi/quark/onnx/quantizers/npu_transformer_quantizer/index", "autoapi/quark/onnx/quantizers/onnx_quantizer/index", "autoapi/quark/onnx/quantizers/qdq_quantizer/index", "autoapi/quark/onnx/quarot/index", "autoapi/quark/onnx/refine/index", "autoapi/quark/onnx/registry/index", "autoapi/quark/onnx/simulate_dpu/index", "autoapi/quark/onnx/simulate_dpu_softmax/index", "autoapi/quark/onnx/smooth_quant/index", "autoapi/quark/onnx/tools/convert_a8w8_npu_to_a8w8_cpu/index", "autoapi/quark/onnx/tools/convert_customqdq_to_qdq/index", "autoapi/quark/onnx/tools/convert_dynamic_to_fixed/index", "autoapi/quark/onnx/tools/convert_fp16_to_bf16/index", "autoapi/quark/onnx/tools/convert_fp16_to_bfp16/index", "autoapi/quark/onnx/tools/convert_fp16_to_fp32/index", "autoapi/quark/onnx/tools/convert_fp32_to_bf16/index", "autoapi/quark/onnx/tools/convert_fp32_to_bfp16/index", "autoapi/quark/onnx/tools/convert_fp32_to_fp16/index", "autoapi/quark/onnx/tools/convert_lstm_to_customlstm/index", "autoapi/quark/onnx/tools/convert_nchw_to_nhwc/index", "autoapi/quark/onnx/tools/convert_onnx_to_onnxtxt/index", "autoapi/quark/onnx/tools/convert_onnxtxt_to_onnx/index", "autoapi/quark/onnx/tools/convert_opset_version/index", "autoapi/quark/onnx/tools/convert_qdq_to_qop/index", "autoapi/quark/onnx/tools/convert_quant_to_float/index", "autoapi/quark/onnx/tools/convert_resize_fs_to_pof2s/index", "autoapi/quark/onnx/tools/convert_s8s8_to_u8s8/index", "autoapi/quark/onnx/tools/convert_shared_initializer_to_unique/index", "autoapi/quark/onnx/tools/convert_u16s8_to_s16s8/index", "autoapi/quark/onnx/tools/convert_u16u8_to_u8u8/index", "autoapi/quark/onnx/tools/evaluate/index", "autoapi/quark/onnx/tools/float16/index", "autoapi/quark/onnx/tools/index", "autoapi/quark/onnx/tools/insert_clip_bfloat16_qdq/index", "autoapi/quark/onnx/tools/print_a16w8_a8w8_nodes/index", "autoapi/quark/onnx/tools/random_quantize/index", "autoapi/quark/onnx/tools/remove_bf16_cast/index", "autoapi/quark/onnx/tools/remove_initializer_from_input/index", "autoapi/quark/onnx/tools/remove_qdq/index", "autoapi/quark/onnx/tools/remove_qdq_between_ops/index", "autoapi/quark/onnx/tools/remove_qdq_mul_add/index", "autoapi/quark/onnx/tools/replace_bfloat16_qdq_cast/index", "autoapi/quark/onnx/tools/replace_inf_weights/index", "autoapi/quark/onnx/tools/save_tensor_hist/index", "autoapi/quark/onnx/tools/save_weights_hist/index", "autoapi/quark/onnx/utils/index", "autoapi/quark/onnx/utils/model_utils/index", "autoapi/quark/shares/index", "autoapi/quark/shares/utils/import_utils/index", "autoapi/quark/shares/utils/index", "autoapi/quark/shares/utils/log/index", "autoapi/quark/shares/utils/testing_utils/index", "autoapi/quark/torch/algorithm/api/index", "autoapi/quark/torch/algorithm/awq/auto_smooth/index", "autoapi/quark/torch/algorithm/awq/awq/index", "autoapi/quark/torch/algorithm/awq/index", "autoapi/quark/torch/algorithm/awq/modules/act/index", "autoapi/quark/torch/algorithm/awq/modules/index", "autoapi/quark/torch/algorithm/awq/scale/index", "autoapi/quark/torch/algorithm/awq/smooth/index", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_tuning/index", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_utils/index", "autoapi/quark/torch/algorithm/blockwise_tuning/index", "autoapi/quark/torch/algorithm/gptq/gptq/index", "autoapi/quark/torch/algorithm/gptq/index", "autoapi/quark/torch/algorithm/index", "autoapi/quark/torch/algorithm/osscar/index", "autoapi/quark/torch/algorithm/osscar/osscar/index", "autoapi/quark/torch/algorithm/processor/index", "autoapi/quark/torch/algorithm/quarot/index", "autoapi/quark/torch/algorithm/quarot/monkeypatch/index", "autoapi/quark/torch/algorithm/quarot/quarot/index", "autoapi/quark/torch/algorithm/quarot/utils/index", "autoapi/quark/torch/algorithm/rotation/hadamard/index", "autoapi/quark/torch/algorithm/rotation/index", "autoapi/quark/torch/algorithm/rotation/rotation/index", "autoapi/quark/torch/algorithm/rotation/rotation_utils/index", "autoapi/quark/torch/algorithm/utils/auto_config/index", "autoapi/quark/torch/algorithm/utils/index", "autoapi/quark/torch/algorithm/utils/module/index", "autoapi/quark/torch/algorithm/utils/prepare/index", "autoapi/quark/torch/algorithm/utils/utils/index", "autoapi/quark/torch/export/api/index", "autoapi/quark/torch/export/config/config/index", "autoapi/quark/torch/export/config/index", "autoapi/quark/torch/export/constants/index", "autoapi/quark/torch/export/gguf_export/api/index", "autoapi/quark/torch/export/gguf_export/gguf_model_converter/index", "autoapi/quark/torch/export/gguf_export/gguf_model_writer/index", "autoapi/quark/torch/export/gguf_export/index", "autoapi/quark/torch/export/gguf_export/tensor_convert/index", "autoapi/quark/torch/export/gguf_export/utils/index", "autoapi/quark/torch/export/index", "autoapi/quark/torch/export/json_export/builder/index", "autoapi/quark/torch/export/json_export/builder/llm_info/index", "autoapi/quark/torch/export/json_export/builder/llm_info_builder/index", "autoapi/quark/torch/export/json_export/builder/native_model_info_builder/index", "autoapi/quark/torch/export/json_export/converter/index", "autoapi/quark/torch/export/json_export/converter/llm_info_converter/index", "autoapi/quark/torch/export/json_export/index", "autoapi/quark/torch/export/json_export/utils/index", "autoapi/quark/torch/export/json_export/utils/utils/index", "autoapi/quark/torch/export/main_export/index", "autoapi/quark/torch/export/main_export/model_post_process/index", "autoapi/quark/torch/export/main_export/quant_config_parser/index", "autoapi/quark/torch/export/main_import/index", "autoapi/quark/torch/export/main_import/pretrained_config/index", "autoapi/quark/torch/export/nn/index", "autoapi/quark/torch/export/nn/modules/index", "autoapi/quark/torch/export/nn/modules/qparamslinear/index", "autoapi/quark/torch/export/nn/modules/realquantizer/index", "autoapi/quark/torch/export/onnx/index", "autoapi/quark/torch/export/utils/index", "autoapi/quark/torch/extensions/brevitas/algos/index", "autoapi/quark/torch/extensions/brevitas/api/index", "autoapi/quark/torch/extensions/brevitas/config/index", "autoapi/quark/torch/extensions/brevitas/index", "autoapi/quark/torch/extensions/brevitas/mapping/index", "autoapi/quark/torch/extensions/brevitas/verification/index", "autoapi/quark/torch/extensions/index", "autoapi/quark/torch/index", "autoapi/quark/torch/kernel/hw_emulation/extensions/index", "autoapi/quark/torch/kernel/hw_emulation/hw_emulation_interface/index", "autoapi/quark/torch/kernel/hw_emulation/index", "autoapi/quark/torch/kernel/index", "autoapi/quark/torch/pruning/api/index", "autoapi/quark/torch/pruning/config/index", "autoapi/quark/torch/pruning/index", "autoapi/quark/torch/pruning/model_transformation/index", "autoapi/quark/torch/pruning/utils/index", "autoapi/quark/torch/quantization/api/index", "autoapi/quark/torch/quantization/config/config/index", "autoapi/quark/torch/quantization/config/config_verification/index", "autoapi/quark/torch/quantization/config/index", "autoapi/quark/torch/quantization/config/type/index", "autoapi/quark/torch/quantization/config/utils/index", "autoapi/quark/torch/quantization/constants/index", "autoapi/quark/torch/quantization/debug/index", "autoapi/quark/torch/quantization/graph/fx/base/index", "autoapi/quark/torch/quantization/graph/fx/index", "autoapi/quark/torch/quantization/graph/index", "autoapi/quark/torch/quantization/graph/optimization/activate_dropout/index", "autoapi/quark/torch/quantization/graph/optimization/index", "autoapi/quark/torch/quantization/graph/optimization/model_optimization/index", "autoapi/quark/torch/quantization/graph/optimization/modify_reshape_param/index", "autoapi/quark/torch/quantization/graph/optimization/opt_pass_manager/index", "autoapi/quark/torch/quantization/graph/optimization/post_quant/index", "autoapi/quark/torch/quantization/graph/optimization/post_quant/opt_pass_after_quant/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/opt_pass_before_quant/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv2d_to_qtconv2d/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv_bn_to_qt_model/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_convtranspose2d_to_qtconvtranspose2d/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_linear_to_qtlinear/index", "autoapi/quark/torch/quantization/graph/optimization/remove_dropout_node/index", "autoapi/quark/torch/quantization/graph/optimization/utils/index", "autoapi/quark/torch/quantization/graph/processor/index", "autoapi/quark/torch/quantization/graph/processor/insert_quantizer/index", "autoapi/quark/torch/quantization/graph/processor/processor/index", "autoapi/quark/torch/quantization/graph/processor/processor_utils/index", "autoapi/quark/torch/quantization/graph/torch_utils/index", "autoapi/quark/torch/quantization/index", "autoapi/quark/torch/quantization/model_transformation/index", "autoapi/quark/torch/quantization/nn/index", "autoapi/quark/torch/quantization/nn/modules/index", "autoapi/quark/torch/quantization/nn/modules/mixin/index", "autoapi/quark/torch/quantization/nn/modules/quantize_conv/index", "autoapi/quark/torch/quantization/nn/modules/quantize_conv_bn_fused/index", "autoapi/quark/torch/quantization/nn/modules/quantize_embed/index", "autoapi/quark/torch/quantization/nn/modules/quantize_linear/index", "autoapi/quark/torch/quantization/nn/utils/index", "autoapi/quark/torch/quantization/observer/index", "autoapi/quark/torch/quantization/observer/lsq_observer/index", "autoapi/quark/torch/quantization/observer/observer/index", "autoapi/quark/torch/quantization/observer/tqt_observer/index", "autoapi/quark/torch/quantization/tensor_quantize/index", "autoapi/quark/torch/quantization/utils/index", "autoapi/quark/torch/utils/index", "autoapi/quark/torch/utils/pack/index", "autoapi/quark/version/index", "basic_usage", "index", "install", "intro", "onnx/accuracy_algorithms/ada", "onnx/accuracy_algorithms/cle", "onnx/accuracy_algorithms/quarot", "onnx/accuracy_algorithms/sq", "onnx/accuracy_improvement_algorithms", "onnx/appendix_full_quant_config_features", "onnx/basic_usage_onnx", "onnx/config/calibration_datasets", "onnx/config/calibration_methods", "onnx/config/quantization_schemes", "onnx/config/quantization_strategies", "onnx/config/quantization_symmetry", "onnx/config/user_guide_onnx_model_inference_save_input_npy", "onnx/example_quark_onnx_BFP", "onnx/example_quark_onnx_MX", "onnx/example_quark_onnx_adaquant", "onnx/example_quark_onnx_adaround", "onnx/example_quark_onnx_auto_search", "onnx/example_quark_onnx_cle", "onnx/example_quark_onnx_dynamic_quantization_llama2", "onnx/example_quark_onnx_dynamic_quantization_opt", "onnx/example_quark_onnx_gptq", "onnx/example_quark_onnx_image_classification", "onnx/example_quark_onnx_language_models", "onnx/example_quark_onnx_mixed_precision", "onnx/example_quark_onnx_quarot", "onnx/example_quark_onnx_ryzenai", "onnx/example_quark_onnx_ryzenai_yolov3_customer_evaluator", "onnx/example_quark_onnx_smoothquant", "onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2", "onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2", "onnx/example_ryzenai_autosearch_resnet50", "onnx/gpu_usage_guide", "onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice", "onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice", "onnx/onnx_apis", "onnx/onnx_examples", "onnx/onnx_faq", "onnx/optional_utilities", "onnx/ryzen_ai_best_practice", "onnx/tools", "onnx/tutorial_bf16_quantization", "onnx/tutorial_bfp16_quantization", "onnx/tutorial_microexponents_quantization", "onnx/tutorial_microscaling_quantization", "onnx/tutorial_mix_precision", "onnx/user_guide_auto_search", "onnx/user_guide_config_description", "onnx/user_guide_supported_optype_datatype", "pytorch/adv_mx", "pytorch/adv_two_level", "pytorch/basic_usage_pytorch", "pytorch/calibration_datasets", "pytorch/calibration_methods", "pytorch/debug", "pytorch/example_quark_torch_brevitas", "pytorch/example_quark_torch_diffusers", "pytorch/example_quark_torch_llm_eval", "pytorch/example_quark_torch_llm_eval_harness", "pytorch/example_quark_torch_llm_eval_harness_offline", "pytorch/example_quark_torch_llm_eval_perplexity", "pytorch/example_quark_torch_llm_eval_rouge_meteor", "pytorch/example_quark_torch_llm_pruning", "pytorch/example_quark_torch_llm_ptq", "pytorch/example_quark_torch_llm_qat", "pytorch/example_quark_torch_pytorch_light", "pytorch/example_quark_torch_vision", "pytorch/export/gguf_llamacpp", "pytorch/export/quark_export", "pytorch/export/quark_export_gguf", "pytorch/export/quark_export_hf", "pytorch/export/quark_export_oga", "pytorch/export/quark_export_onnx", "pytorch/export/quark_export_quark", "pytorch/extensions", "pytorch/llm_quark", "pytorch/pytorch_apis", "pytorch/pytorch_examples", "pytorch/pytorch_faq", "pytorch/quantization_schemes", "pytorch/quantization_strategies", "pytorch/quantization_symmetry", "pytorch/quark_save_load", "pytorch/quark_torch_best_practices", "pytorch/smoothquant", "pytorch/tutorial_bfp16", "pytorch/tutorial_quarot", "pytorch/tutorial_rotation", "pytorch/user_guide_config_description", "release_note"], "envversion": {"sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1}, "filenames": ["autoapi/quark/index.rst", "autoapi/quark/onnx/auto_search/index.rst", "autoapi/quark/onnx/bias_correction/index.rst", "autoapi/quark/onnx/calibrate/index.rst", "autoapi/quark/onnx/cpu_quantizer/index.rst", "autoapi/quark/onnx/equalization/index.rst", "autoapi/quark/onnx/finetuning/create_torch/base_fn_quantizers/index.rst", "autoapi/quark/onnx/finetuning/create_torch/base_qdq_quantizers/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model_test/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model_utils/index.rst", "autoapi/quark/onnx/finetuning/create_torch/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_base_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_conv_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_gemm_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_matmul_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_norm_ops/index.rst", "autoapi/quark/onnx/finetuning/fast_finetune/index.rst", "autoapi/quark/onnx/finetuning/index.rst", "autoapi/quark/onnx/finetuning/onnx_evaluate/index.rst", "autoapi/quark/onnx/finetuning/onnx_subgraph/index.rst", "autoapi/quark/onnx/finetuning/torch_utils/index.rst", "autoapi/quark/onnx/finetuning/torch_utils_test/index.rst", "autoapi/quark/onnx/finetuning/train_torch/index.rst", "autoapi/quark/onnx/finetuning/train_torch/train_model/index.rst", "autoapi/quark/onnx/finetuning/train_torch/train_model_loss/index.rst", "autoapi/quark/onnx/finetuning/train_torch/train_model_param/index.rst", "autoapi/quark/onnx/gptq/gptq/index.rst", "autoapi/quark/onnx/gptq/index.rst", "autoapi/quark/onnx/graph_transformations/index.rst", "autoapi/quark/onnx/graph_transformations/model_transformer/index.rst", "autoapi/quark/onnx/graph_transformations/model_transformer_test/index.rst", "autoapi/quark/onnx/graph_transformations/transforms/index.rst", "autoapi/quark/onnx/graph_transformations/transforms_pipeline/index.rst", "autoapi/quark/onnx/index.rst", "autoapi/quark/onnx/mprecision/auto_mixprecision/index.rst", "autoapi/quark/onnx/mprecision/index.rst", "autoapi/quark/onnx/mprecision/mixed_bfp/index.rst", "autoapi/quark/onnx/onnx_quantizer/index.rst", "autoapi/quark/onnx/operators/custom_ops/build_vai_custom_op/index.rst", "autoapi/quark/onnx/operators/custom_ops/index.rst", "autoapi/quark/onnx/operators/index.rst", "autoapi/quark/onnx/operators/vai_ops/concat/index.rst", "autoapi/quark/onnx/operators/vai_ops/hardsigmoid/index.rst", "autoapi/quark/onnx/operators/vai_ops/index.rst", "autoapi/quark/onnx/operators/vai_ops/layernorm/index.rst", "autoapi/quark/onnx/operators/vai_ops/prelu/index.rst", "autoapi/quark/onnx/operators/vai_ops/qdq_ops/index.rst", "autoapi/quark/onnx/operators/vai_ops/softmax/index.rst", "autoapi/quark/onnx/optimizations/convert_transforms/index.rst", "autoapi/quark/onnx/optimizations/convert_transforms_pipeline/index.rst", "autoapi/quark/onnx/optimizations/index.rst", "autoapi/quark/onnx/optimize/index.rst", "autoapi/quark/onnx/qdq_quantizer/index.rst", "autoapi/quark/onnx/quant_utils/index.rst", "autoapi/quark/onnx/quantization/api/index.rst", "autoapi/quark/onnx/quantization/config/config/index.rst", "autoapi/quark/onnx/quantization/config/custom_config/index.rst", "autoapi/quark/onnx/quantization/config/index.rst", "autoapi/quark/onnx/quantization/index.rst", "autoapi/quark/onnx/quantize/index.rst", "autoapi/quark/onnx/quantizers/bfp_quantizer/index.rst", "autoapi/quark/onnx/quantizers/cpu_quantizer/index.rst", "autoapi/quark/onnx/quantizers/extended_quantizer/index.rst", "autoapi/quark/onnx/quantizers/index.rst", "autoapi/quark/onnx/quantizers/matmul_nbits_quantizer/index.rst", "autoapi/quark/onnx/quantizers/npu_cnn_quantizer/index.rst", "autoapi/quark/onnx/quantizers/npu_transformer_quantizer/index.rst", "autoapi/quark/onnx/quantizers/onnx_quantizer/index.rst", "autoapi/quark/onnx/quantizers/qdq_quantizer/index.rst", "autoapi/quark/onnx/quarot/index.rst", "autoapi/quark/onnx/refine/index.rst", "autoapi/quark/onnx/registry/index.rst", "autoapi/quark/onnx/simulate_dpu/index.rst", "autoapi/quark/onnx/simulate_dpu_softmax/index.rst", "autoapi/quark/onnx/smooth_quant/index.rst", "autoapi/quark/onnx/tools/convert_a8w8_npu_to_a8w8_cpu/index.rst", "autoapi/quark/onnx/tools/convert_customqdq_to_qdq/index.rst", "autoapi/quark/onnx/tools/convert_dynamic_to_fixed/index.rst", "autoapi/quark/onnx/tools/convert_fp16_to_bf16/index.rst", "autoapi/quark/onnx/tools/convert_fp16_to_bfp16/index.rst", "autoapi/quark/onnx/tools/convert_fp16_to_fp32/index.rst", "autoapi/quark/onnx/tools/convert_fp32_to_bf16/index.rst", "autoapi/quark/onnx/tools/convert_fp32_to_bfp16/index.rst", "autoapi/quark/onnx/tools/convert_fp32_to_fp16/index.rst", "autoapi/quark/onnx/tools/convert_lstm_to_customlstm/index.rst", "autoapi/quark/onnx/tools/convert_nchw_to_nhwc/index.rst", "autoapi/quark/onnx/tools/convert_onnx_to_onnxtxt/index.rst", "autoapi/quark/onnx/tools/convert_onnxtxt_to_onnx/index.rst", "autoapi/quark/onnx/tools/convert_opset_version/index.rst", "autoapi/quark/onnx/tools/convert_qdq_to_qop/index.rst", "autoapi/quark/onnx/tools/convert_quant_to_float/index.rst", "autoapi/quark/onnx/tools/convert_resize_fs_to_pof2s/index.rst", "autoapi/quark/onnx/tools/convert_s8s8_to_u8s8/index.rst", "autoapi/quark/onnx/tools/convert_shared_initializer_to_unique/index.rst", "autoapi/quark/onnx/tools/convert_u16s8_to_s16s8/index.rst", "autoapi/quark/onnx/tools/convert_u16u8_to_u8u8/index.rst", "autoapi/quark/onnx/tools/evaluate/index.rst", "autoapi/quark/onnx/tools/float16/index.rst", "autoapi/quark/onnx/tools/index.rst", "autoapi/quark/onnx/tools/insert_clip_bfloat16_qdq/index.rst", "autoapi/quark/onnx/tools/print_a16w8_a8w8_nodes/index.rst", "autoapi/quark/onnx/tools/random_quantize/index.rst", "autoapi/quark/onnx/tools/remove_bf16_cast/index.rst", "autoapi/quark/onnx/tools/remove_initializer_from_input/index.rst", "autoapi/quark/onnx/tools/remove_qdq/index.rst", "autoapi/quark/onnx/tools/remove_qdq_between_ops/index.rst", "autoapi/quark/onnx/tools/remove_qdq_mul_add/index.rst", "autoapi/quark/onnx/tools/replace_bfloat16_qdq_cast/index.rst", "autoapi/quark/onnx/tools/replace_inf_weights/index.rst", "autoapi/quark/onnx/tools/save_tensor_hist/index.rst", "autoapi/quark/onnx/tools/save_weights_hist/index.rst", "autoapi/quark/onnx/utils/index.rst", "autoapi/quark/onnx/utils/model_utils/index.rst", "autoapi/quark/shares/index.rst", "autoapi/quark/shares/utils/import_utils/index.rst", "autoapi/quark/shares/utils/index.rst", "autoapi/quark/shares/utils/log/index.rst", "autoapi/quark/shares/utils/testing_utils/index.rst", "autoapi/quark/torch/algorithm/api/index.rst", "autoapi/quark/torch/algorithm/awq/auto_smooth/index.rst", "autoapi/quark/torch/algorithm/awq/awq/index.rst", "autoapi/quark/torch/algorithm/awq/index.rst", "autoapi/quark/torch/algorithm/awq/modules/act/index.rst", "autoapi/quark/torch/algorithm/awq/modules/index.rst", "autoapi/quark/torch/algorithm/awq/scale/index.rst", "autoapi/quark/torch/algorithm/awq/smooth/index.rst", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_tuning/index.rst", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_utils/index.rst", "autoapi/quark/torch/algorithm/blockwise_tuning/index.rst", "autoapi/quark/torch/algorithm/gptq/gptq/index.rst", "autoapi/quark/torch/algorithm/gptq/index.rst", "autoapi/quark/torch/algorithm/index.rst", "autoapi/quark/torch/algorithm/osscar/index.rst", "autoapi/quark/torch/algorithm/osscar/osscar/index.rst", "autoapi/quark/torch/algorithm/processor/index.rst", "autoapi/quark/torch/algorithm/quarot/index.rst", "autoapi/quark/torch/algorithm/quarot/monkeypatch/index.rst", "autoapi/quark/torch/algorithm/quarot/quarot/index.rst", "autoapi/quark/torch/algorithm/quarot/utils/index.rst", "autoapi/quark/torch/algorithm/rotation/hadamard/index.rst", "autoapi/quark/torch/algorithm/rotation/index.rst", "autoapi/quark/torch/algorithm/rotation/rotation/index.rst", "autoapi/quark/torch/algorithm/rotation/rotation_utils/index.rst", "autoapi/quark/torch/algorithm/utils/auto_config/index.rst", "autoapi/quark/torch/algorithm/utils/index.rst", "autoapi/quark/torch/algorithm/utils/module/index.rst", "autoapi/quark/torch/algorithm/utils/prepare/index.rst", "autoapi/quark/torch/algorithm/utils/utils/index.rst", "autoapi/quark/torch/export/api/index.rst", "autoapi/quark/torch/export/config/config/index.rst", "autoapi/quark/torch/export/config/index.rst", "autoapi/quark/torch/export/constants/index.rst", "autoapi/quark/torch/export/gguf_export/api/index.rst", "autoapi/quark/torch/export/gguf_export/gguf_model_converter/index.rst", "autoapi/quark/torch/export/gguf_export/gguf_model_writer/index.rst", "autoapi/quark/torch/export/gguf_export/index.rst", "autoapi/quark/torch/export/gguf_export/tensor_convert/index.rst", "autoapi/quark/torch/export/gguf_export/utils/index.rst", "autoapi/quark/torch/export/index.rst", "autoapi/quark/torch/export/json_export/builder/index.rst", "autoapi/quark/torch/export/json_export/builder/llm_info/index.rst", "autoapi/quark/torch/export/json_export/builder/llm_info_builder/index.rst", "autoapi/quark/torch/export/json_export/builder/native_model_info_builder/index.rst", "autoapi/quark/torch/export/json_export/converter/index.rst", "autoapi/quark/torch/export/json_export/converter/llm_info_converter/index.rst", "autoapi/quark/torch/export/json_export/index.rst", "autoapi/quark/torch/export/json_export/utils/index.rst", "autoapi/quark/torch/export/json_export/utils/utils/index.rst", "autoapi/quark/torch/export/main_export/index.rst", "autoapi/quark/torch/export/main_export/model_post_process/index.rst", "autoapi/quark/torch/export/main_export/quant_config_parser/index.rst", "autoapi/quark/torch/export/main_import/index.rst", "autoapi/quark/torch/export/main_import/pretrained_config/index.rst", "autoapi/quark/torch/export/nn/index.rst", "autoapi/quark/torch/export/nn/modules/index.rst", "autoapi/quark/torch/export/nn/modules/qparamslinear/index.rst", "autoapi/quark/torch/export/nn/modules/realquantizer/index.rst", "autoapi/quark/torch/export/onnx/index.rst", "autoapi/quark/torch/export/utils/index.rst", "autoapi/quark/torch/extensions/brevitas/algos/index.rst", "autoapi/quark/torch/extensions/brevitas/api/index.rst", "autoapi/quark/torch/extensions/brevitas/config/index.rst", "autoapi/quark/torch/extensions/brevitas/index.rst", "autoapi/quark/torch/extensions/brevitas/mapping/index.rst", "autoapi/quark/torch/extensions/brevitas/verification/index.rst", "autoapi/quark/torch/extensions/index.rst", "autoapi/quark/torch/index.rst", "autoapi/quark/torch/kernel/hw_emulation/extensions/index.rst", "autoapi/quark/torch/kernel/hw_emulation/hw_emulation_interface/index.rst", "autoapi/quark/torch/kernel/hw_emulation/index.rst", "autoapi/quark/torch/kernel/index.rst", "autoapi/quark/torch/pruning/api/index.rst", "autoapi/quark/torch/pruning/config/index.rst", "autoapi/quark/torch/pruning/index.rst", "autoapi/quark/torch/pruning/model_transformation/index.rst", "autoapi/quark/torch/pruning/utils/index.rst", "autoapi/quark/torch/quantization/api/index.rst", "autoapi/quark/torch/quantization/config/config/index.rst", "autoapi/quark/torch/quantization/config/config_verification/index.rst", "autoapi/quark/torch/quantization/config/index.rst", "autoapi/quark/torch/quantization/config/type/index.rst", "autoapi/quark/torch/quantization/config/utils/index.rst", "autoapi/quark/torch/quantization/constants/index.rst", "autoapi/quark/torch/quantization/debug/index.rst", "autoapi/quark/torch/quantization/graph/fx/base/index.rst", "autoapi/quark/torch/quantization/graph/fx/index.rst", "autoapi/quark/torch/quantization/graph/index.rst", "autoapi/quark/torch/quantization/graph/optimization/activate_dropout/index.rst", "autoapi/quark/torch/quantization/graph/optimization/index.rst", "autoapi/quark/torch/quantization/graph/optimization/model_optimization/index.rst", "autoapi/quark/torch/quantization/graph/optimization/modify_reshape_param/index.rst", "autoapi/quark/torch/quantization/graph/optimization/opt_pass_manager/index.rst", "autoapi/quark/torch/quantization/graph/optimization/post_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/post_quant/opt_pass_after_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/opt_pass_before_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv2d_to_qtconv2d/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv_bn_to_qt_model/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_convtranspose2d_to_qtconvtranspose2d/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_linear_to_qtlinear/index.rst", "autoapi/quark/torch/quantization/graph/optimization/remove_dropout_node/index.rst", "autoapi/quark/torch/quantization/graph/optimization/utils/index.rst", "autoapi/quark/torch/quantization/graph/processor/index.rst", "autoapi/quark/torch/quantization/graph/processor/insert_quantizer/index.rst", "autoapi/quark/torch/quantization/graph/processor/processor/index.rst", "autoapi/quark/torch/quantization/graph/processor/processor_utils/index.rst", "autoapi/quark/torch/quantization/graph/torch_utils/index.rst", "autoapi/quark/torch/quantization/index.rst", "autoapi/quark/torch/quantization/model_transformation/index.rst", "autoapi/quark/torch/quantization/nn/index.rst", "autoapi/quark/torch/quantization/nn/modules/index.rst", "autoapi/quark/torch/quantization/nn/modules/mixin/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_conv/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_conv_bn_fused/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_embed/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_linear/index.rst", "autoapi/quark/torch/quantization/nn/utils/index.rst", "autoapi/quark/torch/quantization/observer/index.rst", "autoapi/quark/torch/quantization/observer/lsq_observer/index.rst", "autoapi/quark/torch/quantization/observer/observer/index.rst", "autoapi/quark/torch/quantization/observer/tqt_observer/index.rst", "autoapi/quark/torch/quantization/tensor_quantize/index.rst", "autoapi/quark/torch/quantization/utils/index.rst", "autoapi/quark/torch/utils/index.rst", "autoapi/quark/torch/utils/pack/index.rst", "autoapi/quark/version/index.rst", "basic_usage.rst", "index.rst", "install.rst", "intro.rst", "onnx/accuracy_algorithms/ada.rst", "onnx/accuracy_algorithms/cle.rst", "onnx/accuracy_algorithms/quarot.rst", "onnx/accuracy_algorithms/sq.rst", "onnx/accuracy_improvement_algorithms.rst", "onnx/appendix_full_quant_config_features.rst", "onnx/basic_usage_onnx.rst", "onnx/config/calibration_datasets.rst", "onnx/config/calibration_methods.rst", "onnx/config/quantization_schemes.rst", "onnx/config/quantization_strategies.rst", "onnx/config/quantization_symmetry.rst", "onnx/config/user_guide_onnx_model_inference_save_input_npy.rst", "onnx/example_quark_onnx_BFP.rst", "onnx/example_quark_onnx_MX.rst", "onnx/example_quark_onnx_adaquant.rst", "onnx/example_quark_onnx_adaround.rst", "onnx/example_quark_onnx_auto_search.rst", "onnx/example_quark_onnx_cle.rst", "onnx/example_quark_onnx_dynamic_quantization_llama2.rst", "onnx/example_quark_onnx_dynamic_quantization_opt.rst", "onnx/example_quark_onnx_gptq.rst", "onnx/example_quark_onnx_image_classification.rst", "onnx/example_quark_onnx_language_models.rst", "onnx/example_quark_onnx_mixed_precision.rst", "onnx/example_quark_onnx_quarot.rst", "onnx/example_quark_onnx_ryzenai.rst", "onnx/example_quark_onnx_ryzenai_yolov3_customer_evaluator.rst", "onnx/example_quark_onnx_smoothquant.rst", "onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.rst", "onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.rst", "onnx/example_ryzenai_autosearch_resnet50.rst", "onnx/gpu_usage_guide.rst", "onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.rst", "onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.rst", "onnx/onnx_apis.rst", "onnx/onnx_examples.rst", "onnx/onnx_faq.rst", "onnx/optional_utilities.rst", "onnx/ryzen_ai_best_practice.rst", "onnx/tools.rst", "onnx/tutorial_bf16_quantization.rst", "onnx/tutorial_bfp16_quantization.rst", "onnx/tutorial_microexponents_quantization.rst", "onnx/tutorial_microscaling_quantization.rst", "onnx/tutorial_mix_precision.rst", "onnx/user_guide_auto_search.rst", "onnx/user_guide_config_description.rst", "onnx/user_guide_supported_optype_datatype.rst", "pytorch/adv_mx.rst", "pytorch/adv_two_level.rst", "pytorch/basic_usage_pytorch.rst", "pytorch/calibration_datasets.rst", "pytorch/calibration_methods.rst", "pytorch/debug.rst", "pytorch/example_quark_torch_brevitas.rst", "pytorch/example_quark_torch_diffusers.rst", "pytorch/example_quark_torch_llm_eval.rst", "pytorch/example_quark_torch_llm_eval_harness.rst", "pytorch/example_quark_torch_llm_eval_harness_offline.rst", "pytorch/example_quark_torch_llm_eval_perplexity.rst", "pytorch/example_quark_torch_llm_eval_rouge_meteor.rst", "pytorch/example_quark_torch_llm_pruning.rst", "pytorch/example_quark_torch_llm_ptq.rst", "pytorch/example_quark_torch_llm_qat.rst", "pytorch/example_quark_torch_pytorch_light.rst", "pytorch/example_quark_torch_vision.rst", "pytorch/export/gguf_llamacpp.rst", "pytorch/export/quark_export.rst", "pytorch/export/quark_export_gguf.rst", "pytorch/export/quark_export_hf.rst", "pytorch/export/quark_export_oga.rst", "pytorch/export/quark_export_onnx.rst", "pytorch/export/quark_export_quark.rst", "pytorch/extensions.rst", "pytorch/llm_quark.rst", "pytorch/pytorch_apis.rst", "pytorch/pytorch_examples.rst", "pytorch/pytorch_faq.rst", "pytorch/quantization_schemes.rst", "pytorch/quantization_strategies.rst", "pytorch/quantization_symmetry.rst", "pytorch/quark_save_load.rst", "pytorch/quark_torch_best_practices.rst", "pytorch/smoothquant.rst", "pytorch/tutorial_bfp16.rst", "pytorch/tutorial_quarot.rst", "pytorch/tutorial_rotation.rst", "pytorch/user_guide_config_description.rst", "release_note.rst"], "indexentries": {"activatedropoutnode (class in quark.torch.quantization.graph.optimization.activate_dropout)": [[209, "quark.torch.quantization.graph.optimization.activate_dropout.ActivateDropoutNode", false]], "activation_stats_hook() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.activation_stats_hook", false]], "activationequalization (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.ActivationEqualization", false]], "adaroundconstants (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundConstants", false]], "adaroundintquantizer (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer", false]], "add_checks() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassmanager method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager.add_checks", false]], "add_pass() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassmanager method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager.add_pass", false]], "add_pre_optimization_config() (quark.torch.quantization.config.config.config method)": [[199, "quark.torch.quantization.config.config.Config.add_pre_optimization_config", false]], "add_qk_rotation_after_function_call_in_forward() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.add_qk_rotation_after_function_call_in_forward", false]], "add_wrapper_after_function_call_in_method() (in module quark.torch.algorithm.quarot.monkeypatch)": [[138, "quark.torch.algorithm.quarot.monkeypatch.add_wrapper_after_function_call_in_method", false]], "addqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform", false]], "adjust_quantize_info() (in module quark.onnx.refine)": [[72, "quark.onnx.refine.adjust_quantize_info", false]], "algoconfig (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.AlgoConfig", false]], "algoconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AlgoConfig", false]], "algoconfigbase (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.AlgoConfigBase", false]], "algoconfigbase (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AlgoConfigBase", false]], "align_quantize_info() (in module quark.onnx.refine)": [[72, "quark.onnx.refine.align_quantize_info", false]], "allow_exported_model_train_eval() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.allow_exported_model_train_eval", false]], "allow_multi_consumers (quark.onnx.graph_transformations.transforms.transform property)": [[33, "quark.onnx.graph_transformations.transforms.Transform.allow_multi_consumers", false]], "apply() (quark.onnx.graph_transformations.transforms_pipeline.transformspipeline method)": [[34, "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline.apply", false]], "apply() (quark.onnx.optimizations.convert_transforms_pipeline.convertqdqtoqoptransformspipeline method)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.ConvertQDQToQOPTransformsPipeline.apply", false]], "apply() (quark.onnx.optimizations.convert_transforms_pipeline.removeqdqtransformspipeline method)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.RemoveQDQTransformsPipeline.apply", false]], "assembleidxs (class in quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.AssembleIdxs", false]], "augment_graph() (quark.onnx.calibrate.powoftwocalibrater method)": [[3, "quark.onnx.calibrate.PowOfTwoCalibrater.augment_graph", false]], "auto_mixprecision() (in module quark.onnx.mprecision.auto_mixprecision)": [[36, "quark.onnx.mprecision.auto_mixprecision.auto_mixprecision", false]], "autosmoothquantconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AutoSmoothQuantConfig", false]], "autosmoothquantprocessor (class in quark.torch.algorithm.awq.auto_smooth)": [[121, "quark.torch.algorithm.awq.auto_smooth.AutoSmoothQuantProcessor", false]], "average_l2() (in module quark.onnx.finetuning.onnx_evaluate)": [[20, "quark.onnx.finetuning.onnx_evaluate.average_L2", false]], "awqconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AWQConfig", false]], "awqprocessor (class in quark.torch.algorithm.awq.awq)": [[122, "quark.torch.algorithm.awq.awq.AwqProcessor", false]], "backend (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.Backend", false]], "barplot() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.barplot", false]], "basealgoprocessor (class in quark.torch.algorithm.processor)": [[136, "quark.torch.algorithm.processor.BaseAlgoProcessor", false]], "basevocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.BaseVocab", false]], "batchnorm_ops (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.BATCHNORM_OPS", false]], "bfloat16spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Bfloat16Spec", false]], "bfp16spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.BFP16Spec", false]], "bfpquantizer (class in quark.onnx.finetuning.create_torch.base_fn_quantizers)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPQuantizer", false]], "biascorrection (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.BiasCorrection", false]], "blockwisetuningconfig (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.BlockwiseTuningConfig", false]], "blockwisetuningprocessor (class in quark.torch.algorithm.blockwise_tuning.blockwise_tuning)": [[128, "quark.torch.algorithm.blockwise_tuning.blockwise_tuning.BlockwiseTuningProcessor", false]], "bpevocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.BpeVocab", false]], "buildin_eval_func() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.buildin_eval_func", false]], "cacheddatareader (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.CachedDataReader", false]], "cacheddataset (class in quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.CachedDataset", false]], "calc_recon_loss() (quark.onnx.finetuning.train_torch.train_model_loss.trainloss static method)": [[26, "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss.calc_recon_loss", false]], "calc_round_loss() (quark.onnx.finetuning.train_torch.train_model_loss.trainloss class method)": [[26, "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss.calc_round_loss", false]], "calculate_qparams() (quark.torch.quantization.observer.observer.pergroupminmaxobserver method)": [[241, "quark.torch.quantization.observer.observer.PerGroupMinMaxObserver.calculate_qparams", false]], "calculate_qparams() (quark.torch.quantization.observer.observer.uniformscalingobserver method)": [[241, "quark.torch.quantization.observer.observer.UniformScalingObserver.calculate_qparams", false]], "calculate_quantization_params() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.calculate_quantization_params", false]], "call() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassbase method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase.call", false]], "call() (quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.convertclip2reluqopass method)": [[215, "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass.call", false]], "call() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertbn2d2convqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass.call", false]], "call() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertreducemean2gapqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass.call", false]], "call() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.splitquantmodulecalledoveronce method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce.call", false]], "cat_ops (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.CAT_OPS", false]], "check_hard_sigmoid_condition() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_hard_sigmoid_condition", false]], "check_min_max_valid() (in module quark.torch.quantization.nn.utils)": [[238, "quark.torch.quantization.nn.utils.check_min_max_valid", false]], "check_model_quantizable() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_model_quantizable", false]], "check_onnx_model() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_onnx_model", false]], "check_reduce_mean_condition() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_reduce_mean_condition", false]], "check_relu_like_node() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_relu_like_node", false]], "cle_pair_type (class in quark.onnx.equalization)": [[5, "quark.onnx.equalization.CLE_PAIR_TYPE", false]], "cle_transforms() (in module quark.onnx.equalization)": [[5, "quark.onnx.equalization.cle_transforms", false]], "collect_quantization_statistics() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.collect_quantization_statistics", false]], "compute_range() (quark.onnx.calibrate.powoftwocalibrater method)": [[3, "quark.onnx.calibrate.PowOfTwoCalibrater.compute_range", false]], "compute_scale_zp() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.compute_scale_zp", false]], "compute_scale_zp_fp() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.compute_scale_zp_fp", false]], "config (class in quark.onnx.quantization.config.config)": [[57, "quark.onnx.quantization.config.config.Config", false]], "config (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.Config", false]], "config (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.Config", false]], "config (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Config", false]], "configbase (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.ConfigBase", false]], "configbase (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.ConfigBase", false]], "configverifier (class in quark.torch.extensions.brevitas.verification)": [[186, "quark.torch.extensions.brevitas.verification.ConfigVerifier", false]], "convert_act() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_act", false]], "convert_bn_to_conv() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_bn_to_conv", false]], "convert_clip_to_relu() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_clip_to_relu", false]], "convert_conv() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_conv", false]], "convert_customqdq_to_qdq() (in module quark.onnx.tools.convert_customqdq_to_qdq)": [[78, "quark.onnx.tools.convert_customqdq_to_qdq.convert_customqdq_to_qdq", false]], "convert_exported_model_to_gguf() (in module quark.torch.export.gguf_export.api)": [[154, "quark.torch.export.gguf_export.api.convert_exported_model_to_gguf", false]], "convert_float16_to_float() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_float16_to_float", false]], "convert_float_to_float16() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_float_to_float16", false]], "convert_float_to_float16_model_path() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_float_to_float16_model_path", false]], "convert_gemm() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_gemm", false]], "convert_initializers_to_float() (in module quark.onnx.tools.convert_quant_to_float)": [[92, "quark.onnx.tools.convert_quant_to_float.convert_initializers_to_float", false]], "convert_lstm_to_customlstm() (in module quark.onnx.tools.convert_lstm_to_customlstm)": [[86, "quark.onnx.tools.convert_lstm_to_customlstm.convert_lstm_to_customlstm", false]], "convert_matmul() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_matmul", false]], "convert_norm() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_norm", false]], "convert_np_to_float() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_np_to_float", false]], "convert_np_to_float16() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_np_to_float16", false]], "convert_onnx_to_torch() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.convert_onnx_to_torch", false]], "convert_ops_to_modules() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_ops_to_modules", false]], "convert_qdq_to_qop() (in module quark.onnx.tools.convert_qdq_to_qop)": [[91, "quark.onnx.tools.convert_qdq_to_qop.convert_qdq_to_qop", false]], "convert_reduce_mean_to_global_avg_pool() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_reduce_mean_to_global_avg_pool", false]], "convert_split_to_slice() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_split_to_slice", false]], "convert_tensor_float16_to_float() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_tensor_float16_to_float", false]], "convert_tensor_float_to_float16() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_tensor_float_to_float16", false]], "convert_torch_to_onnx() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.convert_torch_to_onnx", false]], "convertbn2d2convqopass (class in quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass", false]], "convertclip2reluqopass (class in quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant)": [[215, "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass", false]], "convertqdqtoqoptransformspipeline (class in quark.onnx.optimizations.convert_transforms_pipeline)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.ConvertQDQToQOPTransformsPipeline", false]], "convertreducemean2gapqopass (class in quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass", false]], "convqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform", false]], "copy_func_with_new_globals() (in module quark.torch.algorithm.quarot.monkeypatch)": [[138, "quark.torch.algorithm.quarot.monkeypatch.copy_func_with_new_globals", false]], "cos_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.cos_metric", false]], "create_calibrator_float_scale() (in module quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.create_calibrator_float_scale", false]], "create_calibrator_power_of_two() (in module quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.create_calibrator_power_of_two", false]], "create_session() (in module quark.onnx.finetuning.onnx_evaluate)": [[20, "quark.onnx.finetuning.onnx_evaluate.create_session", false]], "custom_ops_infer_shapes() (in module quark.onnx.tools.convert_customqdq_to_qdq)": [[78, "quark.onnx.tools.convert_customqdq_to_qdq.custom_ops_infer_shapes", false]], "custom_ops_infer_shapes() (in module quark.onnx.tools.convert_lstm_to_customlstm)": [[86, "quark.onnx.tools.convert_lstm_to_customlstm.custom_ops_infer_shapes", false]], "customformatter (class in quark.shares.utils.log)": [[118, "quark.shares.utils.log.CustomFormatter", false]], "customqdq_to_contribqdq() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.customqdq_to_contribqdq", false]], "dataclass_pretty_string() (in module quark.torch.quantization.config.utils)": [[203, "quark.torch.quantization.config.utils.dataclass_pretty_string", false]], "datatypespec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.DataTypeSpec", false]], "delete_directory_content() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.delete_directory_content", false]], "dequantize_data() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.dequantize_data", false]], "devicetype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.DeviceType", false]], "distribution_plot() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.distribution_plot", false]], "dpu_leaky_relu_alpha() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.dpu_leaky_relu_alpha", false]], "dtype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.Dtype", false]], "dump_model() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.dump_model", false]], "duplicatefilter (class in quark.shares.utils.log)": [[118, "quark.shares.utils.log.DuplicateFilter", false]], "embeddingtype (class in quark.torch.export.json_export.builder.llm_info)": [[162, "quark.torch.export.json_export.builder.llm_info.EmbeddingType", false]], "ensures() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassbase method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase.ensures", false]], "entropycalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.EntropyCalibrater", false]], "equalization (class in quark.onnx.equalization)": [[5, "quark.onnx.equalization.Equalization", false]], "export_gguf_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.export_gguf_model", false]], "export_onnx_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.export_onnx_model", false]], "export_onnx_model() (quark.torch.extensions.brevitas.api.modelexporter method)": [[182, "quark.torch.extensions.brevitas.api.ModelExporter.export_onnx_model", false]], "export_quark_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.export_quark_model", false]], "exporterconfig (class in quark.torch.export.config.config)": [[151, "quark.torch.export.config.config.ExporterConfig", false]], "extract_attr_values() (in module quark.onnx.finetuning.create_torch.create_model_utils)": [[11, "quark.onnx.finetuning.create_torch.create_model_utils.extract_attr_values", false]], "extract_padding_params() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.extract_padding_params", false]], "extract_padding_params_for_conv() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.extract_padding_params_for_conv", false]], "extract_weight_and_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.extract_weight_and_bias", false]], "fakequantizebase (class in quark.torch.quantization.tensor_quantize)": [[243, "quark.torch.quantization.tensor_quantize.FakeQuantizeBase", false]], "fast_finetune() (in module quark.onnx.finetuning.fast_finetune)": [[18, "quark.onnx.finetuning.fast_finetune.fast_finetune", false]], "filter() (quark.shares.utils.log.duplicatefilter method)": [[118, "quark.shares.utils.log.DuplicateFilter.filter", false]], "find_int16_scale() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.find_int16_scale", false]], "find_node_by_output() (in module quark.onnx.tools.remove_qdq_between_ops)": [[107, "quark.onnx.tools.remove_qdq_between_ops.find_node_by_output", false]], "find_node_by_output() (in module quark.onnx.tools.remove_qdq_mul_add)": [[108, "quark.onnx.tools.remove_qdq_mul_add.find_node_by_output", false]], "find_quant_scale_zp() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.find_quant_scale_zp", false]], "find_quantized_value() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.find_quantized_value", false]], "float16spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Float16Spec", false]], "fold_batch_norm() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fold_batch_norm", false]], "fold_batch_norm_after_concat() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fold_batch_norm_after_concat", false]], "format() (quark.shares.utils.log.customformatter method)": [[118, "quark.shares.utils.log.CustomFormatter.format", false]], "forward() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.forward", false]], "forward() (quark.torch.algorithm.rotation.rotation_utils.rmsnorm method)": [[144, "quark.torch.algorithm.rotation.rotation_utils.RMSNorm.forward", false]], "forward() (quark.torch.quantization.observer.observer.pertensorhistogramobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorHistogramObserver.forward", false]], "forward() (quark.torch.quantization.observer.observer.pertensorminmaxobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorMinMaxObserver.forward", false]], "fp8e4m3perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E4M3PerChannelSpec", false]], "fp8e4m3pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E4M3PerGroupSpec", false]], "fp8e4m3pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E4M3PerTensorSpec", false]], "fp8e5m2perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E5M2PerChannelSpec", false]], "fp8e5m2pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E5M2PerGroupSpec", false]], "fp8e5m2pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E5M2PerTensorSpec", false]], "fpquantizer (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.FPQuantizer", false]], "freeze() (quark.torch.quantization.api.modelquantizer static method)": [[198, "quark.torch.quantization.api.ModelQuantizer.freeze", false]], "freeze_model() (in module quark.torch.quantization.graph.processor.processor)": [[226, "quark.torch.quantization.graph.processor.processor.freeze_model", false]], "fuse_instance_norm() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fuse_instance_norm", false]], "fuse_l2_norm() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fuse_l2_norm", false]], "generate_initializer() (in module quark.onnx.utils.model_utils)": [[114, "quark.onnx.utils.model_utils.generate_initializer", false]], "generate_input_initializer() (in module quark.onnx.graph_transformations.model_transformer_test)": [[32, "quark.onnx.graph_transformations.model_transformer_test.generate_input_initializer", false]], "get_annotate_tensors() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_annotate_tensors", false]], "get_bias() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.get_bias", false]], "get_clip_min_max() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_clip_min_max", false]], "get_configs() (quark.onnx.graph_transformations.transforms_pipeline.transformspipeline method)": [[34, "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline.get_configs", false]], "get_datatype_shape() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_datatype_shape", false]], "get_exclude_nodes() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_exclude_nodes", false]], "get_export_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.get_export_model", false]], "get_fix_position() (quark.torch.quantization.observer.tqt_observer.tqtobserver method)": [[242, "quark.torch.quantization.observer.tqt_observer.TQTObserver.get_fix_position", false]], "get_hadamard_matrices() (in module quark.torch.algorithm.rotation.hadamard)": [[141, "quark.torch.algorithm.rotation.hadamard.get_hadamard_matrices", false]], "get_min_max_by_mse() (quark.torch.quantization.observer.observer.pertensormseobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorMSEObserver.get_min_max_by_mse", false]], "get_min_max_by_percentile() (quark.torch.quantization.observer.observer.pertensorpercentileobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorPercentileObserver.get_min_max_by_percentile", false]], "get_modules_optimized_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.get_modules_optimized_bias", false]], "get_modules_optimized_weight() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.get_modules_optimized_weight", false]], "get_nested_attr_from_module() (in module quark.torch.algorithm.utils.module)": [[147, "quark.torch.algorithm.utils.module.get_nested_attr_from_module", false]], "get_next() (quark.onnx.quant_utils.cacheddatareader method)": [[55, "quark.onnx.quant_utils.CachedDataReader.get_next", false]], "get_next() (quark.onnx.quant_utils.pathdatareader method)": [[55, "quark.onnx.quant_utils.PathDataReader.get_next", false]], "get_next() (quark.onnx.quant_utils.randomdatareader method)": [[55, "quark.onnx.quant_utils.RandomDataReader.get_next", false]], "get_next() (quark.onnx.tools.save_tensor_hist.histdatareader method)": [[111, "quark.onnx.tools.save_tensor_hist.HistDataReader.get_next", false]], "get_qdq_to_remove() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_qdq_to_remove", false]], "get_qmin_qmax_for_qtype() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_qmin_qmax_for_qType", false]], "get_qrange_for_qtype() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_qrange_for_qType", false]], "get_rotation_matrix() (in module quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.get_rotation_matrix", false]], "get_tensor_value() (in module quark.onnx.utils.model_utils)": [[114, "quark.onnx.utils.model_utils.get_tensor_value", false]], "get_weight() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.get_weight", false]], "gpfa2q (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.GPFA2Q", false]], "gpfq (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.GPFQ", false]], "gptq (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.GPTQ", false]], "gptqconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.GPTQConfig", false]], "gptqprocessor (class in quark.torch.algorithm.gptq.gptq)": [[131, "quark.torch.algorithm.gptq.gptq.GptqProcessor", false]], "graphtransform (class in quark.torch.quantization.graph.fx.base)": [[206, "quark.torch.quantization.graph.fx.base.GraphTransform", false]], "hadamard_multiply() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.hadamard_multiply", false]], "hadamard_transform() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.hadamard_transform", false]], "hardmard_transform() (in module quark.torch.algorithm.rotation.hadamard)": [[141, "quark.torch.algorithm.rotation.hadamard.hardmard_transform", false]], "histdatareader (class in quark.onnx.tools.save_tensor_hist)": [[111, "quark.onnx.tools.save_tensor_hist.HistDataReader", false]], "import_model() (quark.torch.export.api.modelimporter method)": [[150, "quark.torch.export.api.ModelImporter.import_model", false]], "import_model_info() (quark.torch.export.api.modelimporter method)": [[150, "quark.torch.export.api.ModelImporter.import_model_info", false]], "in_place_replace_layer() (in module quark.torch.quantization.model_transformation)": [[230, "quark.torch.quantization.model_transformation.in_place_replace_layer", false]], "infer_shape() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.infer_shape", false]], "inference_model() (in module quark.onnx.finetuning.onnx_evaluate)": [[20, "quark.onnx.finetuning.onnx_evaluate.inference_model", false]], "initialize_alpha() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.adaroundintquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer.initialize_alpha", false]], "insert_quantizer() (in module quark.torch.quantization.graph.processor.insert_quantizer)": [[225, "quark.torch.quantization.graph.processor.insert_quantizer.insert_quantizer", false]], "insert_stats_hooks() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.insert_stats_hooks", false]], "int16method (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.Int16Method", false]], "int4perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int4PerChannelSpec", false]], "int4pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int4PerGroupSpec", false]], "int4pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int4PerTensorSpec", false]], "int8perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int8PerChannelSpec", false]], "int8pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int8PerGroupSpec", false]], "int8pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int8PerTensorSpec", false]], "intquantizer (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer", false]], "is_approximately_equal() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_approximately_equal", false]], "is_batchnorm2d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_batchnorm2d_node", false]], "is_cat_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_cat_node", false]], "is_clip_with_min_max() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_clip_with_min_max", false]], "is_conv1d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_conv1d_node", false]], "is_conv2d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_conv2d_node", false]], "is_conv3d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_conv3d_node", false]], "is_convtranspose2d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_convtranspose2d_node", false]], "is_dropout_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_dropout_node", false]], "is_leaky_relu_with_alpha() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_leaky_relu_with_alpha", false]], "is_node_needs_annotated() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_node_needs_annotated", false]], "is_ort_version_below() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_ort_version_below", false]], "jsonexporterconfig (class in quark.torch.export.config.config)": [[151, "quark.torch.export.config.config.JsonExporterConfig", false]], "l1_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.l1_metric", false]], "l2_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.l2_metric", false]], "layernormtype (class in quark.torch.export.json_export.builder.llm_info)": [[162, "quark.torch.export.json_export.builder.llm_info.LayerNormType", false]], "llamamodelwriter (class in quark.torch.export.gguf_export.gguf_model_writer)": [[156, "quark.torch.export.gguf_export.gguf_model_writer.LlamaModelWriter", false]], "load_params() (in module quark.torch.quantization.api)": [[198, "quark.torch.quantization.api.load_params", false]], "load_pre_optimization_config_from_file() (in module quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.load_pre_optimization_config_from_file", false]], "load_quant_algo_config_from_file() (in module quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.load_quant_algo_config_from_file", false]], "load_weight_and_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.load_weight_and_bias", false]], "lsqobserver (class in quark.torch.quantization.observer.lsq_observer)": [[240, "quark.torch.quantization.observer.lsq_observer.LSQObserver", false]], "mark_exclude_nodes() (in module quark.torch.quantization.graph.processor.processor)": [[226, "quark.torch.quantization.graph.processor.processor.mark_exclude_nodes", false]], "matmulnbitsquantizer (class in quark.onnx.quantizers.matmul_nbits_quantizer)": [[66, "quark.onnx.quantizers.matmul_nbits_quantizer.MatMulNBitsQuantizer", false]], "matmulqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform", false]], "minmaxcalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.MinMaxCalibrater", false]], "modelexporter (class in quark.torch.export.api)": [[150, "quark.torch.export.api.ModelExporter", false]], "modelexporter (class in quark.torch.extensions.brevitas.api)": [[182, "quark.torch.extensions.brevitas.api.ModelExporter", false]], "modelimporter (class in quark.torch.export.api)": [[150, "quark.torch.export.api.ModelImporter", false]], "modeloptimizer (class in quark.onnx.finetuning.train_torch.train_model)": [[25, "quark.onnx.finetuning.train_torch.train_model.ModelOptimizer", false]], "modelpruner (class in quark.torch.pruning.api)": [[193, "quark.torch.pruning.api.ModelPruner", false]], "modelquantizer (class in quark.onnx.quantization.api)": [[56, "quark.onnx.quantization.api.ModelQuantizer", false]], "modelquantizer (class in quark.torch.extensions.brevitas.api)": [[182, "quark.torch.extensions.brevitas.api.ModelQuantizer", false]], "modelquantizer (class in quark.torch.quantization.api)": [[198, "quark.torch.quantization.api.ModelQuantizer", false]], "modeltransformer (class in quark.onnx.graph_transformations.model_transformer)": [[31, "quark.onnx.graph_transformations.model_transformer.ModelTransformer", false]], "modeltransformer.nodetype (class in quark.onnx.graph_transformations.model_transformer)": [[31, "quark.onnx.graph_transformations.model_transformer.ModelTransformer.NodeType", false]], "modeltransformertest (class in quark.onnx.graph_transformations.model_transformer_test)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest", false]], "modelwriter (class in quark.torch.export.gguf_export.gguf_model_writer)": [[156, "quark.torch.export.gguf_export.gguf_model_writer.ModelWriter", false]], "modified_annotate_input() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.modified_annotate_input", false]], "modify_reshape_param() (in module quark.torch.quantization.graph.optimization.modify_reshape_param)": [[212, "quark.torch.quantization.graph.optimization.modify_reshape_param.modify_reshape_param", false]], "module": [[0, "module-quark", false], [1, "module-quark.onnx.auto_search", false], [2, "module-quark.onnx.bias_correction", false], [3, "module-quark.onnx.calibrate", false], [4, "module-quark.onnx.cpu_quantizer", false], [5, "module-quark.onnx.equalization", false], [6, "module-quark.onnx.finetuning.create_torch.base_fn_quantizers", false], [7, "module-quark.onnx.finetuning.create_torch.base_qdq_quantizers", false], [8, "module-quark.onnx.finetuning.create_torch.create_model", false], [9, "module-quark.onnx.finetuning.create_torch.create_model_ops", false], [10, "module-quark.onnx.finetuning.create_torch.create_model_test", false], [11, "module-quark.onnx.finetuning.create_torch.create_model_utils", false], [12, "module-quark.onnx.finetuning.create_torch", false], [13, "module-quark.onnx.finetuning.create_torch.quant_base_ops", false], [14, "module-quark.onnx.finetuning.create_torch.quant_conv_ops", false], [15, "module-quark.onnx.finetuning.create_torch.quant_gemm_ops", false], [16, "module-quark.onnx.finetuning.create_torch.quant_matmul_ops", false], [17, "module-quark.onnx.finetuning.create_torch.quant_norm_ops", false], [18, "module-quark.onnx.finetuning.fast_finetune", false], [19, "module-quark.onnx.finetuning", false], [20, "module-quark.onnx.finetuning.onnx_evaluate", false], [21, "module-quark.onnx.finetuning.onnx_subgraph", false], [22, "module-quark.onnx.finetuning.torch_utils", false], [23, "module-quark.onnx.finetuning.torch_utils_test", false], [24, "module-quark.onnx.finetuning.train_torch", false], [25, "module-quark.onnx.finetuning.train_torch.train_model", false], [26, "module-quark.onnx.finetuning.train_torch.train_model_loss", false], [27, "module-quark.onnx.finetuning.train_torch.train_model_param", false], [28, "module-quark.onnx.gptq.gptq", false], [29, "module-quark.onnx.gptq", false], [30, "module-quark.onnx.graph_transformations", false], [31, "module-quark.onnx.graph_transformations.model_transformer", false], [32, "module-quark.onnx.graph_transformations.model_transformer_test", false], [33, "module-quark.onnx.graph_transformations.transforms", false], [34, "module-quark.onnx.graph_transformations.transforms_pipeline", false], [35, "module-quark.onnx", false], [36, "module-quark.onnx.mprecision.auto_mixprecision", false], [37, "module-quark.onnx.mprecision", false], [38, "module-quark.onnx.mprecision.mixed_bfp", false], [39, "module-quark.onnx.onnx_quantizer", false], [40, "module-quark.onnx.operators.custom_ops.build_vai_custom_op", false], [41, "module-quark.onnx.operators.custom_ops", false], [42, "module-quark.onnx.operators", false], [43, "module-quark.onnx.operators.vai_ops.concat", false], [44, "module-quark.onnx.operators.vai_ops.hardsigmoid", false], [45, "module-quark.onnx.operators.vai_ops", false], [46, "module-quark.onnx.operators.vai_ops.layernorm", false], [47, "module-quark.onnx.operators.vai_ops.prelu", false], [48, "module-quark.onnx.operators.vai_ops.qdq_ops", false], [49, "module-quark.onnx.operators.vai_ops.softmax", false], [50, "module-quark.onnx.optimizations.convert_transforms", false], [51, "module-quark.onnx.optimizations.convert_transforms_pipeline", false], [52, "module-quark.onnx.optimizations", false], [53, "module-quark.onnx.optimize", false], [54, "module-quark.onnx.qdq_quantizer", false], [55, "module-quark.onnx.quant_utils", false], [56, "module-quark.onnx.quantization.api", false], [57, "module-quark.onnx.quantization.config.config", false], [58, "module-quark.onnx.quantization.config.custom_config", false], [59, "module-quark.onnx.quantization.config", false], [60, "module-quark.onnx.quantization", false], [61, "module-quark.onnx.quantize", false], [62, "module-quark.onnx.quantizers.bfp_quantizer", false], [63, "module-quark.onnx.quantizers.cpu_quantizer", false], [64, "module-quark.onnx.quantizers.extended_quantizer", false], [65, "module-quark.onnx.quantizers", false], [66, "module-quark.onnx.quantizers.matmul_nbits_quantizer", false], [67, "module-quark.onnx.quantizers.npu_cnn_quantizer", false], [68, "module-quark.onnx.quantizers.npu_transformer_quantizer", false], [69, "module-quark.onnx.quantizers.onnx_quantizer", false], [70, "module-quark.onnx.quantizers.qdq_quantizer", false], [71, "module-quark.onnx.quarot", false], [72, "module-quark.onnx.refine", false], [73, "module-quark.onnx.registry", false], [74, "module-quark.onnx.simulate_dpu", false], [75, "module-quark.onnx.simulate_dpu_softmax", false], [76, "module-quark.onnx.smooth_quant", false], [77, "module-quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu", false], [78, "module-quark.onnx.tools.convert_customqdq_to_qdq", false], [79, "module-quark.onnx.tools.convert_dynamic_to_fixed", false], [80, "module-quark.onnx.tools.convert_fp16_to_bf16", false], [81, "module-quark.onnx.tools.convert_fp16_to_bfp16", false], [82, "module-quark.onnx.tools.convert_fp16_to_fp32", false], [83, "module-quark.onnx.tools.convert_fp32_to_bf16", false], [84, "module-quark.onnx.tools.convert_fp32_to_bfp16", false], [85, "module-quark.onnx.tools.convert_fp32_to_fp16", false], [86, "module-quark.onnx.tools.convert_lstm_to_customlstm", false], [87, "module-quark.onnx.tools.convert_nchw_to_nhwc", false], [88, "module-quark.onnx.tools.convert_onnx_to_onnxtxt", false], [89, "module-quark.onnx.tools.convert_onnxtxt_to_onnx", false], [90, "module-quark.onnx.tools.convert_opset_version", false], [91, "module-quark.onnx.tools.convert_qdq_to_qop", false], [92, "module-quark.onnx.tools.convert_quant_to_float", false], [93, "module-quark.onnx.tools.convert_resize_fs_to_pof2s", false], [94, "module-quark.onnx.tools.convert_s8s8_to_u8s8", false], [95, "module-quark.onnx.tools.convert_shared_initializer_to_unique", false], [96, "module-quark.onnx.tools.convert_u16s8_to_s16s8", false], [97, "module-quark.onnx.tools.convert_u16u8_to_u8u8", false], [98, "module-quark.onnx.tools.evaluate", false], [99, "module-quark.onnx.tools.float16", false], [100, "module-quark.onnx.tools", false], [101, "module-quark.onnx.tools.insert_clip_bfloat16_qdq", false], [102, "module-quark.onnx.tools.print_a16w8_a8w8_nodes", false], [103, "module-quark.onnx.tools.random_quantize", false], [104, "module-quark.onnx.tools.remove_bf16_cast", false], [105, "module-quark.onnx.tools.remove_initializer_from_input", false], [106, "module-quark.onnx.tools.remove_qdq", false], [107, "module-quark.onnx.tools.remove_qdq_between_ops", false], [108, "module-quark.onnx.tools.remove_qdq_mul_add", false], [109, "module-quark.onnx.tools.replace_bfloat16_qdq_cast", false], [110, "module-quark.onnx.tools.replace_inf_weights", false], [111, "module-quark.onnx.tools.save_tensor_hist", false], [112, "module-quark.onnx.tools.save_weights_hist", false], [113, "module-quark.onnx.utils", false], [114, "module-quark.onnx.utils.model_utils", false], [115, "module-quark.shares", false], [116, "module-quark.shares.utils.import_utils", false], [117, "module-quark.shares.utils", false], [118, "module-quark.shares.utils.log", false], [119, "module-quark.shares.utils.testing_utils", false], [120, "module-quark.torch.algorithm.api", false], [121, "module-quark.torch.algorithm.awq.auto_smooth", false], [122, "module-quark.torch.algorithm.awq.awq", false], [123, "module-quark.torch.algorithm.awq", false], [124, "module-quark.torch.algorithm.awq.modules.act", false], [125, "module-quark.torch.algorithm.awq.modules", false], [126, "module-quark.torch.algorithm.awq.scale", false], [127, "module-quark.torch.algorithm.awq.smooth", false], [128, "module-quark.torch.algorithm.blockwise_tuning.blockwise_tuning", false], [129, "module-quark.torch.algorithm.blockwise_tuning.blockwise_utils", false], [130, "module-quark.torch.algorithm.blockwise_tuning", false], [131, "module-quark.torch.algorithm.gptq.gptq", false], [132, "module-quark.torch.algorithm.gptq", false], [133, "module-quark.torch.algorithm", false], [134, "module-quark.torch.algorithm.osscar", false], [135, "module-quark.torch.algorithm.osscar.osscar", false], [136, "module-quark.torch.algorithm.processor", false], [137, "module-quark.torch.algorithm.quarot", false], [138, "module-quark.torch.algorithm.quarot.monkeypatch", false], [139, "module-quark.torch.algorithm.quarot.quarot", false], [140, "module-quark.torch.algorithm.quarot.utils", false], [141, "module-quark.torch.algorithm.rotation.hadamard", false], [142, "module-quark.torch.algorithm.rotation", false], [143, "module-quark.torch.algorithm.rotation.rotation", false], [144, "module-quark.torch.algorithm.rotation.rotation_utils", false], [145, "module-quark.torch.algorithm.utils.auto_config", false], [146, "module-quark.torch.algorithm.utils", false], [147, "module-quark.torch.algorithm.utils.module", false], [148, "module-quark.torch.algorithm.utils.prepare", false], [149, "module-quark.torch.algorithm.utils.utils", false], [150, "module-quark.torch.export.api", false], [151, "module-quark.torch.export.config.config", false], [152, "module-quark.torch.export.config", false], [153, "module-quark.torch.export.constants", false], [154, "module-quark.torch.export.gguf_export.api", false], [155, "module-quark.torch.export.gguf_export.gguf_model_converter", false], [156, "module-quark.torch.export.gguf_export.gguf_model_writer", false], [157, "module-quark.torch.export.gguf_export", false], [158, "module-quark.torch.export.gguf_export.tensor_convert", false], [159, "module-quark.torch.export.gguf_export.utils", false], [160, "module-quark.torch.export", false], [161, "module-quark.torch.export.json_export.builder", false], [162, "module-quark.torch.export.json_export.builder.llm_info", false], [163, "module-quark.torch.export.json_export.builder.llm_info_builder", false], [164, "module-quark.torch.export.json_export.builder.native_model_info_builder", false], [165, "module-quark.torch.export.json_export.converter", false], [166, "module-quark.torch.export.json_export.converter.llm_info_converter", false], [167, "module-quark.torch.export.json_export", false], [168, "module-quark.torch.export.json_export.utils", false], [169, "module-quark.torch.export.json_export.utils.utils", false], [170, "module-quark.torch.export.main_export", false], [171, "module-quark.torch.export.main_export.model_post_process", false], [172, "module-quark.torch.export.main_export.quant_config_parser", false], [173, "module-quark.torch.export.main_import", false], [174, "module-quark.torch.export.main_import.pretrained_config", false], [175, "module-quark.torch.export.nn", false], [176, "module-quark.torch.export.nn.modules", false], [177, "module-quark.torch.export.nn.modules.qparamslinear", false], [178, "module-quark.torch.export.nn.modules.realquantizer", false], [179, "module-quark.torch.export.onnx", false], [180, "module-quark.torch.export.utils", false], [181, "module-quark.torch.extensions.brevitas.algos", false], [182, "module-quark.torch.extensions.brevitas.api", false], [183, "module-quark.torch.extensions.brevitas.config", false], [184, "module-quark.torch.extensions.brevitas", false], [185, "module-quark.torch.extensions.brevitas.mapping", false], [186, "module-quark.torch.extensions.brevitas.verification", false], [187, "module-quark.torch.extensions", false], [188, "module-quark.torch", false], [189, "module-quark.torch.kernel.hw_emulation.extensions", false], [190, "module-quark.torch.kernel.hw_emulation.hw_emulation_interface", false], [191, "module-quark.torch.kernel.hw_emulation", false], [192, "module-quark.torch.kernel", false], [193, "module-quark.torch.pruning.api", false], [194, "module-quark.torch.pruning.config", false], [195, "module-quark.torch.pruning", false], [196, "module-quark.torch.pruning.model_transformation", false], [197, "module-quark.torch.pruning.utils", false], [198, "module-quark.torch.quantization.api", false], [199, "module-quark.torch.quantization.config.config", false], [200, "module-quark.torch.quantization.config.config_verification", false], [201, "module-quark.torch.quantization.config", false], [202, "module-quark.torch.quantization.config.type", false], [203, "module-quark.torch.quantization.config.utils", false], [204, "module-quark.torch.quantization.constants", false], [205, "module-quark.torch.quantization.debug", false], [206, "module-quark.torch.quantization.graph.fx.base", false], [207, "module-quark.torch.quantization.graph.fx", false], [208, "module-quark.torch.quantization.graph", false], [209, "module-quark.torch.quantization.graph.optimization.activate_dropout", false], [210, "module-quark.torch.quantization.graph.optimization", false], [211, "module-quark.torch.quantization.graph.optimization.model_optimization", false], [212, "module-quark.torch.quantization.graph.optimization.modify_reshape_param", false], [213, "module-quark.torch.quantization.graph.optimization.opt_pass_manager", false], [214, "module-quark.torch.quantization.graph.optimization.post_quant", false], [215, "module-quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant", false], [216, "module-quark.torch.quantization.graph.optimization.pre_quant", false], [217, "module-quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant", false], [218, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d", false], [219, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model", false], [220, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d", false], [221, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear", false], [222, "module-quark.torch.quantization.graph.optimization.remove_dropout_node", false], [223, "module-quark.torch.quantization.graph.optimization.utils", false], [224, "module-quark.torch.quantization.graph.processor", false], [225, "module-quark.torch.quantization.graph.processor.insert_quantizer", false], [226, "module-quark.torch.quantization.graph.processor.processor", false], [227, "module-quark.torch.quantization.graph.processor.processor_utils", false], [228, "module-quark.torch.quantization.graph.torch_utils", false], [229, "module-quark.torch.quantization", false], [230, "module-quark.torch.quantization.model_transformation", false], [231, "module-quark.torch.quantization.nn", false], [232, "module-quark.torch.quantization.nn.modules", false], [233, "module-quark.torch.quantization.nn.modules.mixin", false], [234, "module-quark.torch.quantization.nn.modules.quantize_conv", false], [235, "module-quark.torch.quantization.nn.modules.quantize_conv_bn_fused", false], [236, "module-quark.torch.quantization.nn.modules.quantize_embed", false], [237, "module-quark.torch.quantization.nn.modules.quantize_linear", false], [238, "module-quark.torch.quantization.nn.utils", false], [239, "module-quark.torch.quantization.observer", false], [240, "module-quark.torch.quantization.observer.lsq_observer", false], [241, "module-quark.torch.quantization.observer.observer", false], [242, "module-quark.torch.quantization.observer.tqt_observer", false], [243, "module-quark.torch.quantization.tensor_quantize", false], [244, "module-quark.torch.quantization.utils", false], [245, "module-quark.torch.utils", false], [246, "module-quark.torch.utils.pack", false], [247, "module-quark.version", false]], "mulqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform", false]], "mx6spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.MX6Spec", false]], "mx9spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.MX9Spec", false]], "mxquantizer (class in quark.onnx.finetuning.create_torch.base_fn_quantizers)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.MXQuantizer", false]], "mxspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.MXSpec", false]], "nodetree (class in quark.onnx.graph_transformations.transforms)": [[33, "quark.onnx.graph_transformations.transforms.NodeTree", false]], "nonscaledfakequantize (class in quark.torch.quantization.tensor_quantize)": [[243, "quark.torch.quantization.tensor_quantize.NonScaledFakeQuantize", false]], "nonscaledrealquantizer (class in quark.torch.export.nn.modules.realquantizer)": [[178, "quark.torch.export.nn.modules.realquantizer.NonScaledRealQuantizer", false]], "novocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.NoVocab", false]], "observerbase (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.ObserverBase", false]], "onnxexporterconfig (class in quark.torch.export.config.config)": [[151, "quark.torch.export.config.config.OnnxExporterConfig", false]], "onnxquantizer (class in quark.onnx.onnx_quantizer)": [[39, "quark.onnx.onnx_quantizer.ONNXQuantizer", false]], "optimize (class in quark.onnx.optimize)": [[53, "quark.onnx.optimize.Optimize", false]], "optimize() (in module quark.onnx.optimize)": [[53, "quark.onnx.optimize.optimize", false]], "optimize_module() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.optimize_module", false]], "optpassbase (class in quark.torch.quantization.graph.optimization.opt_pass_manager)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase", false]], "optpassmanager (class in quark.torch.quantization.graph.optimization.opt_pass_manager)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager", false]], "optypepattern (class in quark.onnx.graph_transformations.transforms)": [[33, "quark.onnx.graph_transformations.transforms.OpTypePattern", false]], "osscarconfig (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.OSSCARConfig", false]], "osscarprocessor (class in quark.torch.algorithm.osscar.osscar)": [[135, "quark.torch.algorithm.osscar.osscar.OsscarProcessor", false]], "param_is_symmetric() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.param_is_symmetric", false]], "paramtype (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.ParamType", false]], "parse_options_to_params() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.parse_options_to_params", false]], "pathdatareader (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.PathDataReader", false]], "pattern() (quark.onnx.graph_transformations.transforms.transform method)": [[33, "quark.onnx.graph_transformations.transforms.Transform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.addqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.convqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.matmulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.mulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.removeqdqtransform method)": [[50, "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.sigmoidqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform.pattern", false]], "perblockbfpobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerBlockBFPObserver", false]], "perblockmxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerBlockMXObserver", false]], "percentilecalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.PercentileCalibrater", false]], "perchannelminmaxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerChannelMinMaxObserver", false]], "pergroupminmaxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerGroupMinMaxObserver", false]], "pertensorhistogramobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorHistogramObserver", false]], "pertensorhistogramobserverpro (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorHistogramObserverPro", false]], "pertensorminmaxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorMinMaxObserver", false]], "pertensormseobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorMSEObserver", false]], "pertensorpercentileobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorPercentileObserver", false]], "placeholderobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PlaceholderObserver", false]], "pos2scale() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.pos2scale", false]], "pos2scale() (in module quark.onnx.tools.convert_resize_fs_to_pof2s)": [[93, "quark.onnx.tools.convert_resize_fs_to_pof2s.pos2scale", false]], "poweroftwomethod (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.PowerOfTwoMethod", false]], "powoftwocalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.PowOfTwoCalibrater", false]], "powoftwocollector (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.PowOfTwoCollector", false]], "preprocess (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.Preprocess", false]], "preprocess_import_info() (in module quark.torch.export.utils)": [[180, "quark.torch.export.utils.preprocess_import_info", false]], "prequantoptconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.PreQuantOptConfig", false]], "print_quantize_dynamic_info() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.print_quantize_dynamic_info", false]], "print_quantize_info() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.print_quantize_info", false]], "process_model_transformation() (in module quark.torch.quantization.model_transformation)": [[230, "quark.torch.quantization.model_transformation.process_model_transformation", false]], "pruning_model() (quark.torch.pruning.api.modelpruner method)": [[193, "quark.torch.pruning.api.ModelPruner.pruning_model", false]], "psnr_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.psnr_metric", false]], "qconv1d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConv1d", false]], "qconv2d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConv2d", false]], "qconv3d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConv3d", false]], "qconvtranspose1d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConvTranspose1d", false]], "qconvtranspose2d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConvTranspose2d", false]], "qconvtranspose3d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConvTranspose3d", false]], "qdqnputransformerquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.QDQNPUTransformerQuantizer", false]], "qdqquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.QDQQuantizer", false]], "qgemm (class in quark.onnx.finetuning.create_torch.quant_gemm_ops)": [[15, "quark.onnx.finetuning.create_torch.quant_gemm_ops.QGemm", false]], "qinstancenorm1d (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QInstanceNorm1d", false]], "qinstancenorm2d (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QInstanceNorm2d", false]], "qinstancenorm3d (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QInstanceNorm3d", false]], "qkrotation (class in quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.QKRotation", false]], "qlayernorm (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QLayerNorm", false]], "qmatmul (class in quark.onnx.finetuning.create_torch.quant_matmul_ops)": [[16, "quark.onnx.finetuning.create_torch.quant_matmul_ops.QMatMul", false]], "qschemetype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.QSchemeType", false]], "quantconv2d (class in quark.torch.quantization.nn.modules.quantize_conv)": [[234, "quark.torch.quantization.nn.modules.quantize_conv.QuantConv2d", false]], "quantconvtranspose2d (class in quark.torch.quantization.nn.modules.quantize_conv)": [[234, "quark.torch.quantization.nn.modules.quantize_conv.QuantConvTranspose2d", false]], "quantizationconfig (class in quark.onnx.quantization.config.config)": [[57, "quark.onnx.quantization.config.config.QuantizationConfig", false]], "quantizationconfig (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.QuantizationConfig", false]], "quantizationconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.QuantizationConfig", false]], "quantizationmode (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.QuantizationMode", false]], "quantizationmodule (class in quark.onnx.finetuning.create_torch.quant_base_ops)": [[13, "quark.onnx.finetuning.create_torch.quant_base_ops.QuantizationModule", false]], "quantizationspec (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.QuantizationSpec", false]], "quantizationspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.QuantizationSpec", false]], "quantize_bias_static() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_bias_static", false]], "quantize_data_pof2s() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.quantize_data_pof2s", false]], "quantize_dynamic() (in module quark.onnx.quantize)": [[61, "quark.onnx.quantize.quantize_dynamic", false]], "quantize_initializer() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_initializer", false]], "quantize_model() (quark.onnx.quantization.api.modelquantizer method)": [[56, "quark.onnx.quantization.api.ModelQuantizer.quantize_model", false]], "quantize_model() (quark.torch.extensions.brevitas.api.modelquantizer method)": [[182, "quark.torch.extensions.brevitas.api.ModelQuantizer.quantize_model", false]], "quantize_model() (quark.torch.quantization.api.modelquantizer method)": [[198, "quark.torch.quantization.api.ModelQuantizer.quantize_model", false]], "quantize_weight() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_weight", false]], "quantize_weight_per_channel() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_weight_per_channel", false]], "quantizedconvbatchnorm2d (class in quark.torch.quantization.nn.modules.quantize_conv_bn_fused)": [[235, "quark.torch.quantization.nn.modules.quantize_conv_bn_fused.QuantizedConvBatchNorm2d", false]], "quantizewrapper (class in quark.onnx.finetuning.create_torch.quant_base_ops)": [[13, "quark.onnx.finetuning.create_torch.quant_base_ops.QuantizeWrapper", false]], "quantlinear (class in quark.torch.quantization.nn.modules.quantize_linear)": [[237, "quark.torch.quantization.nn.modules.quantize_linear.QuantLinear", false]], "quanttype (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.QuantType", false]], "quark": [[0, "module-quark", false]], "quark.onnx": [[35, "module-quark.onnx", false]], "quark.onnx.auto_search": [[1, "module-quark.onnx.auto_search", false]], "quark.onnx.bias_correction": [[2, "module-quark.onnx.bias_correction", false]], "quark.onnx.calibrate": [[3, "module-quark.onnx.calibrate", false]], "quark.onnx.cpu_quantizer": [[4, "module-quark.onnx.cpu_quantizer", false]], "quark.onnx.equalization": [[5, "module-quark.onnx.equalization", false]], "quark.onnx.finetuning": [[19, "module-quark.onnx.finetuning", false]], "quark.onnx.finetuning.create_torch": [[12, "module-quark.onnx.finetuning.create_torch", false]], "quark.onnx.finetuning.create_torch.base_fn_quantizers": [[6, "module-quark.onnx.finetuning.create_torch.base_fn_quantizers", false]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers": [[7, "module-quark.onnx.finetuning.create_torch.base_qdq_quantizers", false]], "quark.onnx.finetuning.create_torch.create_model": [[8, "module-quark.onnx.finetuning.create_torch.create_model", false]], "quark.onnx.finetuning.create_torch.create_model_ops": [[9, "module-quark.onnx.finetuning.create_torch.create_model_ops", false]], "quark.onnx.finetuning.create_torch.create_model_test": [[10, "module-quark.onnx.finetuning.create_torch.create_model_test", false]], "quark.onnx.finetuning.create_torch.create_model_utils": [[11, "module-quark.onnx.finetuning.create_torch.create_model_utils", false]], "quark.onnx.finetuning.create_torch.quant_base_ops": [[13, "module-quark.onnx.finetuning.create_torch.quant_base_ops", false]], "quark.onnx.finetuning.create_torch.quant_conv_ops": [[14, "module-quark.onnx.finetuning.create_torch.quant_conv_ops", false]], "quark.onnx.finetuning.create_torch.quant_gemm_ops": [[15, "module-quark.onnx.finetuning.create_torch.quant_gemm_ops", false]], "quark.onnx.finetuning.create_torch.quant_matmul_ops": [[16, "module-quark.onnx.finetuning.create_torch.quant_matmul_ops", false]], "quark.onnx.finetuning.create_torch.quant_norm_ops": [[17, "module-quark.onnx.finetuning.create_torch.quant_norm_ops", false]], "quark.onnx.finetuning.fast_finetune": [[18, "module-quark.onnx.finetuning.fast_finetune", false]], "quark.onnx.finetuning.onnx_evaluate": [[20, "module-quark.onnx.finetuning.onnx_evaluate", false]], "quark.onnx.finetuning.onnx_subgraph": [[21, "module-quark.onnx.finetuning.onnx_subgraph", false]], "quark.onnx.finetuning.torch_utils": [[22, "module-quark.onnx.finetuning.torch_utils", false]], "quark.onnx.finetuning.torch_utils_test": [[23, "module-quark.onnx.finetuning.torch_utils_test", false]], "quark.onnx.finetuning.train_torch": [[24, "module-quark.onnx.finetuning.train_torch", false]], "quark.onnx.finetuning.train_torch.train_model": [[25, "module-quark.onnx.finetuning.train_torch.train_model", false]], "quark.onnx.finetuning.train_torch.train_model_loss": [[26, "module-quark.onnx.finetuning.train_torch.train_model_loss", false]], "quark.onnx.finetuning.train_torch.train_model_param": [[27, "module-quark.onnx.finetuning.train_torch.train_model_param", false]], "quark.onnx.gptq": [[29, "module-quark.onnx.gptq", false]], "quark.onnx.gptq.gptq": [[28, "module-quark.onnx.gptq.gptq", false]], "quark.onnx.graph_transformations": [[30, "module-quark.onnx.graph_transformations", false]], "quark.onnx.graph_transformations.model_transformer": [[31, "module-quark.onnx.graph_transformations.model_transformer", false]], "quark.onnx.graph_transformations.model_transformer_test": [[32, "module-quark.onnx.graph_transformations.model_transformer_test", false]], "quark.onnx.graph_transformations.transforms": [[33, "module-quark.onnx.graph_transformations.transforms", false]], "quark.onnx.graph_transformations.transforms_pipeline": [[34, "module-quark.onnx.graph_transformations.transforms_pipeline", false]], "quark.onnx.mprecision": [[37, "module-quark.onnx.mprecision", false]], "quark.onnx.mprecision.auto_mixprecision": [[36, "module-quark.onnx.mprecision.auto_mixprecision", false]], "quark.onnx.mprecision.mixed_bfp": [[38, "module-quark.onnx.mprecision.mixed_bfp", false]], "quark.onnx.onnx_quantizer": [[39, "module-quark.onnx.onnx_quantizer", false]], "quark.onnx.operators": [[42, "module-quark.onnx.operators", false]], "quark.onnx.operators.custom_ops": [[41, "module-quark.onnx.operators.custom_ops", false]], "quark.onnx.operators.custom_ops.build_vai_custom_op": [[40, "module-quark.onnx.operators.custom_ops.build_vai_custom_op", false]], "quark.onnx.operators.vai_ops": [[45, "module-quark.onnx.operators.vai_ops", false]], "quark.onnx.operators.vai_ops.concat": [[43, "module-quark.onnx.operators.vai_ops.concat", false]], "quark.onnx.operators.vai_ops.hardsigmoid": [[44, "module-quark.onnx.operators.vai_ops.hardsigmoid", false]], "quark.onnx.operators.vai_ops.layernorm": [[46, "module-quark.onnx.operators.vai_ops.layernorm", false]], "quark.onnx.operators.vai_ops.prelu": [[47, "module-quark.onnx.operators.vai_ops.prelu", false]], "quark.onnx.operators.vai_ops.qdq_ops": [[48, "module-quark.onnx.operators.vai_ops.qdq_ops", false]], "quark.onnx.operators.vai_ops.softmax": [[49, "module-quark.onnx.operators.vai_ops.softmax", false]], "quark.onnx.optimizations": [[52, "module-quark.onnx.optimizations", false]], "quark.onnx.optimizations.convert_transforms": [[50, "module-quark.onnx.optimizations.convert_transforms", false]], "quark.onnx.optimizations.convert_transforms_pipeline": [[51, "module-quark.onnx.optimizations.convert_transforms_pipeline", false]], "quark.onnx.optimize": [[53, "module-quark.onnx.optimize", false]], "quark.onnx.qdq_quantizer": [[54, "module-quark.onnx.qdq_quantizer", false]], "quark.onnx.quant_utils": [[55, "module-quark.onnx.quant_utils", false]], "quark.onnx.quantization": [[60, "module-quark.onnx.quantization", false]], "quark.onnx.quantization.api": [[56, "module-quark.onnx.quantization.api", false]], "quark.onnx.quantization.config": [[59, "module-quark.onnx.quantization.config", false]], "quark.onnx.quantization.config.config": [[57, "module-quark.onnx.quantization.config.config", false]], "quark.onnx.quantization.config.custom_config": [[58, "module-quark.onnx.quantization.config.custom_config", false]], "quark.onnx.quantize": [[61, "module-quark.onnx.quantize", false]], "quark.onnx.quantizers": [[65, "module-quark.onnx.quantizers", false]], "quark.onnx.quantizers.bfp_quantizer": [[62, "module-quark.onnx.quantizers.bfp_quantizer", false]], "quark.onnx.quantizers.cpu_quantizer": [[63, "module-quark.onnx.quantizers.cpu_quantizer", false]], "quark.onnx.quantizers.extended_quantizer": [[64, "module-quark.onnx.quantizers.extended_quantizer", false]], "quark.onnx.quantizers.matmul_nbits_quantizer": [[66, "module-quark.onnx.quantizers.matmul_nbits_quantizer", false]], "quark.onnx.quantizers.npu_cnn_quantizer": [[67, "module-quark.onnx.quantizers.npu_cnn_quantizer", false]], "quark.onnx.quantizers.npu_transformer_quantizer": [[68, "module-quark.onnx.quantizers.npu_transformer_quantizer", false]], "quark.onnx.quantizers.onnx_quantizer": [[69, "module-quark.onnx.quantizers.onnx_quantizer", false]], "quark.onnx.quantizers.qdq_quantizer": [[70, "module-quark.onnx.quantizers.qdq_quantizer", false]], "quark.onnx.quarot": [[71, "module-quark.onnx.quarot", false]], "quark.onnx.refine": [[72, "module-quark.onnx.refine", false]], "quark.onnx.registry": [[73, "module-quark.onnx.registry", false]], "quark.onnx.simulate_dpu": [[74, "module-quark.onnx.simulate_dpu", false]], "quark.onnx.simulate_dpu_softmax": [[75, "module-quark.onnx.simulate_dpu_softmax", false]], "quark.onnx.smooth_quant": [[76, "module-quark.onnx.smooth_quant", false]], "quark.onnx.tools": [[100, "module-quark.onnx.tools", false]], "quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu": [[77, "module-quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu", false]], "quark.onnx.tools.convert_customqdq_to_qdq": [[78, "module-quark.onnx.tools.convert_customqdq_to_qdq", false]], "quark.onnx.tools.convert_dynamic_to_fixed": [[79, "module-quark.onnx.tools.convert_dynamic_to_fixed", false]], "quark.onnx.tools.convert_fp16_to_bf16": [[80, "module-quark.onnx.tools.convert_fp16_to_bf16", false]], "quark.onnx.tools.convert_fp16_to_bfp16": [[81, "module-quark.onnx.tools.convert_fp16_to_bfp16", false]], "quark.onnx.tools.convert_fp16_to_fp32": [[82, "module-quark.onnx.tools.convert_fp16_to_fp32", false]], "quark.onnx.tools.convert_fp32_to_bf16": [[83, "module-quark.onnx.tools.convert_fp32_to_bf16", false]], "quark.onnx.tools.convert_fp32_to_bfp16": [[84, "module-quark.onnx.tools.convert_fp32_to_bfp16", false]], "quark.onnx.tools.convert_fp32_to_fp16": [[85, "module-quark.onnx.tools.convert_fp32_to_fp16", false]], "quark.onnx.tools.convert_lstm_to_customlstm": [[86, "module-quark.onnx.tools.convert_lstm_to_customlstm", false]], "quark.onnx.tools.convert_nchw_to_nhwc": [[87, "module-quark.onnx.tools.convert_nchw_to_nhwc", false]], "quark.onnx.tools.convert_onnx_to_onnxtxt": [[88, "module-quark.onnx.tools.convert_onnx_to_onnxtxt", false]], "quark.onnx.tools.convert_onnxtxt_to_onnx": [[89, "module-quark.onnx.tools.convert_onnxtxt_to_onnx", false]], "quark.onnx.tools.convert_opset_version": [[90, "module-quark.onnx.tools.convert_opset_version", false]], "quark.onnx.tools.convert_qdq_to_qop": [[91, "module-quark.onnx.tools.convert_qdq_to_qop", false]], "quark.onnx.tools.convert_quant_to_float": [[92, "module-quark.onnx.tools.convert_quant_to_float", false]], "quark.onnx.tools.convert_resize_fs_to_pof2s": [[93, "module-quark.onnx.tools.convert_resize_fs_to_pof2s", false]], "quark.onnx.tools.convert_s8s8_to_u8s8": [[94, "module-quark.onnx.tools.convert_s8s8_to_u8s8", false]], "quark.onnx.tools.convert_shared_initializer_to_unique": [[95, "module-quark.onnx.tools.convert_shared_initializer_to_unique", false]], "quark.onnx.tools.convert_u16s8_to_s16s8": [[96, "module-quark.onnx.tools.convert_u16s8_to_s16s8", false]], "quark.onnx.tools.convert_u16u8_to_u8u8": [[97, "module-quark.onnx.tools.convert_u16u8_to_u8u8", false]], "quark.onnx.tools.evaluate": [[98, "module-quark.onnx.tools.evaluate", false]], "quark.onnx.tools.float16": [[99, "module-quark.onnx.tools.float16", false]], "quark.onnx.tools.insert_clip_bfloat16_qdq": [[101, "module-quark.onnx.tools.insert_clip_bfloat16_qdq", false]], "quark.onnx.tools.print_a16w8_a8w8_nodes": [[102, "module-quark.onnx.tools.print_a16w8_a8w8_nodes", false]], "quark.onnx.tools.random_quantize": [[103, "module-quark.onnx.tools.random_quantize", false]], "quark.onnx.tools.remove_bf16_cast": [[104, "module-quark.onnx.tools.remove_bf16_cast", false]], "quark.onnx.tools.remove_initializer_from_input": [[105, "module-quark.onnx.tools.remove_initializer_from_input", false]], "quark.onnx.tools.remove_qdq": [[106, "module-quark.onnx.tools.remove_qdq", false]], "quark.onnx.tools.remove_qdq_between_ops": [[107, "module-quark.onnx.tools.remove_qdq_between_ops", false]], "quark.onnx.tools.remove_qdq_mul_add": [[108, "module-quark.onnx.tools.remove_qdq_mul_add", false]], "quark.onnx.tools.replace_bfloat16_qdq_cast": [[109, "module-quark.onnx.tools.replace_bfloat16_qdq_cast", false]], "quark.onnx.tools.replace_inf_weights": [[110, "module-quark.onnx.tools.replace_inf_weights", false]], "quark.onnx.tools.save_tensor_hist": [[111, "module-quark.onnx.tools.save_tensor_hist", false]], "quark.onnx.tools.save_weights_hist": [[112, "module-quark.onnx.tools.save_weights_hist", false]], "quark.onnx.utils": [[113, "module-quark.onnx.utils", false]], "quark.onnx.utils.model_utils": [[114, "module-quark.onnx.utils.model_utils", false]], "quark.shares": [[115, "module-quark.shares", false]], "quark.shares.utils": [[117, "module-quark.shares.utils", false]], "quark.shares.utils.import_utils": [[116, "module-quark.shares.utils.import_utils", false]], "quark.shares.utils.log": [[118, "module-quark.shares.utils.log", false]], "quark.shares.utils.testing_utils": [[119, "module-quark.shares.utils.testing_utils", false]], "quark.torch": [[188, "module-quark.torch", false]], "quark.torch.algorithm": [[133, "module-quark.torch.algorithm", false]], "quark.torch.algorithm.api": [[120, "module-quark.torch.algorithm.api", false]], "quark.torch.algorithm.awq": [[123, "module-quark.torch.algorithm.awq", false]], "quark.torch.algorithm.awq.auto_smooth": [[121, "module-quark.torch.algorithm.awq.auto_smooth", false]], "quark.torch.algorithm.awq.awq": [[122, "module-quark.torch.algorithm.awq.awq", false]], "quark.torch.algorithm.awq.modules": [[125, "module-quark.torch.algorithm.awq.modules", false]], "quark.torch.algorithm.awq.modules.act": [[124, "module-quark.torch.algorithm.awq.modules.act", false]], "quark.torch.algorithm.awq.scale": [[126, "module-quark.torch.algorithm.awq.scale", false]], "quark.torch.algorithm.awq.smooth": [[127, "module-quark.torch.algorithm.awq.smooth", false]], "quark.torch.algorithm.blockwise_tuning": [[130, "module-quark.torch.algorithm.blockwise_tuning", false]], "quark.torch.algorithm.blockwise_tuning.blockwise_tuning": [[128, "module-quark.torch.algorithm.blockwise_tuning.blockwise_tuning", false]], "quark.torch.algorithm.blockwise_tuning.blockwise_utils": [[129, "module-quark.torch.algorithm.blockwise_tuning.blockwise_utils", false]], "quark.torch.algorithm.gptq": [[132, "module-quark.torch.algorithm.gptq", false]], "quark.torch.algorithm.gptq.gptq": [[131, "module-quark.torch.algorithm.gptq.gptq", false]], "quark.torch.algorithm.osscar": [[134, "module-quark.torch.algorithm.osscar", false]], "quark.torch.algorithm.osscar.osscar": [[135, "module-quark.torch.algorithm.osscar.osscar", false]], "quark.torch.algorithm.processor": [[136, "module-quark.torch.algorithm.processor", false]], "quark.torch.algorithm.quarot": [[137, "module-quark.torch.algorithm.quarot", false]], "quark.torch.algorithm.quarot.monkeypatch": [[138, "module-quark.torch.algorithm.quarot.monkeypatch", false]], "quark.torch.algorithm.quarot.quarot": [[139, "module-quark.torch.algorithm.quarot.quarot", false]], "quark.torch.algorithm.quarot.utils": [[140, "module-quark.torch.algorithm.quarot.utils", false]], "quark.torch.algorithm.rotation": [[142, "module-quark.torch.algorithm.rotation", false]], "quark.torch.algorithm.rotation.hadamard": [[141, "module-quark.torch.algorithm.rotation.hadamard", false]], "quark.torch.algorithm.rotation.rotation": [[143, "module-quark.torch.algorithm.rotation.rotation", false]], "quark.torch.algorithm.rotation.rotation_utils": [[144, "module-quark.torch.algorithm.rotation.rotation_utils", false]], "quark.torch.algorithm.utils": [[146, "module-quark.torch.algorithm.utils", false]], "quark.torch.algorithm.utils.auto_config": [[145, "module-quark.torch.algorithm.utils.auto_config", false]], "quark.torch.algorithm.utils.module": [[147, "module-quark.torch.algorithm.utils.module", false]], "quark.torch.algorithm.utils.prepare": [[148, "module-quark.torch.algorithm.utils.prepare", false]], "quark.torch.algorithm.utils.utils": [[149, "module-quark.torch.algorithm.utils.utils", false]], "quark.torch.export": [[160, "module-quark.torch.export", false]], "quark.torch.export.api": [[150, "module-quark.torch.export.api", false]], "quark.torch.export.config": [[152, "module-quark.torch.export.config", false]], "quark.torch.export.config.config": [[151, "module-quark.torch.export.config.config", false]], "quark.torch.export.constants": [[153, "module-quark.torch.export.constants", false]], "quark.torch.export.gguf_export": [[157, "module-quark.torch.export.gguf_export", false]], "quark.torch.export.gguf_export.api": [[154, "module-quark.torch.export.gguf_export.api", false]], "quark.torch.export.gguf_export.gguf_model_converter": [[155, "module-quark.torch.export.gguf_export.gguf_model_converter", false]], "quark.torch.export.gguf_export.gguf_model_writer": [[156, "module-quark.torch.export.gguf_export.gguf_model_writer", false]], "quark.torch.export.gguf_export.tensor_convert": [[158, "module-quark.torch.export.gguf_export.tensor_convert", false]], "quark.torch.export.gguf_export.utils": [[159, "module-quark.torch.export.gguf_export.utils", false]], "quark.torch.export.json_export": [[167, "module-quark.torch.export.json_export", false]], "quark.torch.export.json_export.builder": [[161, "module-quark.torch.export.json_export.builder", false]], "quark.torch.export.json_export.builder.llm_info": [[162, "module-quark.torch.export.json_export.builder.llm_info", false]], "quark.torch.export.json_export.builder.llm_info_builder": [[163, "module-quark.torch.export.json_export.builder.llm_info_builder", false]], "quark.torch.export.json_export.builder.native_model_info_builder": [[164, "module-quark.torch.export.json_export.builder.native_model_info_builder", false]], "quark.torch.export.json_export.converter": [[165, "module-quark.torch.export.json_export.converter", false]], "quark.torch.export.json_export.converter.llm_info_converter": [[166, "module-quark.torch.export.json_export.converter.llm_info_converter", false]], "quark.torch.export.json_export.utils": [[168, "module-quark.torch.export.json_export.utils", false]], "quark.torch.export.json_export.utils.utils": [[169, "module-quark.torch.export.json_export.utils.utils", false]], "quark.torch.export.main_export": [[170, "module-quark.torch.export.main_export", false]], "quark.torch.export.main_export.model_post_process": [[171, "module-quark.torch.export.main_export.model_post_process", false]], "quark.torch.export.main_export.quant_config_parser": [[172, "module-quark.torch.export.main_export.quant_config_parser", false]], "quark.torch.export.main_import": [[173, "module-quark.torch.export.main_import", false]], "quark.torch.export.main_import.pretrained_config": [[174, "module-quark.torch.export.main_import.pretrained_config", false]], "quark.torch.export.nn": [[175, "module-quark.torch.export.nn", false]], "quark.torch.export.nn.modules": [[176, "module-quark.torch.export.nn.modules", false]], "quark.torch.export.nn.modules.qparamslinear": [[177, "module-quark.torch.export.nn.modules.qparamslinear", false]], "quark.torch.export.nn.modules.realquantizer": [[178, "module-quark.torch.export.nn.modules.realquantizer", false]], "quark.torch.export.onnx": [[179, "module-quark.torch.export.onnx", false]], "quark.torch.export.utils": [[180, "module-quark.torch.export.utils", false]], "quark.torch.extensions": [[187, "module-quark.torch.extensions", false]], "quark.torch.extensions.brevitas": [[184, "module-quark.torch.extensions.brevitas", false]], "quark.torch.extensions.brevitas.algos": [[181, "module-quark.torch.extensions.brevitas.algos", false]], "quark.torch.extensions.brevitas.api": [[182, "module-quark.torch.extensions.brevitas.api", false]], "quark.torch.extensions.brevitas.config": [[183, "module-quark.torch.extensions.brevitas.config", false]], "quark.torch.extensions.brevitas.mapping": [[185, "module-quark.torch.extensions.brevitas.mapping", false]], "quark.torch.extensions.brevitas.verification": [[186, "module-quark.torch.extensions.brevitas.verification", false]], "quark.torch.kernel": [[192, "module-quark.torch.kernel", false]], "quark.torch.kernel.hw_emulation": [[191, "module-quark.torch.kernel.hw_emulation", false]], "quark.torch.kernel.hw_emulation.extensions": [[189, "module-quark.torch.kernel.hw_emulation.extensions", false]], "quark.torch.kernel.hw_emulation.hw_emulation_interface": [[190, "module-quark.torch.kernel.hw_emulation.hw_emulation_interface", false]], "quark.torch.pruning": [[195, "module-quark.torch.pruning", false]], "quark.torch.pruning.api": [[193, "module-quark.torch.pruning.api", false]], "quark.torch.pruning.config": [[194, "module-quark.torch.pruning.config", false]], "quark.torch.pruning.model_transformation": [[196, "module-quark.torch.pruning.model_transformation", false]], "quark.torch.pruning.utils": [[197, "module-quark.torch.pruning.utils", false]], "quark.torch.quantization": [[229, "module-quark.torch.quantization", false]], "quark.torch.quantization.api": [[198, "module-quark.torch.quantization.api", false]], "quark.torch.quantization.config": [[201, "module-quark.torch.quantization.config", false]], "quark.torch.quantization.config.config": [[199, "module-quark.torch.quantization.config.config", false]], "quark.torch.quantization.config.config_verification": [[200, "module-quark.torch.quantization.config.config_verification", false]], "quark.torch.quantization.config.type": [[202, "module-quark.torch.quantization.config.type", false]], "quark.torch.quantization.config.utils": [[203, "module-quark.torch.quantization.config.utils", false]], "quark.torch.quantization.constants": [[204, "module-quark.torch.quantization.constants", false]], "quark.torch.quantization.debug": [[205, "module-quark.torch.quantization.debug", false]], "quark.torch.quantization.graph": [[208, "module-quark.torch.quantization.graph", false]], "quark.torch.quantization.graph.fx": [[207, "module-quark.torch.quantization.graph.fx", false]], "quark.torch.quantization.graph.fx.base": [[206, "module-quark.torch.quantization.graph.fx.base", false]], "quark.torch.quantization.graph.optimization": [[210, "module-quark.torch.quantization.graph.optimization", false]], "quark.torch.quantization.graph.optimization.activate_dropout": [[209, "module-quark.torch.quantization.graph.optimization.activate_dropout", false]], "quark.torch.quantization.graph.optimization.model_optimization": [[211, "module-quark.torch.quantization.graph.optimization.model_optimization", false]], "quark.torch.quantization.graph.optimization.modify_reshape_param": [[212, "module-quark.torch.quantization.graph.optimization.modify_reshape_param", false]], "quark.torch.quantization.graph.optimization.opt_pass_manager": [[213, "module-quark.torch.quantization.graph.optimization.opt_pass_manager", false]], "quark.torch.quantization.graph.optimization.post_quant": [[214, "module-quark.torch.quantization.graph.optimization.post_quant", false]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant": [[215, "module-quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant", false]], "quark.torch.quantization.graph.optimization.pre_quant": [[216, "module-quark.torch.quantization.graph.optimization.pre_quant", false]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant": [[217, "module-quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d": [[218, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model": [[219, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d": [[220, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear": [[221, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear", false]], "quark.torch.quantization.graph.optimization.remove_dropout_node": [[222, "module-quark.torch.quantization.graph.optimization.remove_dropout_node", false]], "quark.torch.quantization.graph.optimization.utils": [[223, "module-quark.torch.quantization.graph.optimization.utils", false]], "quark.torch.quantization.graph.processor": [[224, "module-quark.torch.quantization.graph.processor", false]], "quark.torch.quantization.graph.processor.insert_quantizer": [[225, "module-quark.torch.quantization.graph.processor.insert_quantizer", false]], "quark.torch.quantization.graph.processor.processor": [[226, "module-quark.torch.quantization.graph.processor.processor", false]], "quark.torch.quantization.graph.processor.processor_utils": [[227, "module-quark.torch.quantization.graph.processor.processor_utils", false]], "quark.torch.quantization.graph.torch_utils": [[228, "module-quark.torch.quantization.graph.torch_utils", false]], "quark.torch.quantization.model_transformation": [[230, "module-quark.torch.quantization.model_transformation", false]], "quark.torch.quantization.nn": [[231, "module-quark.torch.quantization.nn", false]], "quark.torch.quantization.nn.modules": [[232, "module-quark.torch.quantization.nn.modules", false]], "quark.torch.quantization.nn.modules.mixin": [[233, "module-quark.torch.quantization.nn.modules.mixin", false]], "quark.torch.quantization.nn.modules.quantize_conv": [[234, "module-quark.torch.quantization.nn.modules.quantize_conv", false]], "quark.torch.quantization.nn.modules.quantize_conv_bn_fused": [[235, "module-quark.torch.quantization.nn.modules.quantize_conv_bn_fused", false]], "quark.torch.quantization.nn.modules.quantize_embed": [[236, "module-quark.torch.quantization.nn.modules.quantize_embed", false]], "quark.torch.quantization.nn.modules.quantize_linear": [[237, "module-quark.torch.quantization.nn.modules.quantize_linear", false]], "quark.torch.quantization.nn.utils": [[238, "module-quark.torch.quantization.nn.utils", false]], "quark.torch.quantization.observer": [[239, "module-quark.torch.quantization.observer", false]], "quark.torch.quantization.observer.lsq_observer": [[240, "module-quark.torch.quantization.observer.lsq_observer", false]], "quark.torch.quantization.observer.observer": [[241, "module-quark.torch.quantization.observer.observer", false]], "quark.torch.quantization.observer.tqt_observer": [[242, "module-quark.torch.quantization.observer.tqt_observer", false]], "quark.torch.quantization.tensor_quantize": [[243, "module-quark.torch.quantization.tensor_quantize", false]], "quark.torch.quantization.utils": [[244, "module-quark.torch.quantization.utils", false]], "quark.torch.utils": [[245, "module-quark.torch.utils", false]], "quark.torch.utils.pack": [[246, "module-quark.torch.utils.pack", false]], "quark.version": [[247, "module-quark.version", false]], "quarot (class in quark.onnx.quarot)": [[71, "quark.onnx.quarot.QuaRot", false]], "quarotconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.QuaRotConfig", false]], "r4wrapper (class in quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.R4Wrapper", false]], "random_hadamard_matrix() (in module quark.torch.algorithm.rotation.hadamard)": [[141, "quark.torch.algorithm.rotation.hadamard.random_hadamard_matrix", false]], "randomdatareader (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.RandomDataReader", false]], "realquantizerbase (class in quark.torch.export.nn.modules.realquantizer)": [[178, "quark.torch.export.nn.modules.realquantizer.RealQuantizerBase", false]], "remove_initializers() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.remove_initializers", false]], "remove_nodes() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.remove_nodes", false]], "remove_qdq() (in module quark.onnx.tools.remove_qdq)": [[106, "quark.onnx.tools.remove_qdq.remove_qdq", false]], "remove_qdq_between_ops() (in module quark.onnx.tools.remove_qdq_between_ops)": [[107, "quark.onnx.tools.remove_qdq_between_ops.remove_qdq_between_ops", false]], "remove_qdq_mul_add() (in module quark.onnx.tools.remove_qdq_mul_add)": [[108, "quark.onnx.tools.remove_qdq_mul_add.remove_qdq_mul_add", false]], "removedropoutnode (class in quark.torch.quantization.graph.optimization.remove_dropout_node)": [[222, "quark.torch.quantization.graph.optimization.remove_dropout_node.RemoveDropoutNode", false]], "removeqdqtransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform", false]], "removeqdqtransformspipeline (class in quark.onnx.optimizations.convert_transforms_pipeline)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.RemoveQDQTransformsPipeline", false]], "replace_conv2d_qtconv2d() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d)": [[218, "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d.replace_conv2d_qtconv2d", false]], "replace_conv2dbn_quantizedconv_module() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model)": [[219, "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model.replace_conv2dbn_quantizedconv_module", false]], "replace_convtranspose2d_qtconvtranspose2d() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d)": [[220, "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d.replace_convtranspose2d_qtconvtranspose2d", false]], "replace_inf_in_onnx_weights() (in module quark.onnx.tools.replace_inf_weights)": [[110, "quark.onnx.tools.replace_inf_weights.replace_inf_in_onnx_weights", false]], "replace_linear_qtlinear() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear)": [[221, "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear.replace_linear_qtlinear", false]], "replacement() (quark.onnx.graph_transformations.transforms.transform method)": [[33, "quark.onnx.graph_transformations.transforms.Transform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.addqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.convqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.matmulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.mulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.removeqdqtransform method)": [[50, "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.sigmoidqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform.replacement", false]], "require_accelerate() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.require_accelerate", false]], "require_torch_gpu() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.require_torch_gpu", false]], "requires() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassbase method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase.requires", false]], "requires() (quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.convertclip2reluqopass method)": [[215, "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass.requires", false]], "requires() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertbn2d2convqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass.requires", false]], "requires() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertreducemean2gapqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass.requires", false]], "requires() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.splitquantmodulecalledoveronce method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce.requires", false]], "reset_iter() (quark.onnx.quant_utils.cacheddatareader method)": [[55, "quark.onnx.quant_utils.CachedDataReader.reset_iter", false]], "reset_min_max_vals() (quark.torch.quantization.observer.observer.uniformscalingobserver method)": [[241, "quark.torch.quantization.observer.observer.UniformScalingObserver.reset_min_max_vals", false]], "reset_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.reset_model", false]], "retry_flaky_test() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.retry_flaky_test", false]], "rmsnorm (class in quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.RMSNorm", false]], "rotate_in_channels() (in module quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.rotate_in_channels", false]], "rotate_in_channels() (quark.onnx.quarot.quarot method)": [[71, "quark.onnx.quarot.QuaRot.rotate_in_channels", false]], "rotate_in_channels2() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.rotate_in_channels2", false]], "rotate_out_channels() (in module quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.rotate_out_channels", false]], "rotate_out_channels() (quark.onnx.quarot.quarot method)": [[71, "quark.onnx.quarot.QuaRot.rotate_out_channels", false]], "rotate_out_channels2() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.rotate_out_channels2", false]], "rotationconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.RotationConfig", false]], "rotationprocessor (class in quark.torch.algorithm.rotation.rotation)": [[143, "quark.torch.algorithm.rotation.rotation.RotationProcessor", false]], "round_impl() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.adaroundintquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer.round_impl", false]], "round_impl() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer.round_impl", false]], "roundtype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.RoundType", false]], "run() (quark.onnx.auto_search.assembleidxs method)": [[1, "quark.onnx.auto_search.AssembleIdxs.run", false]], "run() (quark.onnx.finetuning.train_torch.train_model.modeloptimizer class method)": [[25, "quark.onnx.finetuning.train_torch.train_model.ModelOptimizer.run", false]], "run_onnx_model() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.run_onnx_model", false]], "save_distribution_histogram() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.save_distribution_histogram", false]], "save_model() (in module quark.onnx.utils.model_utils)": [[114, "quark.onnx.utils.model_utils.save_model", false]], "save_params() (in module quark.torch.export.api)": [[150, "quark.torch.export.api.save_params", false]], "save_torch_model() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.save_torch_model", false]], "scale2pos() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.scale2pos", false]], "scale2pos() (in module quark.onnx.tools.convert_resize_fs_to_pof2s)": [[93, "quark.onnx.tools.convert_resize_fs_to_pof2s.scale2pos", false]], "scaledfakequantize (class in quark.torch.quantization.tensor_quantize)": [[243, "quark.torch.quantization.tensor_quantize.ScaledFakeQuantize", false]], "scaledrealquantizer (class in quark.torch.export.nn.modules.realquantizer)": [[178, "quark.torch.export.nn.modules.realquantizer.ScaledRealQuantizer", false]], "scaletype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.ScaleType", false]], "search_forward() (quark.onnx.auto_search.assembleidxs method)": [[1, "quark.onnx.auto_search.AssembleIdxs.search_forward", false]], "searchspace (class in quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.SearchSpace", false]], "sentencepiecetokentypes (class in quark.torch.export.gguf_export.gguf_model_writer)": [[156, "quark.torch.export.gguf_export.gguf_model_writer.SentencePieceTokenTypes", false]], "set_algo_config() (quark.torch.quantization.config.config.config method)": [[199, "quark.torch.quantization.config.config.Config.set_algo_config", false]], "set_bias() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.set_bias", false]], "set_modules_original_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.set_modules_original_bias", false]], "set_modules_original_weight() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.set_modules_original_weight", false]], "set_op_by_name() (in module quark.torch.quantization.utils)": [[244, "quark.torch.quantization.utils.set_op_by_name", false]], "set_weight() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.set_weight", false]], "setup_config_per_layer() (in module quark.torch.quantization.model_transformation)": [[230, "quark.torch.quantization.model_transformation.setup_config_per_layer", false]], "setup_seed() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.setup_seed", false]], "sigmoidqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform", false]], "simulate_transforms() (in module quark.onnx.simulate_dpu)": [[74, "quark.onnx.simulate_dpu.simulate_transforms", false]], "smoothquant (class in quark.onnx.smooth_quant)": [[76, "quark.onnx.smooth_quant.SmoothQuant", false]], "smoothquantconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.SmoothQuantConfig", false]], "smoothquantprocessor (class in quark.torch.algorithm.awq.smooth)": [[127, "quark.torch.algorithm.awq.smooth.SmoothQuantProcessor", false]], "split_large_kernel_pool() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.split_large_kernel_pool", false]], "split_model_info() (in module quark.torch.export.json_export.utils.utils)": [[169, "quark.torch.export.json_export.utils.utils.split_model_info", false]], "split_params_for_dbrxexperts() (in module quark.torch.export.utils)": [[180, "quark.torch.export.utils.split_params_for_DbrxExperts", false]], "splitquantmodulecalledoveronce (class in quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce", false]], "ssim_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.ssim_metric", false]], "subgraph (class in quark.onnx.finetuning.onnx_subgraph)": [[21, "quark.onnx.finetuning.onnx_subgraph.Subgraph", false]], "summarize_activation() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.summarize_activation", false]], "summarize_weight() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.summarize_weight", false]], "t_exponent() (in module quark.torch.quantization.utils)": [[244, "quark.torch.quantization.utils.t_exponent", false]], "tensor_sync() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer.tensor_sync", false]], "three_level_spaces() (quark.onnx.auto_search.searchspace method)": [[1, "quark.onnx.auto_search.SearchSpace.three_level_spaces", false]], "to_real_quantize_params() (quark.torch.export.nn.modules.realquantizer.nonscaledrealquantizer method)": [[178, "quark.torch.export.nn.modules.realquantizer.NonScaledRealQuantizer.to_real_quantize_params", false]], "to_real_quantize_params() (quark.torch.export.nn.modules.realquantizer.scaledrealquantizer method)": [[178, "quark.torch.export.nn.modules.realquantizer.ScaledRealQuantizer.to_real_quantize_params", false]], "torchmodel (class in quark.onnx.finetuning.create_torch.create_model)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel", false]], "tqtobserver (class in quark.torch.quantization.observer.tqt_observer)": [[242, "quark.torch.quantization.observer.tqt_observer.TQTObserver", false]], "tqtspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.TQTSpec", false]], "tqtthresholdinitmeth (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.TQTThresholdInitMeth", false]], "train_torch_module_api() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.train_torch_module_api", false]], "trainloss (class in quark.onnx.finetuning.train_torch.train_model_loss)": [[26, "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss", false]], "trainparameters (class in quark.onnx.finetuning.train_torch.train_model_param)": [[27, "quark.onnx.finetuning.train_torch.train_model_param.TrainParameters", false]], "trans_opsfunc_2_quant_module() (in module quark.torch.quantization.graph.optimization.model_optimization)": [[211, "quark.torch.quantization.graph.optimization.model_optimization.trans_opsfunc_2_quant_module", false]], "transform (class in quark.onnx.graph_transformations.transforms)": [[33, "quark.onnx.graph_transformations.transforms.Transform", false]], "transform (class in quark.torch.quantization.graph.fx.base)": [[206, "quark.torch.quantization.graph.fx.base.Transform", false]], "transform() (quark.onnx.graph_transformations.model_transformer.modeltransformer method)": [[31, "quark.onnx.graph_transformations.model_transformer.ModelTransformer.transform", false]], "transform_for_annotation() (in module quark.torch.quantization.graph.processor.processor)": [[226, "quark.torch.quantization.graph.processor.processor.transform_for_annotation", false]], "transformspipeline (class in quark.onnx.graph_transformations.transforms_pipeline)": [[34, "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline", false]], "uint4perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint4PerChannelSpec", false]], "uint4pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint4PerGroupSpec", false]], "uint4pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint4PerTensorSpec", false]], "uint8perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint8PerChannelSpec", false]], "uint8pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint8PerGroupSpec", false]], "uint8pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint8PerTensorSpec", false]], "uniformscalingobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.UniformScalingObserver", false]], "update_buffer() (quark.torch.quantization.tensor_quantize.fakequantizebase method)": [[243, "quark.torch.quantization.tensor_quantize.FakeQuantizeBase.update_buffer", false]], "vitisbfpquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisBFPQuantizer", false]], "vitisextendedquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisExtendedQuantizer", false]], "vitisonnxquantizer (class in quark.onnx.onnx_quantizer)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer", false]], "vitisqdqcpuquantizer (class in quark.onnx.cpu_quantizer)": [[4, "quark.onnx.cpu_quantizer.VitisQDQCPUQuantizer", false]], "vitisqdqcpuquantizer (class in quark.onnx.quantizers.cpu_quantizer)": [[63, "quark.onnx.quantizers.cpu_quantizer.VitisQDQCPUQuantizer", false]], "vitisqdqnpucnnquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisQDQNPUCNNQuantizer", false]], "vitisqdqquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisQDQQuantizer", false]], "vitisquantformat (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.VitisQuantFormat", false]], "vitisquanttype (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.VitisQuantType", false]], "vocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.Vocab", false]], "weight_stats_hook() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.weight_stats_hook", false]], "zeropointtype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.ZeroPointType", false]]}, "objects": {"": [[0, 0, 0, "-", "quark"]], "quark": [[35, 0, 0, "-", "onnx"], [115, 0, 0, "-", "shares"], [188, 0, 0, "-", "torch"], [247, 0, 0, "-", "version"]], "quark.onnx": [[1, 0, 0, "-", "auto_search"], [2, 0, 0, "-", "bias_correction"], [3, 0, 0, "-", "calibrate"], [4, 0, 0, "-", "cpu_quantizer"], [5, 0, 0, "-", "equalization"], [19, 0, 0, "-", "finetuning"], [29, 0, 0, "-", "gptq"], [30, 0, 0, "-", "graph_transformations"], [37, 0, 0, "-", "mprecision"], [39, 0, 0, "-", "onnx_quantizer"], [42, 0, 0, "-", "operators"], [52, 0, 0, "-", "optimizations"], [53, 0, 0, "-", "optimize"], [54, 0, 0, "-", "qdq_quantizer"], [55, 0, 0, "-", "quant_utils"], [60, 0, 0, "-", "quantization"], [61, 0, 0, "-", "quantize"], [65, 0, 0, "-", "quantizers"], [71, 0, 0, "-", "quarot"], [72, 0, 0, "-", "refine"], [73, 0, 0, "-", "registry"], [74, 0, 0, "-", "simulate_dpu"], [75, 0, 0, "-", "simulate_dpu_softmax"], [76, 0, 0, "-", "smooth_quant"], [100, 0, 0, "-", "tools"], [113, 0, 0, "-", "utils"]], "quark.onnx.auto_search": [[1, 1, 1, "", "AssembleIdxs"], [1, 1, 1, "", "SearchSpace"], [1, 3, 1, "", "buildin_eval_func"], [1, 3, 1, "", "cos_metric"], [1, 3, 1, "", "l1_metric"], [1, 3, 1, "", "l2_metric"], [1, 3, 1, "", "psnr_metric"], [1, 3, 1, "", "ssim_metric"]], "quark.onnx.auto_search.AssembleIdxs": [[1, 2, 1, "", "run"], [1, 2, 1, "", "search_forward"]], "quark.onnx.auto_search.SearchSpace": [[1, 2, 1, "", "three_level_spaces"]], "quark.onnx.calibrate": [[3, 1, 1, "", "EntropyCalibrater"], [3, 1, 1, "", "MinMaxCalibrater"], [3, 1, 1, "", "PercentileCalibrater"], [3, 1, 1, "", "PowOfTwoCalibrater"], [3, 1, 1, "", "PowOfTwoCollector"], [3, 3, 1, "", "create_calibrator_float_scale"], [3, 3, 1, "", "create_calibrator_power_of_two"]], "quark.onnx.calibrate.PowOfTwoCalibrater": [[3, 2, 1, "", "augment_graph"], [3, 2, 1, "", "compute_range"]], "quark.onnx.cpu_quantizer": [[4, 1, 1, "", "VitisQDQCPUQuantizer"]], "quark.onnx.equalization": [[5, 1, 1, "", "CLE_PAIR_TYPE"], [5, 1, 1, "", "Equalization"], [5, 3, 1, "", "cle_transforms"]], "quark.onnx.finetuning": [[12, 0, 0, "-", "create_torch"], [18, 0, 0, "-", "fast_finetune"], [20, 0, 0, "-", "onnx_evaluate"], [21, 0, 0, "-", "onnx_subgraph"], [22, 0, 0, "-", "torch_utils"], [23, 0, 0, "-", "torch_utils_test"], [24, 0, 0, "-", "train_torch"]], "quark.onnx.finetuning.create_torch": [[6, 0, 0, "-", "base_fn_quantizers"], [7, 0, 0, "-", "base_qdq_quantizers"], [8, 0, 0, "-", "create_model"], [9, 0, 0, "-", "create_model_ops"], [10, 0, 0, "-", "create_model_test"], [11, 0, 0, "-", "create_model_utils"], [13, 0, 0, "-", "quant_base_ops"], [14, 0, 0, "-", "quant_conv_ops"], [15, 0, 0, "-", "quant_gemm_ops"], [16, 0, 0, "-", "quant_matmul_ops"], [17, 0, 0, "-", "quant_norm_ops"]], "quark.onnx.finetuning.create_torch.base_fn_quantizers": [[6, 1, 1, "", "BFPQuantizer"], [6, 1, 1, "", "MXQuantizer"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers": [[7, 1, 1, "", "AdaroundConstants"], [7, 1, 1, "", "AdaroundINTQuantizer"], [7, 1, 1, "", "FPQuantizer"], [7, 1, 1, "", "INTQuantizer"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer": [[7, 2, 1, "", "initialize_alpha"], [7, 2, 1, "", "round_impl"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer": [[7, 2, 1, "", "round_impl"], [7, 2, 1, "", "tensor_sync"]], "quark.onnx.finetuning.create_torch.create_model": [[8, 1, 1, "", "TorchModel"]], "quark.onnx.finetuning.create_torch.create_model.TorchModel": [[8, 2, 1, "", "forward"], [8, 2, 1, "", "get_bias"], [8, 2, 1, "", "get_weight"], [8, 2, 1, "", "set_bias"], [8, 2, 1, "", "set_weight"]], "quark.onnx.finetuning.create_torch.create_model_ops": [[9, 3, 1, "", "convert_act"], [9, 3, 1, "", "convert_conv"], [9, 3, 1, "", "convert_gemm"], [9, 3, 1, "", "convert_matmul"], [9, 3, 1, "", "convert_norm"], [9, 3, 1, "", "convert_ops_to_modules"], [9, 3, 1, "", "extract_padding_params"], [9, 3, 1, "", "extract_padding_params_for_conv"], [9, 3, 1, "", "extract_weight_and_bias"], [9, 3, 1, "", "get_modules_optimized_bias"], [9, 3, 1, "", "get_modules_optimized_weight"], [9, 3, 1, "", "load_weight_and_bias"], [9, 3, 1, "", "param_is_symmetric"], [9, 3, 1, "", "set_modules_original_bias"], [9, 3, 1, "", "set_modules_original_weight"]], "quark.onnx.finetuning.create_torch.create_model_utils": [[11, 3, 1, "", "extract_attr_values"]], "quark.onnx.finetuning.create_torch.quant_base_ops": [[13, 1, 1, "", "QuantizationModule"], [13, 1, 1, "", "QuantizeWrapper"]], "quark.onnx.finetuning.create_torch.quant_conv_ops": [[14, 1, 1, "", "QConv1d"], [14, 1, 1, "", "QConv2d"], [14, 1, 1, "", "QConv3d"], [14, 1, 1, "", "QConvTranspose1d"], [14, 1, 1, "", "QConvTranspose2d"], [14, 1, 1, "", "QConvTranspose3d"]], "quark.onnx.finetuning.create_torch.quant_gemm_ops": [[15, 1, 1, "", "QGemm"]], "quark.onnx.finetuning.create_torch.quant_matmul_ops": [[16, 1, 1, "", "QMatMul"]], "quark.onnx.finetuning.create_torch.quant_norm_ops": [[17, 1, 1, "", "QInstanceNorm1d"], [17, 1, 1, "", "QInstanceNorm2d"], [17, 1, 1, "", "QInstanceNorm3d"], [17, 1, 1, "", "QLayerNorm"]], "quark.onnx.finetuning.fast_finetune": [[18, 3, 1, "", "fast_finetune"]], "quark.onnx.finetuning.onnx_evaluate": [[20, 3, 1, "", "average_L2"], [20, 3, 1, "", "create_session"], [20, 3, 1, "", "inference_model"]], "quark.onnx.finetuning.onnx_subgraph": [[21, 1, 1, "", "Subgraph"]], "quark.onnx.finetuning.torch_utils": [[22, 1, 1, "", "CachedDataset"], [22, 3, 1, "", "convert_onnx_to_torch"], [22, 3, 1, "", "convert_torch_to_onnx"], [22, 3, 1, "", "optimize_module"], [22, 3, 1, "", "parse_options_to_params"], [22, 3, 1, "", "save_torch_model"], [22, 3, 1, "", "setup_seed"], [22, 3, 1, "", "train_torch_module_api"]], "quark.onnx.finetuning.train_torch": [[25, 0, 0, "-", "train_model"], [26, 0, 0, "-", "train_model_loss"], [27, 0, 0, "-", "train_model_param"]], "quark.onnx.finetuning.train_torch.train_model": [[25, 1, 1, "", "ModelOptimizer"]], "quark.onnx.finetuning.train_torch.train_model.ModelOptimizer": [[25, 2, 1, "", "run"]], "quark.onnx.finetuning.train_torch.train_model_loss": [[26, 1, 1, "", "TrainLoss"]], "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss": [[26, 2, 1, "", "calc_recon_loss"], [26, 2, 1, "", "calc_round_loss"]], "quark.onnx.finetuning.train_torch.train_model_param": [[27, 1, 1, "", "TrainParameters"]], "quark.onnx.gptq": [[28, 0, 0, "-", "gptq"]], "quark.onnx.graph_transformations": [[31, 0, 0, "-", "model_transformer"], [32, 0, 0, "-", "model_transformer_test"], [33, 0, 0, "-", "transforms"], [34, 0, 0, "-", "transforms_pipeline"]], "quark.onnx.graph_transformations.model_transformer": [[31, 1, 1, "", "ModelTransformer"]], "quark.onnx.graph_transformations.model_transformer.ModelTransformer": [[31, 1, 1, "", "NodeType"], [31, 2, 1, "", "transform"]], "quark.onnx.graph_transformations.model_transformer_test": [[32, 1, 1, "", "ModelTransformerTest"], [32, 3, 1, "", "generate_input_initializer"]], "quark.onnx.graph_transformations.transforms": [[33, 1, 1, "", "NodeTree"], [33, 1, 1, "", "OpTypePattern"], [33, 1, 1, "", "Transform"]], "quark.onnx.graph_transformations.transforms.Transform": [[33, 4, 1, "", "allow_multi_consumers"], [33, 2, 1, "", "pattern"], [33, 2, 1, "", "replacement"]], "quark.onnx.graph_transformations.transforms_pipeline": [[34, 1, 1, "", "TransformsPipeline"]], "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline": [[34, 2, 1, "", "apply"], [34, 2, 1, "", "get_configs"]], "quark.onnx.mprecision": [[36, 0, 0, "-", "auto_mixprecision"], [38, 0, 0, "-", "mixed_bfp"]], "quark.onnx.mprecision.auto_mixprecision": [[36, 3, 1, "", "auto_mixprecision"]], "quark.onnx.onnx_quantizer": [[39, 1, 1, "", "ONNXQuantizer"], [39, 1, 1, "", "VitisONNXQuantizer"]], "quark.onnx.onnx_quantizer.VitisONNXQuantizer": [[39, 2, 1, "", "calculate_quantization_params"], [39, 2, 1, "", "find_quant_scale_zp"], [39, 2, 1, "", "find_quantized_value"], [39, 2, 1, "", "quantize_bias_static"], [39, 2, 1, "", "quantize_initializer"], [39, 2, 1, "", "quantize_weight"], [39, 2, 1, "", "quantize_weight_per_channel"]], "quark.onnx.operators": [[41, 0, 0, "-", "custom_ops"], [45, 0, 0, "-", "vai_ops"]], "quark.onnx.operators.custom_ops": [[40, 0, 0, "-", "build_vai_custom_op"]], "quark.onnx.operators.vai_ops": [[43, 0, 0, "-", "concat"], [44, 0, 0, "-", "hardsigmoid"], [46, 0, 0, "-", "layernorm"], [47, 0, 0, "-", "prelu"], [48, 0, 0, "-", "qdq_ops"], [49, 0, 0, "-", "softmax"]], "quark.onnx.optimizations": [[50, 0, 0, "-", "convert_transforms"], [51, 0, 0, "-", "convert_transforms_pipeline"]], "quark.onnx.optimizations.convert_transforms": [[50, 1, 1, "", "AddQDQToQOPTransform"], [50, 1, 1, "", "ConvQDQToQOPTransform"], [50, 1, 1, "", "MatMulQDQToQOPTransform"], [50, 1, 1, "", "MulQDQToQOPTransform"], [50, 1, 1, "", "RemoveQDQTransform"], [50, 1, 1, "", "SigmoidQDQToQOPTransform"]], "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms_pipeline": [[51, 1, 1, "", "ConvertQDQToQOPTransformsPipeline"], [51, 1, 1, "", "RemoveQDQTransformsPipeline"]], "quark.onnx.optimizations.convert_transforms_pipeline.ConvertQDQToQOPTransformsPipeline": [[51, 2, 1, "", "apply"]], "quark.onnx.optimizations.convert_transforms_pipeline.RemoveQDQTransformsPipeline": [[51, 2, 1, "", "apply"]], "quark.onnx.optimize": [[53, 1, 1, "", "Optimize"], [53, 3, 1, "", "optimize"]], "quark.onnx.optimize.Optimize": [[53, 2, 1, "", "convert_bn_to_conv"], [53, 2, 1, "", "convert_clip_to_relu"], [53, 2, 1, "", "convert_reduce_mean_to_global_avg_pool"], [53, 2, 1, "", "convert_split_to_slice"], [53, 2, 1, "", "fold_batch_norm"], [53, 2, 1, "", "fold_batch_norm_after_concat"], [53, 2, 1, "", "fuse_instance_norm"], [53, 2, 1, "", "fuse_l2_norm"], [53, 2, 1, "", "split_large_kernel_pool"]], "quark.onnx.qdq_quantizer": [[54, 1, 1, "", "QDQNPUTransformerQuantizer"], [54, 1, 1, "", "QDQQuantizer"], [54, 1, 1, "", "VitisBFPQuantizer"], [54, 1, 1, "", "VitisExtendedQuantizer"], [54, 1, 1, "", "VitisQDQNPUCNNQuantizer"], [54, 1, 1, "", "VitisQDQQuantizer"]], "quark.onnx.quant_utils": [[55, 1, 1, "", "CachedDataReader"], [55, 1, 1, "", "Int16Method"], [55, 1, 1, "", "PathDataReader"], [55, 1, 1, "", "PowerOfTwoMethod"], [55, 1, 1, "", "RandomDataReader"], [55, 1, 1, "", "VitisQuantFormat"], [55, 1, 1, "", "VitisQuantType"], [55, 3, 1, "", "check_hard_sigmoid_condition"], [55, 3, 1, "", "check_model_quantizable"], [55, 3, 1, "", "check_onnx_model"], [55, 3, 1, "", "check_reduce_mean_condition"], [55, 3, 1, "", "check_relu_like_node"], [55, 3, 1, "", "compute_scale_zp"], [55, 3, 1, "", "compute_scale_zp_fp"], [55, 3, 1, "", "customqdq_to_contribqdq"], [55, 3, 1, "", "dequantize_data"], [55, 3, 1, "", "dpu_leaky_relu_alpha"], [55, 3, 1, "", "dump_model"], [55, 3, 1, "", "find_int16_scale"], [55, 3, 1, "", "get_annotate_tensors"], [55, 3, 1, "", "get_clip_min_max"], [55, 3, 1, "", "get_datatype_shape"], [55, 3, 1, "", "get_exclude_nodes"], [55, 3, 1, "", "get_qdq_to_remove"], [55, 3, 1, "", "get_qmin_qmax_for_qType"], [55, 3, 1, "", "get_qrange_for_qType"], [55, 3, 1, "", "infer_shape"], [55, 3, 1, "", "is_approximately_equal"], [55, 3, 1, "", "is_clip_with_min_max"], [55, 3, 1, "", "is_leaky_relu_with_alpha"], [55, 3, 1, "", "is_node_needs_annotated"], [55, 3, 1, "", "is_ort_version_below"], [55, 3, 1, "", "modified_annotate_input"], [55, 3, 1, "", "pos2scale"], [55, 3, 1, "", "print_quantize_dynamic_info"], [55, 3, 1, "", "print_quantize_info"], [55, 3, 1, "", "quantize_data_pof2s"], [55, 3, 1, "", "remove_initializers"], [55, 3, 1, "", "remove_nodes"], [55, 3, 1, "", "run_onnx_model"], [55, 3, 1, "", "scale2pos"]], "quark.onnx.quant_utils.CachedDataReader": [[55, 2, 1, "", "get_next"], [55, 2, 1, "", "reset_iter"]], "quark.onnx.quant_utils.PathDataReader": [[55, 2, 1, "", "get_next"]], "quark.onnx.quant_utils.RandomDataReader": [[55, 2, 1, "", "get_next"]], "quark.onnx.quantization": [[56, 0, 0, "-", "api"], [59, 0, 0, "-", "config"]], "quark.onnx.quantization.api": [[56, 1, 1, "", "ModelQuantizer"]], "quark.onnx.quantization.api.ModelQuantizer": [[56, 2, 1, "", "quantize_model"]], "quark.onnx.quantization.config": [[57, 0, 0, "-", "config"], [58, 0, 0, "-", "custom_config"]], "quark.onnx.quantization.config.config": [[57, 1, 1, "", "Config"], [57, 1, 1, "", "QuantizationConfig"]], "quark.onnx.quantize": [[61, 3, 1, "", "quantize_dynamic"]], "quark.onnx.quantizers": [[62, 0, 0, "-", "bfp_quantizer"], [63, 0, 0, "-", "cpu_quantizer"], [64, 0, 0, "-", "extended_quantizer"], [66, 0, 0, "-", "matmul_nbits_quantizer"], [67, 0, 0, "-", "npu_cnn_quantizer"], [68, 0, 0, "-", "npu_transformer_quantizer"], [69, 0, 0, "-", "onnx_quantizer"], [70, 0, 0, "-", "qdq_quantizer"]], "quark.onnx.quantizers.cpu_quantizer": [[63, 1, 1, "", "VitisQDQCPUQuantizer"]], "quark.onnx.quantizers.matmul_nbits_quantizer": [[66, 1, 1, "", "MatMulNBitsQuantizer"]], "quark.onnx.quarot": [[71, 1, 1, "", "QuaRot"]], "quark.onnx.quarot.QuaRot": [[71, 2, 1, "", "rotate_in_channels"], [71, 2, 1, "", "rotate_out_channels"]], "quark.onnx.refine": [[72, 3, 1, "", "adjust_quantize_info"], [72, 3, 1, "", "align_quantize_info"]], "quark.onnx.simulate_dpu": [[74, 3, 1, "", "simulate_transforms"]], "quark.onnx.smooth_quant": [[76, 1, 1, "", "SmoothQuant"]], "quark.onnx.tools": [[77, 0, 0, "-", "convert_a8w8_npu_to_a8w8_cpu"], [78, 0, 0, "-", "convert_customqdq_to_qdq"], [79, 0, 0, "-", "convert_dynamic_to_fixed"], [80, 0, 0, "-", "convert_fp16_to_bf16"], [81, 0, 0, "-", "convert_fp16_to_bfp16"], [82, 0, 0, "-", "convert_fp16_to_fp32"], [83, 0, 0, "-", "convert_fp32_to_bf16"], [84, 0, 0, "-", "convert_fp32_to_bfp16"], [85, 0, 0, "-", "convert_fp32_to_fp16"], [86, 0, 0, "-", "convert_lstm_to_customlstm"], [87, 0, 0, "-", "convert_nchw_to_nhwc"], [88, 0, 0, "-", "convert_onnx_to_onnxtxt"], [89, 0, 0, "-", "convert_onnxtxt_to_onnx"], [90, 0, 0, "-", "convert_opset_version"], [91, 0, 0, "-", "convert_qdq_to_qop"], [92, 0, 0, "-", "convert_quant_to_float"], [93, 0, 0, "-", "convert_resize_fs_to_pof2s"], [94, 0, 0, "-", "convert_s8s8_to_u8s8"], [95, 0, 0, "-", "convert_shared_initializer_to_unique"], [96, 0, 0, "-", "convert_u16s8_to_s16s8"], [97, 0, 0, "-", "convert_u16u8_to_u8u8"], [98, 0, 0, "-", "evaluate"], [99, 0, 0, "-", "float16"], [101, 0, 0, "-", "insert_clip_bfloat16_qdq"], [102, 0, 0, "-", "print_a16w8_a8w8_nodes"], [103, 0, 0, "-", "random_quantize"], [104, 0, 0, "-", "remove_bf16_cast"], [105, 0, 0, "-", "remove_initializer_from_input"], [106, 0, 0, "-", "remove_qdq"], [107, 0, 0, "-", "remove_qdq_between_ops"], [108, 0, 0, "-", "remove_qdq_mul_add"], [109, 0, 0, "-", "replace_bfloat16_qdq_cast"], [110, 0, 0, "-", "replace_inf_weights"], [111, 0, 0, "-", "save_tensor_hist"], [112, 0, 0, "-", "save_weights_hist"]], "quark.onnx.tools.convert_customqdq_to_qdq": [[78, 3, 1, "", "convert_customqdq_to_qdq"], [78, 3, 1, "", "custom_ops_infer_shapes"]], "quark.onnx.tools.convert_lstm_to_customlstm": [[86, 3, 1, "", "convert_lstm_to_customlstm"], [86, 3, 1, "", "custom_ops_infer_shapes"]], "quark.onnx.tools.convert_qdq_to_qop": [[91, 3, 1, "", "convert_qdq_to_qop"]], "quark.onnx.tools.convert_quant_to_float": [[92, 3, 1, "", "convert_initializers_to_float"]], "quark.onnx.tools.convert_resize_fs_to_pof2s": [[93, 3, 1, "", "pos2scale"], [93, 3, 1, "", "scale2pos"]], "quark.onnx.tools.float16": [[99, 3, 1, "", "convert_float16_to_float"], [99, 3, 1, "", "convert_float_to_float16"], [99, 3, 1, "", "convert_float_to_float16_model_path"], [99, 3, 1, "", "convert_np_to_float"], [99, 3, 1, "", "convert_np_to_float16"], [99, 3, 1, "", "convert_tensor_float16_to_float"], [99, 3, 1, "", "convert_tensor_float_to_float16"]], "quark.onnx.tools.remove_qdq": [[106, 3, 1, "", "remove_qdq"]], "quark.onnx.tools.remove_qdq_between_ops": [[107, 3, 1, "", "find_node_by_output"], [107, 3, 1, "", "remove_qdq_between_ops"]], "quark.onnx.tools.remove_qdq_mul_add": [[108, 3, 1, "", "find_node_by_output"], [108, 3, 1, "", "remove_qdq_mul_add"]], "quark.onnx.tools.replace_inf_weights": [[110, 3, 1, "", "replace_inf_in_onnx_weights"]], "quark.onnx.tools.save_tensor_hist": [[111, 1, 1, "", "HistDataReader"]], "quark.onnx.tools.save_tensor_hist.HistDataReader": [[111, 2, 1, "", "get_next"]], "quark.onnx.utils": [[114, 0, 0, "-", "model_utils"]], "quark.onnx.utils.model_utils": [[114, 3, 1, "", "generate_initializer"], [114, 3, 1, "", "get_tensor_value"], [114, 3, 1, "", "save_model"]], "quark.shares": [[117, 0, 0, "-", "utils"]], "quark.shares.utils": [[116, 0, 0, "-", "import_utils"], [118, 0, 0, "-", "log"], [119, 0, 0, "-", "testing_utils"]], "quark.shares.utils.log": [[118, 1, 1, "", "CustomFormatter"], [118, 1, 1, "", "DuplicateFilter"]], "quark.shares.utils.log.CustomFormatter": [[118, 2, 1, "", "format"]], "quark.shares.utils.log.DuplicateFilter": [[118, 2, 1, "", "filter"]], "quark.shares.utils.testing_utils": [[119, 3, 1, "", "delete_directory_content"], [119, 3, 1, "", "require_accelerate"], [119, 3, 1, "", "require_torch_gpu"], [119, 3, 1, "", "retry_flaky_test"]], "quark.torch": [[133, 0, 0, "-", "algorithm"], [160, 0, 0, "-", "export"], [187, 0, 0, "-", "extensions"], [192, 0, 0, "-", "kernel"], [195, 0, 0, "-", "pruning"], [229, 0, 0, "-", "quantization"], [245, 0, 0, "-", "utils"]], "quark.torch.algorithm": [[120, 0, 0, "-", "api"], [123, 0, 0, "-", "awq"], [130, 0, 0, "-", "blockwise_tuning"], [132, 0, 0, "-", "gptq"], [134, 0, 0, "-", "osscar"], [136, 0, 0, "-", "processor"], [137, 0, 0, "-", "quarot"], [142, 0, 0, "-", "rotation"], [146, 0, 0, "-", "utils"]], "quark.torch.algorithm.awq": [[121, 0, 0, "-", "auto_smooth"], [122, 0, 0, "-", "awq"], [125, 0, 0, "-", "modules"], [126, 0, 0, "-", "scale"], [127, 0, 0, "-", "smooth"]], "quark.torch.algorithm.awq.auto_smooth": [[121, 1, 1, "", "AutoSmoothQuantProcessor"]], "quark.torch.algorithm.awq.awq": [[122, 1, 1, "", "AwqProcessor"]], "quark.torch.algorithm.awq.modules": [[124, 0, 0, "-", "act"]], "quark.torch.algorithm.awq.smooth": [[127, 1, 1, "", "SmoothQuantProcessor"]], "quark.torch.algorithm.blockwise_tuning": [[128, 0, 0, "-", "blockwise_tuning"], [129, 0, 0, "-", "blockwise_utils"]], "quark.torch.algorithm.blockwise_tuning.blockwise_tuning": [[128, 1, 1, "", "BlockwiseTuningProcessor"]], "quark.torch.algorithm.gptq": [[131, 0, 0, "-", "gptq"]], "quark.torch.algorithm.gptq.gptq": [[131, 1, 1, "", "GptqProcessor"]], "quark.torch.algorithm.osscar": [[135, 0, 0, "-", "osscar"]], "quark.torch.algorithm.osscar.osscar": [[135, 1, 1, "", "OsscarProcessor"]], "quark.torch.algorithm.processor": [[136, 1, 1, "", "BaseAlgoProcessor"]], "quark.torch.algorithm.quarot": [[138, 0, 0, "-", "monkeypatch"], [139, 0, 0, "-", "quarot"], [140, 0, 0, "-", "utils"]], "quark.torch.algorithm.quarot.monkeypatch": [[138, 3, 1, "", "add_wrapper_after_function_call_in_method"], [138, 3, 1, "", "copy_func_with_new_globals"]], "quark.torch.algorithm.quarot.utils": [[140, 1, 1, "", "QKRotation"], [140, 1, 1, "", "R4Wrapper"], [140, 3, 1, "", "add_qk_rotation_after_function_call_in_forward"], [140, 3, 1, "", "hadamard_multiply"], [140, 3, 1, "", "hadamard_transform"], [140, 3, 1, "", "rotate_in_channels2"], [140, 3, 1, "", "rotate_out_channels2"]], "quark.torch.algorithm.rotation": [[141, 0, 0, "-", "hadamard"], [143, 0, 0, "-", "rotation"], [144, 0, 0, "-", "rotation_utils"]], "quark.torch.algorithm.rotation.hadamard": [[141, 3, 1, "", "get_hadamard_matrices"], [141, 3, 1, "", "hardmard_transform"], [141, 3, 1, "", "random_hadamard_matrix"]], "quark.torch.algorithm.rotation.rotation": [[143, 1, 1, "", "RotationProcessor"]], "quark.torch.algorithm.rotation.rotation_utils": [[144, 1, 1, "", "RMSNorm"], [144, 3, 1, "", "get_rotation_matrix"], [144, 3, 1, "", "rotate_in_channels"], [144, 3, 1, "", "rotate_out_channels"]], "quark.torch.algorithm.rotation.rotation_utils.RMSNorm": [[144, 2, 1, "", "forward"]], "quark.torch.algorithm.utils": [[145, 0, 0, "-", "auto_config"], [147, 0, 0, "-", "module"], [148, 0, 0, "-", "prepare"], [149, 0, 0, "-", "utils"]], "quark.torch.algorithm.utils.module": [[147, 3, 1, "", "get_nested_attr_from_module"]], "quark.torch.export": [[150, 0, 0, "-", "api"], [152, 0, 0, "-", "config"], [153, 0, 0, "-", "constants"], [157, 0, 0, "-", "gguf_export"], [167, 0, 0, "-", "json_export"], [170, 0, 0, "-", "main_export"], [173, 0, 0, "-", "main_import"], [175, 0, 0, "-", "nn"], [179, 0, 0, "-", "onnx"], [180, 0, 0, "-", "utils"]], "quark.torch.export.api": [[150, 1, 1, "", "ModelExporter"], [150, 1, 1, "", "ModelImporter"], [150, 3, 1, "", "save_params"]], "quark.torch.export.api.ModelExporter": [[150, 2, 1, "", "export_gguf_model"], [150, 2, 1, "", "export_onnx_model"], [150, 2, 1, "", "export_quark_model"], [150, 2, 1, "", "get_export_model"], [150, 2, 1, "", "reset_model"]], "quark.torch.export.api.ModelImporter": [[150, 2, 1, "", "import_model"], [150, 2, 1, "", "import_model_info"]], "quark.torch.export.config": [[151, 0, 0, "-", "config"]], "quark.torch.export.config.config": [[151, 1, 1, "", "ExporterConfig"], [151, 1, 1, "", "JsonExporterConfig"], [151, 1, 1, "", "OnnxExporterConfig"]], "quark.torch.export.gguf_export": [[154, 0, 0, "-", "api"], [155, 0, 0, "-", "gguf_model_converter"], [156, 0, 0, "-", "gguf_model_writer"], [158, 0, 0, "-", "tensor_convert"], [159, 0, 0, "-", "utils"]], "quark.torch.export.gguf_export.api": [[154, 3, 1, "", "convert_exported_model_to_gguf"]], "quark.torch.export.gguf_export.gguf_model_writer": [[156, 1, 1, "", "LlamaModelWriter"], [156, 1, 1, "", "ModelWriter"], [156, 1, 1, "", "SentencePieceTokenTypes"]], "quark.torch.export.gguf_export.utils": [[159, 1, 1, "", "BaseVocab"], [159, 1, 1, "", "BpeVocab"], [159, 1, 1, "", "NoVocab"], [159, 1, 1, "", "Vocab"]], "quark.torch.export.json_export": [[161, 0, 0, "-", "builder"], [165, 0, 0, "-", "converter"], [168, 0, 0, "-", "utils"]], "quark.torch.export.json_export.builder": [[162, 0, 0, "-", "llm_info"], [163, 0, 0, "-", "llm_info_builder"], [164, 0, 0, "-", "native_model_info_builder"]], "quark.torch.export.json_export.builder.llm_info": [[162, 1, 1, "", "EmbeddingType"], [162, 1, 1, "", "LayerNormType"]], "quark.torch.export.json_export.converter": [[166, 0, 0, "-", "llm_info_converter"]], "quark.torch.export.json_export.utils": [[169, 0, 0, "-", "utils"]], "quark.torch.export.json_export.utils.utils": [[169, 3, 1, "", "split_model_info"]], "quark.torch.export.main_export": [[171, 0, 0, "-", "model_post_process"], [172, 0, 0, "-", "quant_config_parser"]], "quark.torch.export.main_import": [[174, 0, 0, "-", "pretrained_config"]], "quark.torch.export.nn": [[176, 0, 0, "-", "modules"]], "quark.torch.export.nn.modules": [[177, 0, 0, "-", "qparamslinear"], [178, 0, 0, "-", "realquantizer"]], "quark.torch.export.nn.modules.realquantizer": [[178, 1, 1, "", "NonScaledRealQuantizer"], [178, 1, 1, "", "RealQuantizerBase"], [178, 1, 1, "", "ScaledRealQuantizer"]], "quark.torch.export.nn.modules.realquantizer.NonScaledRealQuantizer": [[178, 2, 1, "", "to_real_quantize_params"]], "quark.torch.export.nn.modules.realquantizer.ScaledRealQuantizer": [[178, 2, 1, "", "to_real_quantize_params"]], "quark.torch.export.utils": [[180, 3, 1, "", "preprocess_import_info"], [180, 3, 1, "", "split_params_for_DbrxExperts"]], "quark.torch.extensions": [[184, 0, 0, "-", "brevitas"]], "quark.torch.extensions.brevitas": [[181, 0, 0, "-", "algos"], [182, 0, 0, "-", "api"], [183, 0, 0, "-", "config"], [185, 0, 0, "-", "mapping"], [186, 0, 0, "-", "verification"]], "quark.torch.extensions.brevitas.algos": [[181, 1, 1, "", "ActivationEqualization"], [181, 1, 1, "", "BiasCorrection"], [181, 1, 1, "", "GPFA2Q"], [181, 1, 1, "", "GPFQ"], [181, 1, 1, "", "GPTQ"], [181, 1, 1, "", "Preprocess"]], "quark.torch.extensions.brevitas.api": [[182, 1, 1, "", "ModelExporter"], [182, 1, 1, "", "ModelQuantizer"]], "quark.torch.extensions.brevitas.api.ModelExporter": [[182, 2, 1, "", "export_onnx_model"]], "quark.torch.extensions.brevitas.api.ModelQuantizer": [[182, 2, 1, "", "quantize_model"]], "quark.torch.extensions.brevitas.config": [[183, 1, 1, "", "Backend"], [183, 1, 1, "", "Config"], [183, 1, 1, "", "ParamType"], [183, 1, 1, "", "QuantType"], [183, 1, 1, "", "QuantizationConfig"], [183, 1, 1, "", "QuantizationSpec"]], "quark.torch.extensions.brevitas.verification": [[186, 1, 1, "", "ConfigVerifier"]], "quark.torch.kernel": [[191, 0, 0, "-", "hw_emulation"]], "quark.torch.kernel.hw_emulation": [[189, 0, 0, "-", "extensions"], [190, 0, 0, "-", "hw_emulation_interface"]], "quark.torch.pruning": [[193, 0, 0, "-", "api"], [194, 0, 0, "-", "config"], [196, 0, 0, "-", "model_transformation"], [197, 0, 0, "-", "utils"]], "quark.torch.pruning.api": [[193, 1, 1, "", "ModelPruner"]], "quark.torch.pruning.api.ModelPruner": [[193, 2, 1, "", "pruning_model"]], "quark.torch.pruning.config": [[194, 1, 1, "", "AlgoConfig"], [194, 1, 1, "", "AlgoConfigBase"], [194, 1, 1, "", "BlockwiseTuningConfig"], [194, 1, 1, "", "Config"], [194, 1, 1, "", "ConfigBase"], [194, 1, 1, "", "OSSCARConfig"]], "quark.torch.quantization": [[198, 0, 0, "-", "api"], [201, 0, 0, "-", "config"], [204, 0, 0, "-", "constants"], [205, 0, 0, "-", "debug"], [208, 0, 0, "-", "graph"], [230, 0, 0, "-", "model_transformation"], [231, 0, 0, "-", "nn"], [239, 0, 0, "-", "observer"], [243, 0, 0, "-", "tensor_quantize"], [244, 0, 0, "-", "utils"]], "quark.torch.quantization.api": [[198, 1, 1, "", "ModelQuantizer"], [198, 3, 1, "", "load_params"]], "quark.torch.quantization.api.ModelQuantizer": [[198, 2, 1, "", "freeze"], [198, 2, 1, "", "quantize_model"]], "quark.torch.quantization.config": [[199, 0, 0, "-", "config"], [200, 0, 0, "-", "config_verification"], [202, 0, 0, "-", "type"], [203, 0, 0, "-", "utils"]], "quark.torch.quantization.config.config": [[199, 1, 1, "", "AWQConfig"], [199, 1, 1, "", "AlgoConfig"], [199, 1, 1, "", "AlgoConfigBase"], [199, 1, 1, "", "AutoSmoothQuantConfig"], [199, 1, 1, "", "BFP16Spec"], [199, 1, 1, "", "Bfloat16Spec"], [199, 1, 1, "", "Config"], [199, 1, 1, "", "ConfigBase"], [199, 1, 1, "", "DataTypeSpec"], [199, 1, 1, "", "FP8E4M3PerChannelSpec"], [199, 1, 1, "", "FP8E4M3PerGroupSpec"], [199, 1, 1, "", "FP8E4M3PerTensorSpec"], [199, 1, 1, "", "FP8E5M2PerChannelSpec"], [199, 1, 1, "", "FP8E5M2PerGroupSpec"], [199, 1, 1, "", "FP8E5M2PerTensorSpec"], [199, 1, 1, "", "Float16Spec"], [199, 1, 1, "", "GPTQConfig"], [199, 1, 1, "", "Int4PerChannelSpec"], [199, 1, 1, "", "Int4PerGroupSpec"], [199, 1, 1, "", "Int4PerTensorSpec"], [199, 1, 1, "", "Int8PerChannelSpec"], [199, 1, 1, "", "Int8PerGroupSpec"], [199, 1, 1, "", "Int8PerTensorSpec"], [199, 1, 1, "", "MX6Spec"], [199, 1, 1, "", "MX9Spec"], [199, 1, 1, "", "MXSpec"], [199, 1, 1, "", "PreQuantOptConfig"], [199, 1, 1, "", "QuaRotConfig"], [199, 1, 1, "", "QuantizationConfig"], [199, 1, 1, "", "QuantizationSpec"], [199, 1, 1, "", "RotationConfig"], [199, 1, 1, "", "SmoothQuantConfig"], [199, 1, 1, "", "TQTSpec"], [199, 1, 1, "", "Uint4PerChannelSpec"], [199, 1, 1, "", "Uint4PerGroupSpec"], [199, 1, 1, "", "Uint4PerTensorSpec"], [199, 1, 1, "", "Uint8PerChannelSpec"], [199, 1, 1, "", "Uint8PerGroupSpec"], [199, 1, 1, "", "Uint8PerTensorSpec"], [199, 3, 1, "", "load_pre_optimization_config_from_file"], [199, 3, 1, "", "load_quant_algo_config_from_file"]], "quark.torch.quantization.config.config.Config": [[199, 2, 1, "", "add_pre_optimization_config"], [199, 2, 1, "", "set_algo_config"]], "quark.torch.quantization.config.type": [[202, 1, 1, "", "DeviceType"], [202, 1, 1, "", "Dtype"], [202, 1, 1, "", "QSchemeType"], [202, 1, 1, "", "QuantizationMode"], [202, 1, 1, "", "RoundType"], [202, 1, 1, "", "ScaleType"], [202, 1, 1, "", "TQTThresholdInitMeth"], [202, 1, 1, "", "ZeroPointType"]], "quark.torch.quantization.config.utils": [[203, 3, 1, "", "dataclass_pretty_string"]], "quark.torch.quantization.debug": [[205, 3, 1, "", "activation_stats_hook"], [205, 3, 1, "", "barplot"], [205, 3, 1, "", "collect_quantization_statistics"], [205, 3, 1, "", "distribution_plot"], [205, 3, 1, "", "insert_stats_hooks"], [205, 3, 1, "", "save_distribution_histogram"], [205, 3, 1, "", "summarize_activation"], [205, 3, 1, "", "summarize_weight"], [205, 3, 1, "", "weight_stats_hook"]], "quark.torch.quantization.graph": [[207, 0, 0, "-", "fx"], [210, 0, 0, "-", "optimization"], [224, 0, 0, "-", "processor"], [228, 0, 0, "-", "torch_utils"]], "quark.torch.quantization.graph.fx": [[206, 0, 0, "-", "base"]], "quark.torch.quantization.graph.fx.base": [[206, 1, 1, "", "GraphTransform"], [206, 1, 1, "", "Transform"]], "quark.torch.quantization.graph.optimization": [[209, 0, 0, "-", "activate_dropout"], [211, 0, 0, "-", "model_optimization"], [212, 0, 0, "-", "modify_reshape_param"], [213, 0, 0, "-", "opt_pass_manager"], [214, 0, 0, "-", "post_quant"], [216, 0, 0, "-", "pre_quant"], [222, 0, 0, "-", "remove_dropout_node"], [223, 0, 0, "-", "utils"]], "quark.torch.quantization.graph.optimization.activate_dropout": [[209, 1, 1, "", "ActivateDropoutNode"]], "quark.torch.quantization.graph.optimization.model_optimization": [[211, 3, 1, "", "trans_opsfunc_2_quant_module"]], "quark.torch.quantization.graph.optimization.modify_reshape_param": [[212, 3, 1, "", "modify_reshape_param"]], "quark.torch.quantization.graph.optimization.opt_pass_manager": [[213, 1, 1, "", "OptPassBase"], [213, 1, 1, "", "OptPassManager"]], "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase": [[213, 2, 1, "", "call"], [213, 2, 1, "", "ensures"], [213, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager": [[213, 2, 1, "", "add_checks"], [213, 2, 1, "", "add_pass"]], "quark.torch.quantization.graph.optimization.post_quant": [[215, 0, 0, "-", "opt_pass_after_quant"]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant": [[215, 1, 1, "", "ConvertClip2ReLUQOPass"]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass": [[215, 2, 1, "", "call"], [215, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant": [[217, 0, 0, "-", "opt_pass_before_quant"], [218, 0, 0, "-", "replace_conv2d_to_qtconv2d"], [219, 0, 0, "-", "replace_conv_bn_to_qt_model"], [220, 0, 0, "-", "replace_convtranspose2d_to_qtconvtranspose2d"], [221, 0, 0, "-", "replace_linear_to_qtlinear"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant": [[217, 1, 1, "", "ConvertBn2D2ConvQOPass"], [217, 1, 1, "", "ConvertReduceMean2GapQOPass"], [217, 1, 1, "", "SplitQuantModuleCalledOverOnce"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass": [[217, 2, 1, "", "call"], [217, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass": [[217, 2, 1, "", "call"], [217, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce": [[217, 2, 1, "", "call"], [217, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d": [[218, 3, 1, "", "replace_conv2d_qtconv2d"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model": [[219, 3, 1, "", "replace_conv2dbn_quantizedconv_module"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d": [[220, 3, 1, "", "replace_convtranspose2d_qtconvtranspose2d"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear": [[221, 3, 1, "", "replace_linear_qtlinear"]], "quark.torch.quantization.graph.optimization.remove_dropout_node": [[222, 1, 1, "", "RemoveDropoutNode"]], "quark.torch.quantization.graph.processor": [[225, 0, 0, "-", "insert_quantizer"], [226, 0, 0, "-", "processor"], [227, 0, 0, "-", "processor_utils"]], "quark.torch.quantization.graph.processor.insert_quantizer": [[225, 3, 1, "", "insert_quantizer"]], "quark.torch.quantization.graph.processor.processor": [[226, 3, 1, "", "freeze_model"], [226, 3, 1, "", "mark_exclude_nodes"], [226, 3, 1, "", "transform_for_annotation"]], "quark.torch.quantization.graph.torch_utils": [[228, 5, 1, "", "BATCHNORM_OPS"], [228, 5, 1, "", "CAT_OPS"], [228, 3, 1, "", "allow_exported_model_train_eval"], [228, 3, 1, "", "is_batchnorm2d_node"], [228, 3, 1, "", "is_cat_node"], [228, 3, 1, "", "is_conv1d_node"], [228, 3, 1, "", "is_conv2d_node"], [228, 3, 1, "", "is_conv3d_node"], [228, 3, 1, "", "is_convtranspose2d_node"], [228, 3, 1, "", "is_dropout_node"]], "quark.torch.quantization.model_transformation": [[230, 3, 1, "", "in_place_replace_layer"], [230, 3, 1, "", "process_model_transformation"], [230, 3, 1, "", "setup_config_per_layer"]], "quark.torch.quantization.nn": [[232, 0, 0, "-", "modules"], [238, 0, 0, "-", "utils"]], "quark.torch.quantization.nn.modules": [[233, 0, 0, "-", "mixin"], [234, 0, 0, "-", "quantize_conv"], [235, 0, 0, "-", "quantize_conv_bn_fused"], [236, 0, 0, "-", "quantize_embed"], [237, 0, 0, "-", "quantize_linear"]], "quark.torch.quantization.nn.modules.quantize_conv": [[234, 1, 1, "", "QuantConv2d"], [234, 1, 1, "", "QuantConvTranspose2d"]], "quark.torch.quantization.nn.modules.quantize_conv_bn_fused": [[235, 1, 1, "", "QuantizedConvBatchNorm2d"]], "quark.torch.quantization.nn.modules.quantize_linear": [[237, 1, 1, "", "QuantLinear"]], "quark.torch.quantization.nn.utils": [[238, 3, 1, "", "check_min_max_valid"]], "quark.torch.quantization.observer": [[240, 0, 0, "-", "lsq_observer"], [241, 0, 0, "-", "observer"], [242, 0, 0, "-", "tqt_observer"]], "quark.torch.quantization.observer.lsq_observer": [[240, 1, 1, "", "LSQObserver"]], "quark.torch.quantization.observer.observer": [[241, 1, 1, "", "ObserverBase"], [241, 1, 1, "", "PerBlockBFPObserver"], [241, 1, 1, "", "PerBlockMXObserver"], [241, 1, 1, "", "PerChannelMinMaxObserver"], [241, 1, 1, "", "PerGroupMinMaxObserver"], [241, 1, 1, "", "PerTensorHistogramObserver"], [241, 1, 1, "", "PerTensorHistogramObserverPro"], [241, 1, 1, "", "PerTensorMSEObserver"], [241, 1, 1, "", "PerTensorMinMaxObserver"], [241, 1, 1, "", "PerTensorPercentileObserver"], [241, 1, 1, "", "PlaceholderObserver"], [241, 1, 1, "", "UniformScalingObserver"]], "quark.torch.quantization.observer.observer.PerGroupMinMaxObserver": [[241, 2, 1, "", "calculate_qparams"]], "quark.torch.quantization.observer.observer.PerTensorHistogramObserver": [[241, 2, 1, "", "forward"]], "quark.torch.quantization.observer.observer.PerTensorMSEObserver": [[241, 2, 1, "", "get_min_max_by_mse"]], "quark.torch.quantization.observer.observer.PerTensorMinMaxObserver": [[241, 2, 1, "", "forward"]], "quark.torch.quantization.observer.observer.PerTensorPercentileObserver": [[241, 2, 1, "", "get_min_max_by_percentile"]], "quark.torch.quantization.observer.observer.UniformScalingObserver": [[241, 2, 1, "", "calculate_qparams"], [241, 2, 1, "", "reset_min_max_vals"]], "quark.torch.quantization.observer.tqt_observer": [[242, 1, 1, "", "TQTObserver"]], "quark.torch.quantization.observer.tqt_observer.TQTObserver": [[242, 2, 1, "", "get_fix_position"]], "quark.torch.quantization.tensor_quantize": [[243, 1, 1, "", "FakeQuantizeBase"], [243, 1, 1, "", "NonScaledFakeQuantize"], [243, 1, 1, "", "ScaledFakeQuantize"]], "quark.torch.quantization.tensor_quantize.FakeQuantizeBase": [[243, 2, 1, "", "update_buffer"]], "quark.torch.quantization.utils": [[244, 3, 1, "", "set_op_by_name"], [244, 3, 1, "", "t_exponent"]], "quark.torch.utils": [[246, 0, 0, "-", "pack"]]}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"], "4": ["py", "property", "Python property"], "5": ["py", "data", "Python data"]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function", "4": "py:property", "5": "py:data"}, "terms": {"": [1, 3, 9, 13, 14, 15, 16, 17, 22, 25, 32, 39, 55, 90, 93, 118, 140, 150, 182, 193, 198, 213, 217, 230, 241, 250, 251, 252, 257, 258, 259, 260, 264, 266, 278, 279, 284, 290, 294, 295, 296, 297, 298, 300, 302, 303, 307, 311, 315, 317, 318, 319, 334, 335, 336, 338, 340, 341], "0": [1, 3, 5, 7, 13, 15, 27, 33, 39, 55, 57, 99, 110, 159, 181, 194, 199, 202, 203, 215, 218, 219, 220, 234, 235, 241, 250, 251, 252, 253, 255, 257, 264, 266, 271, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 290, 291, 292, 294, 295, 296, 297, 298, 300, 301, 302, 306, 308, 311, 315, 316, 317, 318, 319, 332, 335, 336, 337, 338, 339, 340], "000": [265, 266, 267, 268, 270, 274, 276], "000001": 294, "00001": [257, 285, 286, 291, 295, 296], "0001": 298, "001": [298, 340], "0010": 314, "00456": 277, "01": [3, 27, 199, 257, 278, 279, 298, 340], "02": 277, "0284": 314, "0317": [272, 273, 275, 280], "03236": 319, "03302": 319, "03372": 319, "042": 270, "05": 235, "052": 266, "06": [55, 144], "06d": 264, "07": [5, 99], "0755": 314, "076": 266, "07941": 202, "08": [314, 315, 339, 341], "08066": 202, "08342": 235, "084": 318, "085": 318, "09": [277, 339], "090": 266, "0b": 314, "1": [1, 3, 5, 9, 13, 27, 31, 39, 55, 57, 82, 85, 95, 99, 162, 181, 182, 194, 198, 199, 202, 212, 217, 218, 219, 220, 234, 235, 241, 242, 250, 251, 252, 253, 257, 259, 261, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 286, 289, 290, 291, 295, 296, 297, 298, 302, 303, 306, 308, 310, 312, 313, 330, 331, 332, 335, 336, 338, 339], "10": [250, 257, 264, 276, 277, 290, 302, 314, 316, 318, 338], "100": [3, 27, 241, 294, 295, 296, 298], "1000": [27, 252, 257, 265, 266, 267, 268, 270, 274, 276, 283, 284, 293, 295, 296, 298, 337], "10000": [99, 110, 285, 286, 291, 292], "1024": [251, 290, 317], "10518": 27, "10568": [26, 27], "108": 338, "11": [9, 250, 251, 277, 295, 314], "1110": 251, "1111": 251, "114": [266, 274], "1195": 314, "11b": [309, 310], "12": [250, 266, 278, 283, 285, 288, 291, 336, 338], "120": 272, "123": 338, "125m": [198, 255, 273, 280, 288, 303, 315, 317, 332, 337], "127": [202, 257, 302], "128": [3, 66, 198, 202, 257, 302, 304, 314, 315, 316, 323, 335, 336, 340], "12_quantiz": [266, 274, 285, 291], "13": [290, 295, 314], "135": 338, "13503277": 138, "1370": 314, "1382": 314, "13908052444458": 338, "13b": [150, 321], "14": 314, "140": 338, "148": 338, "14b": 314, "15": [202, 339], "156": 338, "16": [39, 55, 181, 202, 250, 251, 257, 277, 285, 286, 291, 293, 294, 295, 297, 299, 300, 302, 315, 339], "16406": 277, "16b": [315, 341], "17": [250, 276, 278, 290, 300], "1705472343": [294, 295, 296], "172": 338, "18": [257, 266, 278, 318, 341], "1805": 202, "1806": 235, "181324005126953": 338, "182": 266, "18944": 338, "19": [257, 300, 339, 341], "1903": 202, "1959": 314, "1e": [55, 99, 144, 199, 235, 293, 336, 340], "1k": [265, 266, 267, 268, 270, 274, 276], "1st": 302, "2": [1, 5, 9, 27, 31, 39, 55, 57, 82, 85, 99, 162, 194, 199, 202, 218, 219, 220, 235, 242, 250, 251, 254, 257, 259, 264, 266, 267, 268, 274, 277, 278, 279, 282, 283, 286, 288, 290, 295, 296, 297, 298, 302, 303, 306, 308, 310, 312, 313, 332, 338, 339], "20": [27, 181, 250, 311, 313, 338, 339, 341], "200": 283, "2004": [26, 27], "2006": 27, "2012": [265, 266, 267, 268, 270, 274, 276], "2014": 308, "2017": 269, "2020": [26, 27], "2022": [250, 300], "2024": [269, 275, 277, 293, 294, 295, 296, 298, 314, 315, 341], "2025": 297, "2045": 241, "2048": [3, 271, 272, 273, 275, 277, 280, 281, 282, 315], "21": 308, "2155": 339, "22": [286, 339], "224": [257, 259, 264, 265, 290, 318], "23": [251, 278, 308, 337, 339], "232": 270, "2404": 277, "2405": 277, "2462": 314, "248": 266, "24969482421875": 338, "25": [212, 274], "253": 338, "254": 318, "255": [202, 251, 257], "256": [241, 266], "26": [271, 277, 281, 282, 339], "26559": 308, "269912719726": 338, "27": [272, 273, 275, 280, 339], "274": 274, "278": 265, "28": [272, 273, 275, 280, 339], "29": [308, 314, 339], "291": 318, "2988730": 138, "2b": 314, "2d": [217, 338], "2e": [257, 316], "2gb": [3, 22, 61, 71, 76, 99, 257, 289, 290], "2nd": 302, "2x1": 338, "3": [1, 5, 31, 39, 55, 57, 162, 194, 199, 202, 235, 242, 250, 257, 259, 264, 265, 267, 268, 278, 279, 281, 283, 290, 297, 298, 303, 306, 310, 314, 318, 332, 335, 336, 338, 339, 341], "30": [273, 339], "3000": [285, 286, 291], "301": 318, "31": [251, 290, 306, 308], "32": [150, 251, 257, 296, 297, 301, 302, 314, 315, 319, 321, 323, 332, 336], "322": 267, "33": 276, "34745": 308, "34854": 308, "35": 339, "36": [314, 338], "3600": 298, "3604": 273, "3640": 338, "37": 339, "3794": 314, "38": 339, "384": [241, 273, 275, 280], "385": 280, "3892": 336, "3b": 314, "3ex": 302, "4": [1, 39, 57, 66, 105, 194, 199, 202, 250, 251, 257, 264, 265, 267, 268, 271, 273, 277, 278, 279, 283, 303, 304, 314, 316, 318, 319, 332, 336, 338, 339], "40": [338, 339], "4025": 314, "406": 273, "4096": [254, 257, 277, 317], "41": [266, 268, 308], "42": [308, 314, 339], "420": 268, "424": [265, 267, 268], "4315": 280, "43882751464844": 338, "44": [315, 316], "440": 276, "444": 274, "446": 266, "44639": 308, "45": [308, 338], "4532": 318, "4537": 318, "456": 270, "46": [314, 339], "469": 318, "47": 266, "4721": 314, "4759": 318, "480": [272, 273, 275, 280], "4838": 314, "484": 318, "486": 276, "4b": [66, 257], "4bit": 281, "4k": 316, "5": [5, 27, 39, 55, 72, 119, 181, 202, 250, 251, 255, 257, 265, 267, 268, 270, 271, 274, 276, 277, 278, 279, 281, 282, 283, 294, 298, 303, 308, 314, 316, 317, 318, 319, 336, 339, 340], "50": [265, 266, 267, 268, 270, 274, 276, 318], "500": [298, 308, 316], "5000": 308, "502": 270, "5081": 314, "51": 339, "510": 251, "512": [266, 315, 316, 323], "52": [267, 338], "5210": 336, "53238": 308, "53386": 308, "536": 276, "544": 159, "562": 318, "5651": 314, "56758": 308, "5734": 273, "58": 339, "580": 270, "59": 270, "5994": 314, "5b": [314, 315], "5e": 336, "6": [9, 33, 39, 55, 250, 253, 257, 271, 277, 278, 279, 281, 282, 283, 289, 293, 298, 302, 314, 317, 318, 319, 338], "60": [265, 338], "6006": 272, "602": 276, "61475": 308, "618": 276, "62": 274, "622321128845215": 338, "6236": 318, "6239": 318, "6267": [271, 281, 282], "63": [277, 283, 285, 286, 291], "64": [265, 266, 268, 277, 336, 337], "6400": 212, "642": 266, "6466": 318, "648": [265, 318], "65": [265, 267, 268, 318], "652": 265, "658": 266, "662": 266, "664": 270, "67": 266, "68": 318, "684": 266, "6846": [273, 275, 280], "69": 318, "690": [267, 268], "6984167098999": 338, "6986": 314, "6b": [314, 315, 316, 335, 341], "7": [39, 61, 202, 250, 257, 265, 277, 278, 282, 293, 302, 314, 337, 338, 339], "70": [266, 270], "708": [267, 268], "70b": [150, 321], "71": [266, 318], "716": [266, 274], "7224": 314, "73": [266, 274], "74": [266, 274, 276], "742": 266, "74845": 308, "75": 267, "754": 251, "756": 267, "7590": 314, "76": 276, "764": [266, 318], "766": 266, "77445": 308, "7782": 338, "784": 182, "788": [265, 267, 268], "79": 270, "7964": 319, "7b": [150, 254, 277, 282, 288, 301, 310, 312, 313, 314, 315, 317, 319, 321, 322, 338, 340, 341], "8": [39, 202, 251, 257, 265, 267, 268, 271, 273, 277, 278, 279, 297, 299, 300, 302, 314, 316, 337, 338, 339], "80": 212, "802": 268, "806": 265, "8074": 314, "82": [265, 266, 274], "8255": 282, "83": 270, "83954": 308, "85": [265, 267, 268, 339], "854": 270, "85444": 308, "86": 318, "8602": 314, "87": [266, 339], "872": 318, "87ba8cb8a6a4f6525f26255fa513d902b17ab060": 308, "88": [266, 270, 318], "881": 318, "883856773376465": 338, "8859": 271, "89": [266, 318], "891325950622559": 338, "8945": 314, "8952": 319, "8958": 314, "8b": [150, 306, 314, 321, 335, 338, 339, 341], "8bit": 282, "8x7b": [314, 315, 341], "9": [250, 251, 314, 336, 339], "90": [266, 318], "90b": [309, 310], "91": [266, 274], "92": 276, "920": 266, "9210": 281, "922": 266, "9228": 281, "9274": 314, "93": 276, "938": 276, "94": [270, 279], "948962688446045": 338, "95": [279, 318, 339], "9560": 314, "96": [257, 270], "97": [266, 274], "99": [3, 257, 298], "996": 266, "999": [3, 257, 298], "9994": 319, "9b": 314, "9e": 5, "A": [5, 6, 8, 13, 14, 15, 16, 17, 21, 32, 33, 39, 50, 53, 54, 55, 57, 71, 76, 107, 110, 111, 112, 118, 151, 181, 182, 183, 194, 199, 213, 235, 241, 251, 252, 257, 264, 278, 279, 290, 302, 307, 308, 310, 337, 338, 341], "And": [249, 283, 339], "As": [278, 279, 297, 302, 306, 307, 319, 336, 338], "At": 297, "Be": 302, "But": [99, 315, 325, 335, 338], "By": [3, 7, 32, 61, 251, 252, 257, 284, 294, 295, 297, 298, 300, 301, 322, 325], "For": [9, 39, 53, 118, 150, 159, 212, 217, 240, 241, 242, 249, 251, 252, 253, 254, 255, 257, 258, 261, 262, 265, 266, 271, 275, 277, 278, 279, 281, 282, 283, 284, 285, 286, 289, 290, 291, 292, 297, 300, 301, 302, 303, 304, 307, 308, 309, 314, 315, 316, 317, 318, 319, 322, 323, 324, 325, 331, 332, 334, 335, 336, 338, 339, 340], "If": [3, 32, 39, 53, 55, 56, 61, 95, 118, 140, 150, 183, 198, 199, 241, 244, 250, 252, 253, 254, 257, 264, 265, 266, 267, 268, 269, 270, 274, 276, 279, 283, 284, 290, 294, 295, 296, 297, 298, 301, 302, 303, 304, 306, 315, 319, 335, 336, 338, 340], "In": [22, 39, 55, 150, 212, 243, 249, 250, 251, 252, 254, 257, 258, 269, 278, 279, 283, 284, 293, 294, 295, 296, 297, 299, 300, 301, 302, 303, 306, 315, 317, 318, 319, 322, 324, 325, 330, 332, 334, 335, 336, 337, 338, 340], "It": [32, 33, 39, 56, 61, 82, 85, 95, 99, 150, 183, 186, 193, 198, 205, 213, 215, 217, 252, 254, 255, 257, 259, 266, 273, 277, 278, 279, 280, 283, 284, 285, 286, 290, 291, 294, 298, 335, 336, 339, 341], "Its": 257, "No": [259, 290], "Not": [31, 33, 50], "Of": 249, "On": [178, 314], "One": [3, 302, 319, 336], "Such": [159, 302], "THE": [288, 329], "That": 9, "The": [1, 5, 7, 27, 33, 39, 50, 53, 54, 55, 61, 71, 76, 82, 85, 95, 107, 108, 110, 118, 119, 147, 150, 151, 154, 180, 182, 183, 193, 198, 199, 202, 205, 213, 215, 217, 241, 243, 244, 248, 249, 250, 251, 252, 253, 254, 255, 257, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 285, 286, 288, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 302, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 315, 316, 318, 319, 321, 323, 329, 331, 332, 335, 336, 337, 338, 340, 341], "Then": [9, 265, 266, 267, 268, 269, 270, 274, 276, 319, 337, 338], "There": [1, 7, 241, 251, 266, 278, 279, 283, 293, 298, 319, 336, 340], "These": [151, 250, 251, 252, 296, 298, 300, 306], "To": [55, 93, 213, 250, 251, 259, 262, 265, 266, 267, 268, 270, 274, 276, 293, 295, 298, 300, 302, 308, 309, 314, 315, 316, 319, 323, 332, 338], "Will": [61, 300], "With": 302, "_": [264, 290, 306, 318, 324], "_2": 302, "_3sd": 202, "__getitem__": 264, "__init__": [32, 258, 259, 264, 336], "__len__": 264, "__name__": 315, "__quantize_input": 39, "_avgpool_globalaveragepool_output_0_float": 290, "_conv1_conv_output_0_dequantizelinear_output": 290, "_data_load": 143, "_do_calibr": 341, "_export": 318, "_fix_neuron_v2_devic": 242, "_float": 290, "_ll_j": 202, "_load_calibration_data": 258, "_native_batch_norm_legit": 228, "_native_batch_norm_legit_no_train": 228, "_pass": 213, "_preprocess_imag": 259, "_quant": 319, "_size_2_t": [234, 235], "_val": 319, "a16w8": [102, 285, 286, 291, 299], "a18w8": 292, "a1h_in1k": 270, "a2": [315, 341], "a2q": 181, "a8w8": [102, 285, 286, 291, 299, 306], "a8w8_cpu": 77, "a8w8_npu": 77, "ab": [55, 277, 336], "abbrevi": [266, 313, 315], "abc": [121, 122, 127, 128, 131, 135, 136, 143, 156, 178, 194, 199, 206, 209, 222, 241], "abil": 309, "about": [186, 205, 251, 262, 319, 332, 340, 341], "abov": [39, 118, 199, 257, 301, 309, 315, 318, 319, 321, 334, 335, 337], "absmax": 55, "absolut": [260, 301, 305, 306], "abstract": [33, 34, 213], "acc1_freez": 318, "acc1_quant": 318, "acceler": [119, 249, 252, 257, 265, 293, 294, 295, 296, 300, 339, 341], "accept": [1, 193, 198, 257, 278, 279, 283, 304], "access": [5, 31, 55, 162, 205, 252, 253, 254, 255, 265, 266, 267, 268, 270, 271, 274, 275, 276, 277, 281, 282, 301, 307, 314, 315, 316, 317, 318, 323, 338], "accommod": 251, "accompani": 297, "accord": [1, 7, 55, 150, 257, 269, 283, 295, 301, 319], "accordingli": [193, 198, 257, 307, 337], "account": 302, "accraci": 18, "accumul": 181, "accumulator_bit_width": 181, "accur": [181, 199, 249, 257, 262, 295, 332, 336], "accuraci": [3, 61, 98, 183, 199, 249, 251, 252, 253, 254, 255, 257, 260, 265, 266, 267, 268, 269, 270, 274, 276, 278, 279, 283, 284, 285, 286, 291, 297, 298, 299, 300, 301, 305, 318, 319, 335, 338, 341], "accuracy_improv": [252, 253, 254, 255, 265, 266], "accuracy_level": 66, "accuracylevel": 257, "achiev": [228, 249, 251, 252, 262, 266, 277, 286, 290, 294, 297, 298, 299, 300, 302, 307, 332], "across": [57, 151, 183, 194, 199, 202, 248, 251, 252, 257, 285, 286, 291, 335, 337], "act": [9, 25, 159, 215, 286], "act_ord": 181, "act_quant": 199, "activ": [3, 9, 26, 39, 54, 55, 57, 61, 76, 101, 111, 140, 181, 183, 199, 205, 215, 251, 254, 255, 257, 258, 262, 277, 278, 279, 284, 285, 286, 290, 291, 292, 293, 296, 297, 299, 300, 306, 307, 308, 315, 318, 319, 323, 332, 335, 338, 339, 340, 341], "activatedropoutnod": 209, "activation_lay": 317, "activation_qtyp": [4, 39, 54, 63], "activation_stats_hook": 205, "activation_tensor_nam": 297, "activation_tensor_name1": 297, "activation_tensor_name2": 297, "activation_typ": [3, 36, 55, 57, 252, 253, 254, 255, 257, 278, 279, 283, 284, 293, 294, 295, 296, 297, 298, 299, 300], "activationequ": 181, "activationsc": [257, 293], "activationsymmetr": [61, 252, 253, 254, 255, 257, 278, 279, 283, 284, 298, 300], "actord": 257, "acttargetquanttyp": [257, 297], "actual": [203, 257, 258, 260, 262, 305, 319, 332], "ad": [5, 31, 55, 162, 257, 298, 300, 307, 322, 336, 338, 341], "ada": 7, "adapt": [26, 27, 193, 198, 252, 253, 257, 285, 286, 291, 294, 297], "adaptive_avg_pool2d": 217, "adaptiveavgpool2d": 217, "adaqu": [21, 22, 27, 249, 257, 258, 285, 286, 288, 291, 293, 294, 295, 296, 298, 299, 341], "adaround": [7, 21, 22, 26, 27, 249, 257, 258, 283, 284, 285, 286, 288, 291, 293, 294, 298, 299, 341], "adaroundconst": 7, "adaroundintquant": 7, "add": [54, 55, 108, 138, 140, 150, 199, 213, 250, 257, 298, 300, 314, 315, 318, 323, 341], "add_check": 213, "add_export_info_for_hf": 150, "add_pass": 213, "add_pre_optimization_config": 199, "add_qdq_pair_to_weight": 54, "add_qk_rotation_after_function_call_in_forward": 140, "add_wrapper_after_function_call_in_method": 138, "addit": [3, 22, 32, 39, 54, 55, 57, 257, 271, 277, 281, 282, 290, 300, 308, 309, 314, 315, 322], "addition": [151, 251, 257, 294, 295, 296, 316], "addqdqpairtoweight": 257, "addqdqtoqoptransform": 50, "address": [257, 293, 295, 318, 335, 341], "adeptli": 251, "adequ": 264, "adher": 251, "adjust": [39, 72, 199, 251, 252, 257, 260, 266, 278, 279, 284, 292, 295, 296, 298, 305, 308, 318], "adjust_bias_scal": 72, "adjust_hard_sigmoid": 72, "adjust_quantize_info": 72, "adjust_shift_bia": 72, "adjust_shift_cut": 72, "adjust_shift_read": 72, "adjust_shift_swish": 72, "adjust_shift_writ": 72, "adjustbiasscal": 257, "adjusthardsigmoid": 257, "adjustshiftbia": 257, "adjustshiftcut": 257, "adjustshiftread": 257, "adjustshiftswish": 257, "adjustshiftwrit": 257, "adopt": 341, "advanc": [249, 251, 252, 258, 269, 275, 277, 293, 294, 295, 296, 297, 298, 299, 303, 307, 317, 319, 340, 341], "advantag": [251, 278, 279], "affect": [138, 140, 251], "afford": [302, 336], "after": [20, 31, 53, 138, 140, 183, 194, 199, 213, 215, 226, 251, 252, 254, 256, 257, 259, 277, 278, 279, 284, 288, 290, 292, 294, 295, 296, 297, 298, 306, 314, 315, 318, 319, 321, 322, 325, 329, 335, 336, 338], "again": [55, 330], "against": 55, "aggress": [199, 294, 297], "agnost": 319, "ahead": [262, 332, 336], "ai": [39, 61, 150, 258, 296, 297, 315, 337, 341], "aid": 199, "ailab": 140, "aim": [150, 193, 198, 252, 285, 286, 291, 294, 295, 296, 298, 302, 307, 319, 340], "akanametov": 286, "al": [26, 27, 181, 202], "algo": 184, "algo_config": [66, 128, 183, 194, 199, 340], "algoconfig": [194, 199], "algoconfigbas": [194, 199], "algorithm": [3, 22, 26, 27, 181, 183, 194, 199, 202, 249, 251, 252, 254, 255, 257, 258, 260, 269, 273, 277, 278, 279, 280, 281, 284, 285, 286, 291, 293, 298, 299, 303, 315, 317, 319, 323, 338, 340, 341], "algorithm_config": 340, "align": [72, 217, 257, 293, 319, 341], "align_concat": 72, "align_pad": 72, "align_pool": 72, "align_quantize_info": 72, "align_reshap": 72, "align_slic": 72, "align_transpos": 72, "alignconcat": 257, "aligneltwisequanttyp": 257, "alignpad": 257, "alignpool": 257, "alignreshap": 257, "alignslic": 257, "aligntranspos": 257, "all": [1, 3, 9, 25, 31, 56, 61, 82, 85, 95, 99, 118, 119, 193, 198, 205, 217, 250, 252, 257, 269, 275, 277, 283, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 306, 307, 309, 310, 315, 318, 319, 336, 338, 339, 340, 341], "all_config": [278, 279, 283], "all_tensors_to_one_fil": [281, 282, 290], "alloc": [251, 298, 302], "allow": [33, 57, 118, 119, 151, 183, 194, 199, 228, 251, 257, 262, 263, 266, 285, 286, 290, 291, 294, 295, 296, 297, 301, 302, 311, 317, 332, 333], "allow_exported_model_train_ev": 228, "allow_mulit_consum": 33, "allow_multi_consum": 33, "almost": 301, "alon": 336, "along": [301, 308, 319], "alpha": [7, 26, 55, 76, 181, 199, 335, 336, 339, 340, 341], "alpha_valu": 55, "alreadi": [61, 82, 85, 99, 193, 198, 265, 266, 267, 268, 270, 274, 276, 290, 292, 319], "also": [33, 50, 61, 156, 183, 199, 243, 251, 252, 254, 255, 257, 265, 266, 273, 277, 280, 290, 297, 298, 299, 300, 301, 302, 309, 318, 319, 320], "altern": [251, 257, 290, 310, 312, 313], "alwai": [22, 32, 61, 251, 257, 296, 301, 336, 337], "amax": 241, "amd": [217, 248, 250, 253, 254, 255, 256, 257, 260, 261, 262, 264, 269, 279, 284, 290, 292, 299, 302, 304, 305, 326, 329, 331, 332, 334, 335, 336, 341], "amd_quark": [250, 288, 318, 329], "among": 53, "amount": [284, 302], "amplifi": 293, "an": [3, 22, 32, 39, 53, 54, 55, 56, 57, 61, 99, 104, 107, 108, 110, 118, 121, 122, 127, 128, 131, 135, 136, 140, 143, 150, 156, 178, 182, 193, 194, 198, 199, 205, 206, 209, 213, 217, 222, 228, 241, 249, 251, 252, 255, 257, 258, 259, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 288, 289, 290, 292, 294, 295, 296, 297, 298, 301, 302, 303, 304, 307, 313, 314, 315, 316, 319, 322, 323, 335, 336, 337, 338, 341], "analysi": 257, "analyz": [306, 339], "ani": [1, 3, 4, 5, 6, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 50, 51, 54, 55, 57, 61, 63, 66, 71, 72, 74, 78, 86, 91, 106, 107, 108, 114, 119, 121, 122, 136, 138, 140, 143, 147, 150, 169, 180, 183, 186, 199, 203, 205, 213, 237, 241, 243, 251, 278, 279, 283, 297, 301, 319, 336, 339, 341], "annot": [55, 226, 318], "annotate_tensor": 55, "anoth": [255, 302, 307, 319], "apart": [1, 297], "api": [57, 60, 133, 151, 160, 181, 183, 184, 194, 195, 199, 229, 243, 250, 258, 262, 290, 298, 300, 303, 307, 318, 319, 323, 332, 334], "apl": [249, 326, 329, 341], "appar": 251, "appear": 251, "append": [118, 336], "appli": [31, 33, 34, 36, 39, 50, 51, 53, 55, 57, 99, 140, 141, 144, 150, 181, 183, 199, 202, 250, 251, 252, 257, 258, 259, 293, 294, 295, 296, 297, 298, 299, 300, 302, 307, 336, 337, 338], "applic": [118, 202, 251, 252, 278, 279, 292, 297, 298, 307, 315], "approach": [193, 198, 248, 251, 252, 262, 264, 266, 295, 296, 297, 298, 302, 332, 335, 336], "appropri": [118, 297, 316, 319, 335], "approxim": [55, 228, 251, 257, 335], "ar": [3, 7, 9, 32, 33, 39, 50, 55, 61, 99, 110, 118, 138, 140, 150, 156, 159, 186, 193, 198, 199, 202, 205, 217, 228, 238, 244, 250, 252, 253, 254, 255, 257, 262, 265, 266, 275, 277, 278, 279, 281, 283, 288, 289, 290, 291, 292, 293, 295, 296, 297, 298, 299, 300, 303, 306, 307, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 322, 324, 325, 329, 331, 332, 334, 336, 337, 338, 339, 340, 341], "arbitrari": [118, 301], "arbitrarili": 297, "architectur": [150, 334, 336, 338, 341], "area": 293, "aren": 183, "arg": [1, 5, 31, 33, 34, 39, 50, 51, 53, 54, 55, 56, 61, 71, 76, 119, 150, 154, 162, 182, 183, 193, 198, 202, 205, 213, 215, 217, 218, 219, 220, 221, 244, 278, 279, 283, 310, 313, 318, 322, 324, 325, 334], "argmax": [264, 300], "argument": [32, 118, 150, 154, 241, 257, 290, 306, 314, 315], "aris": 289, "arithmet": 251, "around": [140, 199, 257, 298, 308], "arrai": [1, 22, 25, 99, 114, 295, 296], "art": [249, 319], "artifici": 295, "arxiv": [26, 27, 202, 235, 277], "as_text": 114, "asctim": 118, "assembl": 1, "assembleidx": 1, "assert": 32, "assess": 298, "assign": [150, 243, 278, 279, 283, 294, 296, 297, 298, 318], "assist": 251, "associ": [39, 55, 252, 340], "assum": [56, 193, 198, 309], "astyp": 264, "asym": [310, 312, 313], "asymetr": 150, "asymmetr": [39, 183, 249, 251, 257, 258, 261, 263, 299, 303, 319, 321, 323, 331, 333, 335, 341], "aten": [211, 217, 218, 219, 220, 221, 228], "attach": [205, 225, 226, 235], "attempt": 337, "attent": [199, 338, 341], "attr": [6, 11], "attr_path": 147, "attribut": [5, 11, 31, 32, 54, 55, 57, 118, 147, 159, 162, 213, 217, 257, 295, 296, 297, 340], "attributeproto": 11, "augment": [3, 199], "augment_graph": 3, "augmented_model": 3, "augmented_model_path": 3, "author": 32, "auto": [257, 269, 288, 298, 341], "auto_merg": 290, "auto_search": 298, "auto_search_config": [278, 279, 283, 298], "auto_search_conig": [278, 279, 283], "auto_search_in": [278, 279, 283], "auto_search_inst": 298, "auto_search_model": 269, "autoawq": [150, 322, 341], "autofp8": [150, 341], "automat": [32, 36, 57, 249, 278, 279, 290, 335, 339, 341], "automixprecis": [257, 297], "automixusefastft": 257, "automodelforcausallm": [198, 301, 303, 319, 332, 337], "autosearch": 298, "autosearchconfig": 298, "autosearchconfig_default": [278, 279, 283], "autosmoothqu": [199, 335], "autosmoothquantconfig": 199, "autosmoothquantprocessor": 121, "autotoken": [198, 301, 303, 315, 319, 332, 337], "avail": [118, 150, 198, 250, 251, 252, 253, 254, 255, 257, 259, 265, 266, 275, 281, 294, 295, 296, 298, 307, 313, 314, 315, 316, 317, 318, 340, 341], "averag": [3, 20, 217, 252, 254, 257, 260, 305], "average_l2": 20, "averagepool": 300, "averaging_const": 3, "avgpool": 290, "avoid": [55, 93, 181, 257, 278, 279, 299, 314, 315, 340], "awai": [251, 257], "awar": [181, 199, 235, 249, 251, 252, 316, 318, 335, 341], "awq": [150, 199, 249, 303, 310, 312, 313, 319, 322, 335, 340, 341], "awq_auto_config_help": 315, "awq_config": 315, "awqconfig": [199, 340], "awqprocessor": [122, 340], "axi": [39, 54, 199, 257, 261, 264, 295, 296, 301, 331, 340], "b": [33, 39, 55, 61, 118, 242, 257, 278, 279, 302, 315], "b_beta": 13, "back": 251, "backbon": [199, 338], "backend": [181, 183, 249, 262, 297, 319, 332, 341], "backpropag": 318, "backward": 307, "bad": [278, 279], "bag": 341, "balanc": [249, 251, 278, 279, 285, 286, 291, 294, 295, 297, 302, 306, 335, 336, 341], "bandwidth": 294, "bar": [205, 306], "barplot": 205, "base": [3, 22, 32, 33, 34, 39, 50, 53, 54, 55, 56, 110, 118, 138, 147, 159, 182, 193, 198, 202, 213, 217, 225, 226, 230, 243, 249, 251, 252, 257, 260, 269, 273, 275, 277, 278, 279, 280, 283, 284, 290, 292, 298, 300, 302, 305, 306, 308, 317, 319, 325, 327, 335, 336, 341], "base_input": 1, "base_path": 159, "basealgoprocessor": 136, "baselin": 1, "basevocab": 159, "batch": [53, 212, 252, 257, 264, 265, 266, 267, 268, 269, 270, 274, 276, 290, 306], "batch_idx": 264, "batch_it": 324, "batch_norm": 228, "batch_siz": [27, 259, 264, 290, 304, 310, 312, 313, 318, 336], "batchfeatur": [198, 205], "batchnorm": [33, 53, 228, 235, 257, 300], "batchnorm2d": [228, 235], "batchnorm_op": 228, "batchsiz": [252, 257, 284, 294, 295, 296], "bb": 118, "bdcb8f42221bc40c411150a009a3d3a30fa74722": 319, "becaus": [1, 39, 251, 257, 278, 290, 292, 299, 319, 330, 340], "becom": [199, 251, 254, 277, 297], "been": [33, 34, 250, 252, 257, 303, 337], "befor": [53, 101, 118, 140, 183, 213, 215, 217, 226, 250, 254, 257, 265, 277, 278, 279, 290, 292, 300, 306, 314, 316, 317, 319, 336, 338, 340], "begin": [307, 319], "behalf": 251, "behav": 13, "behavior": [6, 61, 228, 252, 257, 323], "behind": 302, "being": [205, 250, 251, 257, 298, 306, 307, 319], "below": [39, 55, 118, 249, 257, 278, 279, 283, 284, 286, 302, 309, 310, 311, 312, 313, 315, 319, 322, 325, 335, 336], "benchmark": 341, "benefici": [251, 301], "benefit": [249, 336], "besid": [252, 257, 299, 300, 339], "best": [199, 262, 278, 288, 290, 298, 332, 341], "beta": 39, "beta_rang": 27, "better": [3, 34, 140, 183, 217, 249, 251, 252, 257, 277, 278, 285, 286, 290, 291, 293, 298, 300, 301, 335, 336, 338, 341], "between": [1, 3, 20, 98, 107, 140, 183, 193, 198, 205, 228, 241, 249, 251, 257, 260, 278, 279, 283, 293, 294, 295, 305, 306, 309, 311, 319, 336], "between_op": 107, "bewar": 336, "bf16": [257, 285, 286, 290, 291, 292, 297, 299, 310, 312, 313, 323, 341], "bf16qdqtocast": 257, "bf16withclip": 257, "bfloat16": [7, 80, 83, 101, 104, 109, 202, 241, 249, 251, 257, 258, 285, 286, 291, 293, 297, 299, 302, 303, 323, 324, 335, 340, 341], "bfloat16_spec": 340, "bfloat16spec": 199, "bfloat_16_onnx_model_path": [80, 83, 292], "bfloat_format": 292, "bfp": [54, 257, 266, 285, 286, 288, 290, 291, 294, 295, 296, 337, 341], "bfp16": [81, 84, 249, 257, 285, 286, 291, 297, 299, 317, 341], "bfp16_adaqu": 265, "bfp16spec": 199, "bfp_16_onnx_model_path": [81, 84, 292], "bfp_method": [257, 295, 300], "bfpattribut": [257, 295, 300], "bfpfixneuron": [6, 257, 294, 295, 297, 300, 341], "bfpquantiz": 6, "bia": [8, 9, 13, 14, 15, 16, 17, 22, 39, 54, 140, 150, 178, 181, 183, 199, 205, 218, 219, 220, 221, 228, 234, 235, 237, 252, 253, 257, 258, 318, 334, 336, 340, 341], "bias": [9, 183, 251, 318], "bias_nam": 39, "bias_to_quant": 54, "biascorrect": [181, 249, 341], "biastargetquanttyp": 257, "bigger": [22, 252, 257], "bin": [3, 199, 205, 241, 272, 273, 275, 280, 290, 302, 336], "bin_edg": 241, "binari": [82, 85, 99, 319], "bit": [61, 66, 178, 183, 202, 251, 257, 273, 278, 279, 285, 286, 291, 293, 294, 295, 296, 297, 298, 299, 302, 307, 315, 335, 337, 338], "bit_width": [183, 257, 295, 300], "bitwidth": 302, "blob": [228, 283, 285, 286, 291], "block": [54, 199, 202, 249, 257, 266, 285, 286, 288, 291, 295, 296, 297, 300, 302, 304, 315, 319, 331, 335, 338, 341], "block_q4_1": 319, "block_recon": 27, "block_siz": [66, 257, 271, 272, 273, 275, 277, 280, 281, 282, 295, 296, 300], "blocksiz": 257, "blockwisetuningconfig": [128, 194], "blockwisetuningprocessor": 128, "blow": 105, "blue": [5, 31, 55, 162], "bn": [33, 217, 228], "bool": [3, 4, 5, 7, 9, 13, 27, 33, 39, 53, 54, 55, 57, 61, 63, 66, 71, 72, 74, 76, 99, 114, 118, 141, 144, 150, 156, 178, 180, 181, 198, 199, 219, 228, 234, 235, 237, 238, 241, 257, 290, 340], "boolean": [199, 252, 253, 254, 255, 257, 290], "boost": 284, "both": [33, 39, 72, 140, 150, 226, 248, 249, 257, 261, 262, 265, 284, 285, 286, 290, 291, 295, 297, 300, 302, 307, 309, 313, 317, 318, 319, 324, 331, 332, 334, 336, 338, 341], "boundari": [257, 297], "boundless": 290, "bpevocab": 159, "brain": 293, "break": [33, 203, 303], "brecq": [249, 317, 341], "brevita": [249, 326, 329, 336, 341], "brevitasquant": 307, "broaden": 307, "broadli": 251, "broken": 301, "buffer": 243, "buffer_nam": 243, "bug": [33, 50], "build": [1, 150, 266, 278, 279, 283, 297, 298, 319, 330], "build_all_config": [278, 279, 283], "builder": [309, 311], "buildin": 1, "buildin_eval_func": 1, "built": [266, 269, 278, 279, 298, 341], "bunch": [301, 319], "byte": 251, "c": [33, 118, 159, 217, 250, 265, 266, 267, 268, 269, 270, 274, 275, 276, 277, 278, 279, 293, 294, 295, 296, 297, 298, 300], "c4ai": [314, 315, 341], "cach": [20, 22, 55, 199, 249, 278, 279, 283, 303, 323, 330, 335, 338, 341], "cacheddataread": [20, 55], "cacheddataset": 22, "calc_recon_loss": 26, "calc_round_loss": 26, "calcuat": [278, 279, 283], "calcul": [1, 3, 20, 26, 39, 55, 199, 241, 251, 253, 257, 269, 278, 279, 298, 309, 318, 319, 337, 341], "calculate_qparam": [241, 243], "calculate_quantization_param": 39, "calib": 259, "calib_": 264, "calib_000001": [259, 264], "calib_000002": [259, 264], "calib_000003": [259, 264], "calib_000004": [259, 264], "calib_000005": [259, 264], "calib_bin_edg": 241, "calib_data": [136, 265, 266, 267, 268, 270, 274, 276, 283, 285, 291], "calib_data_fold": 258, "calib_data_path": [111, 259, 278, 279, 283, 285, 286, 291], "calib_data_read": 258, "calib_dataload": [182, 198, 301, 303, 304, 319, 324, 332, 336, 337], "calib_dataread": 259, "calib_hist": 241, "calib_imag": 286, "calib_length": 318, "calib_load": [182, 318], "calib_num": 278, "calib_path": 278, "calib_prompt": 308, "calib_s": 308, "calibdataread": 258, "calibdatas": 257, "calibmovingaverag": [254, 257, 278, 279, 298], "calibmovingaverageconst": [257, 278, 279, 298], "calibr": [22, 27, 39, 54, 55, 56, 57, 61, 76, 182, 193, 198, 226, 249, 251, 254, 257, 258, 262, 264, 265, 266, 267, 268, 269, 270, 274, 276, 278, 279, 287, 292, 293, 294, 295, 296, 297, 298, 303, 317, 318, 319, 332, 336, 337, 341], "calibrate_method": [3, 4, 39, 54, 55, 57, 63, 252, 253, 254, 255, 257, 278, 279, 283, 284, 293, 294, 295, 296, 297, 298, 299, 300], "calibration_cache_dir": 264, "calibration_data": [258, 259, 264], "calibration_data_path": [55, 56, 259], "calibration_data_read": [55, 56, 252, 253, 254, 255, 257, 259, 284, 298, 299, 300], "calibration_dataset_path": [265, 266, 267, 268, 270, 274, 276, 278, 279], "calibration_image_fold": 259, "calibration_method": 257, "calibrationdataread": [22, 55, 56, 111, 258, 259, 290], "calibrationmethod": [3, 55, 57, 254, 257, 278, 279, 283, 289, 293, 294, 295, 296, 297, 298, 300], "calibtensorrangesymmetr": 257, "call": [9, 22, 32, 39, 118, 138, 140, 178, 213, 215, 217, 228, 250, 251, 283, 307, 317, 319, 336, 337, 338], "call_modul": 225, "callabl": [74, 138, 140, 213], "can": [1, 4, 5, 9, 31, 32, 33, 39, 55, 63, 99, 118, 140, 151, 159, 162, 182, 183, 193, 198, 199, 205, 212, 213, 215, 217, 241, 242, 250, 251, 252, 253, 254, 255, 257, 263, 264, 265, 266, 267, 268, 270, 271, 274, 276, 277, 278, 279, 281, 282, 283, 284, 285, 286, 288, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 306, 307, 308, 309, 313, 314, 315, 316, 318, 319, 322, 325, 329, 333, 335, 336, 338, 340, 341], "candid": 298, "candidate_lay": 34, "candidate_nod": [31, 51], "canni": 341, "cannot": [99, 150, 257, 290, 324, 330, 338], "capabl": [249, 261, 297, 306, 307, 331, 341], "capi": 289, "caption": 308, "captions_sourc": 308, "capture_pre_autograd_graph": 318, "card": 315, "care": [33, 336], "carefulli": [297, 316], "carri": 118, "case": [32, 39, 61, 180, 212, 251, 252, 253, 254, 255, 257, 297, 302, 303, 336, 338], "cast": [82, 85, 104, 109, 257, 292, 300, 341], "cast_cast": 324, "cat": 228, "cat_op": 228, "categor": 251, "categori": 251, "cauchy_": 336, "caus": [252, 279, 289, 290, 293, 341], "caution": 307, "cd": [308, 335], "ceil": 242, "center": 298, "central": [3, 257], "certain": [118, 251, 252, 257, 290, 297, 298, 299], "ch_axi": [7, 198, 199, 261, 301, 319, 331, 332, 336, 337, 340], "challeng": 335, "chang": [32, 99, 199, 212, 217, 228, 250, 300, 307, 310, 312, 313], "channel": [39, 54, 57, 61, 71, 118, 140, 144, 183, 199, 202, 249, 251, 257, 258, 261, 290, 303, 331, 335, 339, 340, 341], "channel_axi": 39, "channel_splitting_ratio": 181, "channel_splitting_split_input": 181, "chapter": [262, 332], "characterist": [251, 252, 297], "chart": 205, "chat": [310, 312, 313, 315, 341], "chatglm": [335, 341], "chatglm3": [314, 315, 316], "check": [9, 39, 55, 159, 213, 215, 217, 238, 250, 265, 266, 267, 268, 270, 274, 276, 298, 318, 330, 336], "check_hard_sigmoid_condit": 55, "check_min_max_valid": 238, "check_model_quantiz": 55, "check_onnx_model": 55, "check_reduce_mean_condit": 55, "check_relu_like_nod": 55, "checker": 159, "checkout": 308, "checkpoint": [271, 277, 281, 282, 310, 312, 313, 314, 315, 316, 322, 323, 325], "choic": [150, 198, 252, 294, 310, 338], "choos": [249, 250, 251, 266, 301, 302, 318, 319, 325, 335], "chosen": [20, 301, 302], "circumst": 39, "citi": 308, "ckpt_path": 315, "cl": [250, 300], "clamp": 319, "class": [258, 264, 265, 266, 267, 268, 270, 274, 276, 278, 279, 283, 299, 304, 307, 336, 337, 340], "classic": [257, 295, 302], "classif": [249, 266, 274, 283, 291, 297, 341], "classifi": 251, "classmethod": [25, 26], "cle": [57, 249, 258, 269, 285, 286, 288, 291, 341], "cle_balance_method": 5, "cle_pair_typ": 5, "cle_scale_append_bia": 5, "cle_scale_use_threshold": 5, "cle_step": 5, "cle_total_layer_diff_threshold": 5, "cle_transform": 5, "cle_weight_threshold": 5, "clean": [278, 279, 283, 341], "clescaleappendbia": [253, 257], "clestep": [253, 257], "cletotallayerdiffthreshold": 257, "cli": [271, 272, 273, 275, 280, 281, 282], "click": [250, 300, 309], "clip": [39, 53, 55, 101, 215, 242, 253, 257, 300, 308], "clip_max": 215, "clip_min": 215, "clip_nod": 55, "clockwis": 338, "clone": 308, "close": [252, 260, 278, 279, 305], "closer": [252, 338], "closest": [55, 257], "cloud": 319, "cmd": [250, 300], "cnn": [54, 57, 249, 257, 289, 299, 341], "cnn_dailymail": 313, "cnn_dm": 313, "cnv": 95, "co": [265, 267, 268, 269, 270, 272, 273, 274, 275, 276, 278, 279, 280, 283, 341], "coars": [295, 296], "coco": [269, 278, 308], "coco2014": 308, "coco2017": [269, 278], "code": [32, 140, 217, 250, 283, 285, 286, 288, 289, 290, 291, 294, 295, 296, 303, 307, 315, 319, 321, 329, 335, 339, 341], "coher": 315, "cohereforai": [314, 315, 341], "colbert": 181, "collaps": 251, "collat": 315, "collate_fn": 304, "collect": [1, 3, 5, 31, 55, 162, 205, 213, 241, 243, 306, 336, 341], "collect_quantization_statist": 205, "color": [5, 31, 55, 162, 302], "column": [257, 302, 335], "com": [138, 266, 274, 283, 285, 286, 291, 308], "combin": [1, 33, 50, 235, 257, 278, 279, 283, 294, 317, 323, 338, 339], "come": 251, "comma": 313, "command": [250, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 290, 300, 314, 315, 319, 323, 335, 341], "comment": [257, 300], "commit": 319, "common": [33, 257, 265, 285, 286, 291, 294, 295, 296, 297, 302, 319, 336, 339], "common_typ": [234, 235], "commonli": [265, 266, 267, 268, 269, 270, 274, 276], "compar": [55, 252, 257, 290, 292, 294, 296, 302, 306, 335, 336, 341], "comparison": [266, 311], "compat": [56, 150, 193, 198, 250, 252, 257, 278, 294, 304, 307, 308, 315, 339, 341], "compil": [72, 95, 250, 257, 297, 300, 306, 315, 339, 341], "complementari": 241, "complet": [150, 298, 303], "complex": [193, 198, 251, 257, 265, 278, 279, 285, 286, 291, 294, 297], "compli": 307, "compon": [57, 183, 194, 199, 278, 279, 307, 341], "composit": 251, "comprehens": [57, 151, 183, 194, 199, 249, 309], "compress": [150, 198, 251, 306, 341], "compromis": [249, 251, 336], "comput": [3, 8, 25, 39, 55, 93, 118, 183, 205, 243, 250, 251, 252, 254, 257, 260, 265, 266, 285, 286, 290, 291, 293, 294, 295, 296, 297, 301, 302, 305, 311, 319], "computation": 297, "compute_rang": 3, "compute_scale_loss": 199, "compute_scale_zp": 55, "compute_scale_zp_fp": 55, "concat": [33, 53, 257, 300], "concat_5": 286, "concaten": [53, 283], "concentr": [254, 277], "concept": [251, 266, 295], "conclud": 298, "concret": [243, 266, 278, 279], "conda": 309, "condit": [55, 215, 269, 278, 279, 283, 298], "conduct": [253, 257, 318], "conf": 1, "config": [1, 4, 33, 34, 51, 56, 63, 127, 128, 131, 135, 143, 150, 169, 178, 182, 184, 186, 193, 195, 198, 230, 234, 235, 237, 240, 241, 242, 243, 252, 253, 254, 255, 257, 258, 259, 265, 266, 267, 268, 269, 271, 272, 273, 275, 276, 277, 280, 281, 282, 283, 284, 285, 286, 291, 293, 294, 295, 296, 297, 298, 299, 301, 303, 308, 315, 318, 319, 321, 322, 324, 325, 332, 336, 337, 338, 339, 341], "configbas": [194, 199], "configur": [3, 27, 32, 34, 55, 56, 57, 150, 151, 183, 193, 194, 198, 199, 241, 249, 251, 252, 258, 262, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 284, 286, 287, 294, 296, 297, 298, 300, 303, 307, 318, 319, 322, 323, 325, 328, 330, 332, 334, 336, 341], "configuration_prepar": 315, "configverifi": 186, "conflict": 290, "conform": 304, "confus": [251, 252, 293, 295, 296, 297, 302, 303, 306, 330, 334, 335, 336], "congratul": 301, "conjunct": 277, "connect": 33, "consecut": 253, "consid": [3, 39, 251, 257, 260, 290, 291, 302, 335, 338], "consider": 290, "consist": [25, 154, 257, 290, 337], "const": [61, 257], "constant": [3, 7, 66, 150, 257], "constrain": [252, 294, 302, 318], "constraint": [33, 50, 53, 72, 74, 298, 302, 335], "construct": [32, 118, 213, 254, 257, 337], "constructor": 32, "consum": [33, 50, 251, 297, 299], "consumpt": [297, 298], "contain": [33, 39, 50, 55, 56, 107, 150, 154, 193, 198, 199, 213, 215, 217, 241, 244, 250, 251, 252, 253, 254, 255, 257, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 280, 282, 283, 285, 286, 290, 291, 300, 314, 319, 323, 324, 325], "content": 341, "context": [260, 305], "continu": [297, 298], "contrib": 55, "control": [57, 150, 151, 183, 194, 199, 255, 257, 278, 279, 290, 297, 301, 335, 337], "control_v11p_sd15_canni": 308, "controlnet": 341, "controlnet_id": 308, "conv": [9, 33, 53, 61, 95, 102, 140, 257, 286, 297, 300], "conv1": [257, 290], "conv1d": 228, "conv2": 257, "conv2d": [33, 199, 217, 218, 219, 228, 230, 234, 235, 251, 303, 341], "conv2d_85": 212, "conv3d": 228, "conv_501_dequantizelinear": 290, "conv_501_dequantizelinear_output": 290, "conv_664": 257, "conv_665": 257, "conv__224": [61, 257], "conv__252": [61, 257], "conv_out": 290, "conv_transpose2d": [220, 228], "conveni": [252, 292], "converg": [252, 257], "convers": [50, 51, 82, 85, 95, 99, 257, 292, 302], "convert": [8, 9, 22, 51, 53, 55, 57, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 106, 114, 118, 150, 154, 182, 215, 257, 265, 281, 282, 290, 293, 300, 304, 319, 341], "convert_a8w8_npu_to_a8w8_cpu": [100, 292], "convert_act": 9, "convert_avg_pool_to_dpu_vers": 74, "convert_bn_to_conv": 53, "convert_clip_to_relu": 53, "convert_conv": 9, "convert_customqdq_to_qdq": 100, "convert_dynamic_to_fix": [100, 265], "convert_exported_model_to_gguf": 154, "convert_float16_to_float": [82, 99], "convert_float32_to_float16": 85, "convert_float_to_float16": [85, 99], "convert_float_to_float16_model_path": 99, "convert_fp16_to_bf16": [100, 292], "convert_fp16_to_bfp16": [100, 292], "convert_fp16_to_fp32": [55, 57, 100, 257, 292, 299, 300], "convert_fp32_to_bf16": [100, 292], "convert_fp32_to_bfp16": [100, 292], "convert_fp32_to_fp16": [100, 281, 282], "convert_gemm": 9, "convert_hard_sigmoid_to_dpu_vers": 74, "convert_hf_to_gguf": 319, "convert_initializers_to_float": 92, "convert_instance_norm_to_dpu_vers": 74, "convert_leaky_relu_to_dpu_vers": 74, "convert_lstm_to_customlstm": 100, "convert_matmul": 9, "convert_nchw_to_nhwc": [55, 57, 100, 257, 292, 299], "convert_norm": 9, "convert_np_to_float": 99, "convert_np_to_float16": 99, "convert_onnx_to_onnxtxt": 100, "convert_onnx_to_torch": 22, "convert_onnxtxt_to_onnx": 100, "convert_ops_to_modul": 9, "convert_opset_vers": 100, "convert_qdq_to_qop": 100, "convert_quant_to_float": 100, "convert_reduce_mean_to_dpu_vers": 74, "convert_reduce_mean_to_global_avg_pool": 53, "convert_resize_fs_to_pof2": 100, "convert_s8s8_to_u8s8": 100, "convert_shared_initializer_to_uniqu": 100, "convert_sigmoid_to_hard_sigmoid": 74, "convert_softmax_to_dpu_vers": 74, "convert_split_to_slic": 53, "convert_tensor_float16_to_float": 99, "convert_tensor_float_to_float16": 99, "convert_to_bfloat_before_bfp": 257, "convert_torch_to_onnx": 22, "convert_transform": 52, "convert_transforms_pipelin": 52, "convert_u16s8_to_s16s8": 100, "convert_u16u8_to_u8u8": [100, 292], "convertavgpooltodpuvers": 257, "convertbn2d2convqopass": 217, "convertbntoconv": 257, "convertclip2reluqopass": 215, "convertcliptorelu": 257, "converthardsigmoidtodpuvers": 257, "convertleakyrelutodpuvers": 257, "convertopsetvers": 257, "convertqdqtoqoptransformspipelin": 51, "convertreducemean2gapqopass": 217, "convertreducemeantodpuvers": 257, "convertreducemeantoglobalavgpool": 257, "convertsigmoidtohardsigmoid": 257, "convertsoftmaxtodpuvers": 257, "convertsplittoslic": 257, "convet": 51, "convient": 1, "convolut": [202, 253, 257, 290, 295, 325], "convqdqtoqoptransform": 50, "convtranspos": [9, 102, 257, 292, 297, 300], "convtranspose2d": [234, 303, 341], "copi": [95, 336], "copy_func_with_new_glob": 138, "copyright": [269, 275, 277, 293, 294, 295, 296, 297, 298], "copysharedinit": 257, "core": [33, 298, 307], "correct": [39, 55, 181, 258, 301], "correctli": 251, "correspond": [7, 55, 93, 151, 205, 217, 241, 257, 265, 267, 268, 270, 276, 278, 279, 306, 336], "cos_metr": 1, "cosin": [1, 98, 292], "cost": [251, 294, 302], "could": [33, 50, 61, 95, 150, 252, 257, 278, 279, 319, 339], "couldn": 319, "count": [257, 298], "counterclockwis": 338, "counterpart": 251, "coupl": 118, "cover": 284, "cp": [271, 272, 273, 275, 277, 280, 281, 282], "cpp": 341, "cpu": [4, 27, 53, 63, 199, 202, 234, 243, 249, 250, 251, 252, 257, 258, 283, 285, 286, 290, 291, 293, 294, 295, 296, 300, 303, 309, 310, 312, 313, 314, 323, 330, 341], "cpuexecutionprovid": [3, 57, 76, 257, 293, 294, 295, 296, 299, 300], "crash": [33, 50], "creat": [3, 5, 20, 31, 32, 39, 54, 55, 61, 118, 121, 122, 127, 128, 131, 135, 136, 143, 156, 162, 178, 194, 199, 206, 209, 222, 241, 257, 258, 259, 265, 266, 267, 268, 270, 274, 276, 297, 304, 307, 315, 336, 338], "create_calibrator_float_scal": 3, "create_calibrator_power_of_two": 3, "create_sess": 20, "creation": [33, 50, 118], "criteria": 298, "criterion": 318, "critic": [57, 118, 194, 199, 251, 257, 295, 297], "cross": [249, 285, 286, 288, 291], "crossentropyloss": 318, "crosslayerequ": 257, "crucial": 316, "csv": [312, 313, 341], "cuda": [119, 178, 242, 243, 250, 252, 257, 258, 265, 278, 279, 283, 284, 293, 294, 295, 296, 300, 303, 310, 312, 313, 341], "cuda_visible_devic": [316, 317], "cudaexecutionprovid": [257, 284, 293, 294, 295, 296, 300], "cudnn_batch_norm": [219, 228], "cudnn_en": 228, "cuh": 242, "cumbersom": 150, "cur_it": 26, "current": [26, 39, 55, 61, 118, 150, 213, 228, 249, 250, 251, 252, 257, 259, 277, 290, 292, 298, 300, 306, 307, 309, 310, 311, 312, 313, 315, 318, 321, 322, 324, 325, 338, 339, 341], "custom": [55, 78, 86, 150, 199, 250, 257, 258, 264, 278, 279, 283, 290, 293, 294, 295, 296, 298, 300, 315, 322, 341], "custom_mod": [150, 180, 322, 323, 325], "custom_op": 250, "custom_ops_infer_shap": [78, 86], "customer_defined_evalu": [278, 279, 283], "customformatt": 118, "customiz": 341, "customqdq_to_contribqdq": 55, "cut": [9, 249, 257], "cv3": 286, "d": [33, 118, 261, 278, 319, 331], "d0": [261, 331], "d_1": 302, "d_2": 302, "daisi": [283, 285, 286, 291], "damp_perc": [199, 340], "dampen": [199, 257], "dao": 140, "data": [1, 3, 20, 22, 25, 27, 39, 55, 56, 57, 61, 76, 111, 121, 122, 127, 128, 131, 135, 136, 151, 181, 182, 183, 193, 198, 199, 202, 205, 241, 243, 249, 252, 254, 257, 258, 260, 262, 263, 278, 279, 284, 289, 290, 293, 294, 295, 296, 297, 303, 304, 305, 308, 315, 317, 318, 319, 323, 332, 333, 335, 336, 337, 340, 341], "data_batch": 304, "data_dir": [278, 318], "data_fold": 258, "data_it": 258, "data_list": 259, "data_load": [27, 121, 122, 127, 128, 131, 135, 264], "data_num": 20, "data_path": [55, 111], "data_prepar": 304, "data_read": [1, 20, 21, 22, 55, 293, 295, 296, 297], "data_s": [27, 55], "data_tensor": 71, "data_typ": 323, "databrick": [315, 341], "dataclass": [203, 299, 340], "dataclass_inst": 203, "dataclass_pretty_str": 203, "dataload": [1, 27, 76, 121, 122, 127, 128, 131, 135, 136, 182, 193, 198, 205, 257, 264, 278, 279, 283, 301, 303, 306, 319, 332, 336, 337, 341], "dataread": [278, 279, 298], "datas": [257, 283, 294, 295, 296, 298], "dataset": [25, 258, 262, 264, 265, 266, 267, 268, 269, 270, 274, 276, 278, 279, 298, 310, 312, 313, 315, 316, 318, 319, 323, 332, 335, 336, 337], "dataset_path": 317, "datashap": 212, "datatyp": [1, 55, 178, 199, 315], "datatypespec": [199, 341], "datefmt": 118, "dbrx": [180, 315, 341], "dbrx_experts_group": 180, "de": 55, "deal": 251, "debug": [57, 118, 194, 199, 257, 290, 335], "debug_mod": [55, 57, 61], "decid": [283, 298], "decim": 251, "decod": [147, 199, 315, 336, 338, 339], "decompos": 302, "decompress": 251, "deconstruct": 32, "decor": [119, 159], "decoupl": 341, "decreas": [252, 257, 336], "dedic": [54, 257], "dedicate_dq_nod": 53, "dedicated_qdq_pair": 54, "dedicatedqdqpair": 257, "deem": [32, 118, 297], "deep": [56, 150, 182, 193, 198, 249, 251, 293, 295, 301, 319, 337], "deepcopi": 336, "deepseek": [315, 341], "def": [159, 258, 259, 264, 278, 279, 283, 304, 315, 336], "default": [3, 7, 32, 39, 56, 57, 61, 95, 110, 118, 150, 151, 183, 194, 198, 199, 212, 217, 235, 252, 253, 254, 255, 257, 269, 278, 279, 283, 290, 292, 297, 300, 306, 308, 322, 323, 325, 338, 340, 341], "default_bfp16_per_block": 337, "default_config": [278, 279], "default_int8_per_tensor_sym_spec": 303, "default_mx_fp_8_per_block": 301, "default_uint4_per_group_asym_spec": [198, 319, 332], "default_w_bfp16_per_block_config": 337, "default_w_fp8_a_fp8_per_tensor_config": 332, "default_w_int8_a_int8_per_tensor_dynamic_config": 332, "default_w_int8_per_tensor_config": 303, "default_w_mx_fp8_per_block_config": 301, "default_w_uint4_per_group_config": [198, 319, 332], "defer": 199, "defin": [1, 33, 50, 56, 71, 159, 183, 185, 193, 198, 199, 202, 213, 251, 257, 258, 259, 264, 266, 278, 279, 283, 295, 298, 299, 301, 303, 304, 306, 307, 319, 332, 336, 340], "definit": [319, 337], "degrad": [292, 297, 302, 335], "degre": [335, 338], "delet": [55, 119, 330], "delete_directory_cont": 119, "delimit": 311, "deliv": 335, "delta": 319, "demand": [249, 251, 278, 279, 292], "demonstr": [252, 253, 254, 255, 262, 264, 284, 299, 308, 317, 318, 332, 335, 337], "denot": [151, 302, 319], "dens": 293, "densenet121": 276, "depend": [118, 193, 198, 251, 307, 316, 323, 335, 336], "deploi": [249, 252, 257, 290, 294, 297, 298], "deploy": [53, 202, 217, 248, 249, 251, 252, 257, 278, 279, 292, 294, 297, 299, 335, 339, 341], "deprec": 257, "depth": 341, "depthtospac": 300, "depthwise_conv": 253, "depthwiseconv": 33, "depthwiseconv2d": 33, "dequant": [7, 54, 178, 251, 302, 319], "dequantize_data": 55, "dequantizelinear": [7, 92, 107, 257, 300, 341], "dequantizerlinear": 324, "deriv": 243, "desc_act": [199, 340], "descend": 199, "descent": 318, "describ": [33, 50, 118, 263, 275, 281, 315, 333, 340], "descript": 340, "design": [7, 249, 251, 252, 278, 279, 285, 286, 291, 292, 297, 298, 317, 319, 335, 336], "desir": [118, 183, 251, 298, 302], "destruct": [306, 336], "detail": [5, 31, 55, 57, 61, 151, 159, 162, 183, 194, 199, 235, 242, 251, 258, 259, 262, 271, 277, 281, 282, 284, 290, 294, 297, 298, 302, 303, 304, 310, 312, 313, 314, 315, 318, 323, 332, 339, 340], "detect": [286, 288, 341], "detection_imag": 286, "determ": 183, "determin": [3, 32, 118, 183, 199, 241, 251, 252, 253, 254, 255, 257, 260, 262, 290, 298, 301, 302, 332, 335, 336, 340], "develop": [249, 250, 300, 307, 317, 319], "deviat": 252, "devic": [178, 183, 193, 198, 199, 202, 234, 237, 240, 241, 242, 243, 252, 257, 265, 266, 269, 275, 277, 293, 294, 295, 296, 297, 298, 300, 304, 310, 312, 313, 314, 315, 318], "device_id": 318, "devicetyp": [199, 202], "di": [261, 331], "diag": 257, "diagon": 257, "dict": [1, 3, 4, 6, 13, 21, 22, 31, 33, 34, 39, 51, 54, 55, 57, 61, 63, 66, 71, 92, 111, 136, 138, 150, 169, 180, 193, 198, 199, 205, 230, 257, 278, 279, 283, 290, 340], "dictat": 298, "dictionari": [3, 39, 54, 55, 57, 61, 118, 193, 198, 199, 205, 252, 253, 254, 255, 257, 269, 297, 304, 315, 336], "diff": 32, "differ": [1, 9, 39, 55, 57, 61, 151, 183, 194, 199, 202, 205, 251, 252, 253, 254, 255, 257, 260, 261, 265, 266, 267, 268, 278, 279, 292, 293, 294, 297, 298, 300, 302, 305, 306, 309, 315, 319, 331, 332, 336, 341], "difficult": 306, "difficulti": [76, 255, 257, 336], "difflib": 32, "diffus": [249, 288, 329, 341], "diffusers_root": 308, "dilat": [218, 219, 220, 234, 235], "dim": 217, "dimens": [39, 141, 254, 257, 261, 290, 306, 331], "dir": 323, "directli": [110, 138, 140, 193, 198, 213, 217, 244, 250, 251, 252, 257, 265, 266, 267, 268, 269, 270, 274, 276, 283, 285, 291, 292, 293, 307, 318, 322, 325, 338], "directori": [55, 119, 150, 154, 205, 250, 257, 259, 288, 290, 300, 307, 309, 315, 318, 319, 323, 329, 330, 341], "disabl": [61, 252, 306], "disable_shape_inf": [82, 85, 99, 281, 282], "disappear": 293, "discard": [252, 257], "discrep": 319, "discret": [254, 277], "discuss": 277, "disk": [114, 205], "displai": 303, "distanc": [20, 283], "distinct": [262, 332], "distribut": [3, 111, 112, 205, 254, 257, 260, 262, 264, 277, 306, 318, 332, 336], "distribution_plot": 205, "div": [257, 300], "div_1": 286, "divid": [140, 331], "dll": [293, 294, 295, 296, 300], "dlrm": 341, "dm": 319, "dml": 323, "dn": [261, 331], "dnn": 337, "do": [7, 22, 61, 95, 140, 180, 251, 252, 257, 260, 269, 278, 279, 290, 292, 293, 295, 296, 297, 300, 302, 303, 306, 315, 330, 334, 335, 336, 338], "do_constant_fold": 150, "do_onnx_ev": [271, 272, 273, 275, 277, 280, 281, 282], "doc": [61, 205, 228, 250, 319], "docstr": 341, "document": [5, 31, 55, 162, 205, 250, 251, 252, 284, 290, 293, 295, 296, 297, 302, 303, 306, 307, 309, 315, 316, 323, 330, 334, 335, 336, 340, 341], "doe": [56, 140, 198, 228, 241, 252, 257, 286, 290, 307, 330], "doesn": [241, 319], "domain": [254, 277], "don": [39, 140, 315], "done": [140, 257, 319], "dot": 244, "down": [26, 27, 301, 338], "down_proj": [306, 335, 340], "download": [250, 265, 266, 267, 268, 269, 270, 271, 274, 276, 277, 279, 281, 282, 283, 285, 286, 288, 291, 308, 314, 315, 316, 319, 323, 329], "downstream": [150, 320, 322, 325], "dpu": [55, 74, 75, 257, 290], "dpu_leaky_relu_alpha": 55, "dq": [7, 36, 107, 108, 257, 293, 300, 341], "dr": [18, 36, 55], "drop": [252, 257, 278, 279, 283, 301, 335], "drop_ratio": 27, "dropout": 228, "dropratio": 257, "dtype": [7, 114, 178, 198, 199, 202, 301, 303, 310, 312, 313, 318, 332, 335, 336, 337, 340], "dual": 295, "dualquantnod": 297, "duck": 159, "due": [251, 254, 257, 277, 281, 282, 302, 336], "dummi": [302, 303, 336], "dummy_path": 27, "dump": [55, 308], "dump_data_fold": 308, "dump_data_read": [55, 290], "dump_float": [55, 290], "dump_model": [55, 290], "dump_result": [55, 290], "duplic": [95, 257], "duplicatefilt": 118, "dure": [34, 39, 54, 55, 93, 193, 198, 199, 202, 249, 250, 251, 252, 255, 257, 259, 262, 264, 277, 278, 279, 284, 290, 298, 300, 304, 318, 332, 335, 340, 341], "dynam": [57, 61, 79, 198, 199, 249, 252, 257, 258, 262, 265, 285, 286, 290, 291, 293, 294, 296, 297, 299, 303, 319, 322, 332, 335, 337, 339, 340, 341], "dynamic_quantized_model": [271, 272], "dynmaic": 257, "e": [33, 71, 95, 118, 140, 150, 183, 199, 205, 217, 226, 230, 241, 243, 244, 257, 278, 279, 302, 306, 309, 311, 322, 323, 325, 335], "e2m1": [266, 296, 301], "e2m3": [266, 296, 301], "e3m2": [266, 296, 301], "e4m3": [266, 296, 301], "e4m3fn": [249, 341], "e5m2": [249, 266, 296], "e8m0": [199, 296], "e_": 302, "each": [3, 39, 54, 57, 107, 150, 183, 199, 205, 213, 217, 230, 241, 244, 248, 252, 257, 259, 260, 261, 264, 278, 279, 290, 293, 294, 295, 296, 298, 301, 302, 306, 309, 311, 315, 319, 331, 335, 336, 337, 340], "eager": [150, 198, 202, 228, 249, 303, 306, 341], "eager_mod": [150, 198, 199, 202], "earli": [252, 257], "earlier": 250, "early_stop": 27, "earlystop": [257, 283, 294, 295, 296], "eas": [251, 252, 293, 295, 296, 297, 302, 303, 306, 330, 334, 335, 336, 341], "easi": [249, 259, 319, 336], "easier": [181, 253, 257, 290, 298], "easili": [317, 336], "ecosystem": 307, "edg": [241, 249, 252], "edit": [250, 300], "effect": [186, 199, 228, 251, 257, 290, 294, 295, 296, 302, 307, 336, 338], "effici": [181, 248, 251, 252, 257, 265, 266, 285, 286, 290, 291, 292, 294, 295, 296, 297, 298, 318, 319, 335, 336, 339], "eg": 150, "either": [1, 118, 193, 198, 251, 298, 304, 313], "element": [55, 199, 202, 241, 244, 249, 251, 257, 266, 294, 295, 296, 297, 301, 302, 337, 341], "element_dtyp": [257, 296], "element_typ": 55, "eleutherai": 315, "elif": [293, 294, 295, 296], "elimin": [290, 335], "ellipsi": [138, 140, 150, 178, 205], "els": [293, 294, 295, 296, 315, 324], "elucid": 251, "ema": 341, "embed": [183, 303, 325, 341], "embeddingbag": 303, "embeddingtyp": 162, "emerg": 297, "emit": 118, "emploi": [251, 335], "empow": 249, "empti": [3, 57, 95, 118, 150, 199, 205, 257, 290, 336], "enabl": [57, 61, 199, 213, 248, 251, 253, 254, 255, 259, 266, 285, 286, 291, 293, 300, 306, 307, 309, 317, 319, 339, 341], "enable_data_cach": 264, "enable_dpu": 257, "enable_npu_cnn": [55, 57, 252, 253, 255, 257, 278, 279, 284, 299, 300], "enable_npu_transform": [55, 57, 254, 257, 299], "enablesubgraph": [61, 257], "enablevaimlbf16": 257, "encapsul": [57, 151, 183, 194, 199, 307], "encod": [150, 154, 319], "encount": [251, 252, 278, 279, 293, 295, 296, 297, 302, 303, 306, 316, 318, 330, 334, 335, 336], "encourag": 309, "end": [57, 61, 252, 257, 309, 311, 319, 323], "end_node1": 257, "end_node2": 257, "end_node3": 257, "energi": [254, 277], "enforc": [54, 55], "engin": 202, "enhanc": [199, 249, 251, 266, 294, 295, 296, 297, 307, 318, 335], "ensur": [39, 56, 186, 193, 198, 213, 243, 250, 252, 257, 259, 260, 278, 279, 284, 285, 286, 290, 291, 292, 298, 300, 301, 307, 309, 316, 317, 318], "entir": [57, 183, 199, 202, 257, 266, 295, 299, 335, 341], "entri": [31, 338], "entropi": [3, 249, 257, 258, 298, 341], "entropycalibrat": 3, "enum": [5, 31, 55, 156, 162], "enum_data": 259, "enumer": [5, 31, 55, 162, 257, 264], "environ": [32, 250, 252, 257, 265, 294, 300, 306, 309], "eor": 311, "ep": [144, 228, 235, 241], "epsilon": [55, 219, 228, 306], "equal": [55, 181, 199, 217, 242, 250, 251, 253, 257, 285, 286, 288, 291, 297, 336], "equalize_iter": 181, "equalize_merge_bia": 181, "equanl": 5, "equat": [55, 251, 319], "equival": [230, 251, 319, 336], "erf": 300, "error": [32, 57, 118, 183, 193, 194, 198, 199, 205, 250, 252, 254, 257, 260, 277, 285, 286, 289, 291, 295, 296, 302, 305, 306, 311, 335, 336, 341], "especi": [61, 253, 254, 255, 257, 262, 266, 286, 332], "essenti": [56, 193, 198, 278, 279], "estim": [260, 305], "et": [26, 27, 181, 202], "etc": [118, 180, 205, 230, 257, 278, 279, 283, 319, 340], "eval": [1, 198, 228, 290, 301, 303, 309, 310, 312, 313, 317, 319, 332, 336, 337, 341], "eval_mod": 311, "eval_model": 341, "eval_task": 316, "evalaut": 309, "evalu": [1, 100, 278, 279, 283, 297, 298, 318, 319, 327, 329, 341], "evaluatefunct": 257, "evalut": 1, "evalutor": [278, 283], "even": [182, 202, 251, 257, 292, 300, 302], "event": 118, "everi": [55, 301, 338], "ex": [250, 300], "exact": 257, "exactli": 319, "exampl": [1, 5, 31, 33, 39, 55, 61, 77, 82, 85, 87, 93, 95, 99, 110, 111, 112, 118, 150, 159, 162, 182, 198, 212, 240, 241, 242, 244, 249, 250, 251, 257, 262, 264, 271, 272, 273, 274, 275, 276, 278, 279, 280, 281, 283, 284, 285, 286, 290, 291, 293, 297, 298, 299, 300, 301, 302, 304, 306, 307, 308, 310, 311, 312, 313, 314, 315, 316, 319, 323, 332, 335, 336, 337, 338, 339, 340, 341], "example_input": [150, 318, 334], "example_quark_torch_llm_eval_harness_offlin": 309, "exce": [278, 279, 289, 298], "exceed": [289, 290, 298], "except": [32, 118, 299, 340], "excess": 53, "exclud": [5, 39, 53, 54, 55, 57, 61, 199, 226, 257, 285, 286, 291, 306, 313, 315, 323, 339, 340, 341], "exclude_lay": [150, 323, 335, 340], "exclude_nod": [285, 286, 291], "exclude_subgraph": 286, "excludeindic": 257, "excut": [1, 269], "exec": 271, "execut": [3, 32, 57, 150, 250, 253, 257, 265, 266, 283, 284, 290, 292, 297, 298, 300, 315, 316, 319, 323], "execution_provid": [3, 55, 57, 257, 284, 299], "exemplifi": 251, "exercis": 307, "exhaust": 298, "exist": [39, 56, 95, 199, 238, 244, 257, 297, 298, 315], "exist_ok": 264, "expand": [257, 300], "expect": [150, 182, 183, 193, 198, 251, 298, 307, 336], "expens": 183, "experi": [252, 253, 254, 255, 335], "experiment": [249, 257, 297, 300, 307, 318, 341], "explain": [264, 278, 279, 293, 295, 301, 338], "explan": 318, "explicit": [32, 251, 293], "explicitli": [53, 251, 257], "explor": [249, 278, 279, 298, 301], "expon": [183, 202, 244, 249, 257, 265, 266, 285, 286, 291, 294, 295, 296, 297, 301, 302, 337], "exponent_bit_width": 183, "exponential_average_factor": 219, "export": [182, 198, 199, 226, 249, 251, 265, 267, 268, 270, 271, 272, 273, 275, 276, 277, 278, 279, 280, 281, 282, 286, 303, 310, 313, 317, 318, 328, 334, 341], "export2oga": 277, "export_config": [150, 321, 322, 324, 325], "export_dir": [150, 318, 319, 321, 324, 325, 334], "export_gguf_model": [150, 319, 321], "export_hf_model": [322, 325], "export_import_hf_model": [322, 325], "export_onnx": [265, 267, 268, 270, 276, 278], "export_onnx_model": [150, 182, 303, 318, 324], "export_path": [150, 182, 303, 319, 321, 322, 324, 325], "export_quark_model": [150, 325], "export_yolo_to_onnx": 286, "exported_model": 318, "exporterconfig": [150, 151, 318, 319, 321, 322, 324, 325], "expos": 307, "express": [33, 251], "extend": [54, 266, 295], "extens": [249, 250, 252, 257, 317, 329, 341], "extent": 76, "extern": [3, 57, 118, 290, 307, 317], "external_data_loc": 290, "external_data_size_threshold": 290, "extra": [22, 61, 257, 293], "extra_op_types_to_quant": [55, 57, 257], "extra_opt": [3, 4, 18, 21, 22, 36, 39, 54, 55, 57, 61, 63, 66, 252, 253, 254, 255, 257, 259, 278, 279, 283, 284, 293, 294, 295, 296, 297, 298, 299, 300], "extract": [9, 11, 319], "extract_attr_valu": 11, "extract_padding_param": 9, "extract_padding_params_for_conv": 9, "extract_weight_and_bia": 9, "extrem": 335, "f": [33, 118, 138, 264, 271, 315, 319], "face": [286, 287, 301, 304, 315, 316, 328], "face_quant": 286, "facebook": [198, 272, 273, 275, 280, 303, 314, 315, 332, 337], "facilit": [264, 307, 323, 339], "fact": [118, 293, 297, 306, 325, 336], "factor": [3, 39, 151, 199, 202, 251, 257, 261, 263, 298, 302, 318, 322, 331, 333, 335, 336], "fail": 32, "failur": [32, 290, 341], "failureexcept": 32, "fair": 311, "fake": [215, 243, 318, 325], "fake_datatyp": 199, "fake_dtyp": 199, "fake_qu": 318, "fake_quant": 215, "fakequant": [178, 198, 225, 235, 306, 318], "fakequantizebas": [178, 205, 243], "fals": [3, 7, 27, 39, 53, 54, 55, 57, 61, 66, 99, 114, 118, 141, 150, 156, 181, 183, 198, 199, 234, 235, 241, 252, 253, 254, 255, 257, 264, 269, 278, 279, 283, 289, 290, 298, 299, 303, 304, 315, 318, 319, 324, 332, 336, 338, 340], "famili": [301, 341], "fashion": 306, "fast": [18, 22, 57, 140, 250, 257, 288, 293, 295, 296, 299, 319, 338, 341], "fast_finetun": [294, 295, 296], "faster": [140, 250, 251, 252, 257, 265, 266, 284, 289, 293, 294, 297], "fastfinetun": [95, 252, 257, 278, 279, 283, 284, 293, 294, 295, 296, 298], "fatal": [57, 194, 199, 257], "favor": [150, 251], "fc1": 336, "feasibl": [251, 319], "featur": [140, 217, 252, 259, 260, 284, 290, 300, 301, 305, 306, 319, 338, 339], "feature_extraction_util": [198, 205], "fed": 251, "feed": [20, 22, 33, 55, 111, 257], "feel": 335, "few": [250, 251, 297, 302, 310, 319], "fewer": [294, 302], "fewshot": 310, "fid": 308, "field": [71, 154], "figur": [283, 285, 286, 291, 319, 335, 336], "file": [22, 56, 61, 82, 85, 99, 110, 118, 150, 154, 198, 199, 250, 252, 253, 254, 255, 257, 259, 264, 265, 266, 275, 278, 279, 281, 283, 285, 286, 288, 290, 291, 293, 294, 295, 296, 300, 306, 307, 309, 313, 314, 315, 316, 317, 318, 319, 321, 322, 325, 329, 334, 335, 338, 339, 341], "file_path": [199, 264], "filenam": 118, "filter": [118, 298], "final": [33, 50, 217, 251, 257, 274, 301, 302, 337], "final_layer_norm": 336, "find": [1, 33, 39, 50, 55, 107, 108, 140, 150, 244, 271, 278, 279, 298, 304, 319, 335, 341], "find_best_candid": 298, "find_int16_scal": 55, "find_n_candid": 298, "find_node_by_output": [107, 108], "find_quant_scale_zp": 39, "find_quantized_valu": 39, "fine": [57, 202, 249, 257, 260, 266, 283, 285, 286, 291, 295, 301, 305, 316, 335, 341], "finer": [266, 295, 296, 302], "finetun": [257, 288, 293, 295, 296, 299, 335], "finetune_batchs": 316, "finetune_checkpoint": 316, "finetune_dataset": 316, "finetune_datasubset": 316, "finetune_epoch": 316, "finetune_it": 316, "finetune_lr": 316, "finetune_seqlen": 316, "finetune_w_uint4_asym": 316, "finfo": 241, "finit": 99, "first": [39, 198, 241, 250, 257, 269, 290, 300, 303, 306, 307, 311, 319, 336], "five": 269, "fix": [54, 55, 79, 93, 99, 249, 252, 257, 263, 265, 290, 294, 318, 333, 336], "fix_shap": 265, "fixed_se": 27, "fixedse": [257, 294, 295, 296], "fixneuron": [257, 290], "fixshap": 257, "fixtur": 32, "flag": [39, 53, 55, 57, 150, 151, 199, 213, 252, 253, 254, 255, 257, 290, 323, 338], "flaki": 119, "flexibl": [252, 262, 294, 295, 296, 297, 298, 307, 332], "float": [1, 3, 4, 5, 7, 8, 9, 13, 20, 22, 25, 26, 27, 39, 54, 55, 63, 72, 76, 82, 83, 84, 85, 92, 93, 99, 110, 144, 150, 181, 183, 198, 199, 202, 205, 219, 235, 241, 249, 251, 252, 255, 257, 258, 262, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 288, 291, 292, 293, 295, 296, 297, 298, 302, 303, 315, 318, 319, 332, 336, 338, 340, 341], "float16": [3, 7, 80, 81, 82, 85, 202, 241, 249, 251, 257, 258, 281, 282, 297, 302, 303, 323, 324, 339, 340, 341], "float16_convert": 99, "float16spec": 199, "float32": [1, 3, 8, 13, 82, 83, 84, 85, 99, 111, 114, 202, 241, 251, 257, 258, 264, 302, 303, 310, 312, 313, 319, 323], "float8": 251, "float_16_onnx_model_path": [80, 81, 82, 85, 292], "float_32_onnx_model_path": [82, 83, 84, 85, 292], "float_bia": 22, "float_dtyp": 178, "float_model": [258, 264, 318], "float_model_path": [18, 36, 258], "float_onnx_model_path": [292, 298], "float_output": 26, "float_quant": 183, "float_result": 20, "float_weight": 22, "floor": [202, 340], "flow": [311, 341], "flux": 341, "fmodel_path": 21, "fmt": 118, "fn_attr": 54, "fn_type": 54, "fname_out": 156, "focus": 309, "fold": [22, 53, 54, 150, 257, 336], "fold_batch_norm": 53, "fold_batch_norm_after_concat": 53, "fold_relu": 54, "foldbatchnorm": 257, "folder": [250, 259, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 285, 286, 288, 291, 300, 306, 308, 314, 315, 318, 323, 329, 330, 338], "folder1": [98, 292], "folder2": [98, 292], "foldrelu": 257, "follow": [26, 27, 33, 39, 50, 181, 199, 215, 243, 248, 250, 251, 257, 258, 259, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 278, 279, 280, 282, 284, 290, 295, 297, 298, 299, 300, 301, 302, 303, 305, 306, 308, 311, 313, 314, 315, 316, 318, 319, 323, 332, 335, 337, 338, 339, 341], "footprint": [251, 294], "forc": [39, 61, 257, 295, 296], "forcequantizenoinputcheck": [61, 257], "form": [251, 325, 338], "format": [3, 9, 54, 57, 88, 89, 118, 150, 151, 202, 249, 251, 257, 258, 259, 265, 267, 268, 269, 270, 274, 276, 279, 285, 286, 288, 290, 291, 292, 293, 294, 295, 296, 301, 303, 304, 311, 318, 319, 320, 323, 341], "formatexcept": 118, "formatt": 118, "formattim": 118, "former": 297, "formul": 318, "formula": 199, "forthcom": 318, "forward": [7, 8, 140, 144, 178, 217, 241, 243, 336], "found": [33, 39, 107, 108, 182, 250, 257, 298, 302, 318, 336], "four": [150, 292, 315], "fp": 242, "fp16": [57, 257, 290, 293, 308, 319, 323, 330, 335, 341], "fp16_model": [281, 282], "fp32": [57, 92, 257, 285, 286, 291, 293, 301, 317, 318, 319], "fp4": [202, 249, 266, 301, 340, 341], "fp4_e2m1": [257, 296], "fp6": [202, 266, 301], "fp6_e2m3": [202, 249, 257, 296, 340, 341], "fp6_e3m2": [202, 249, 257, 296, 340, 341], "fp8": [150, 180, 199, 202, 240, 241, 242, 249, 266, 301, 303, 308, 322, 324, 335, 341], "fp8_attention_qu": 150, "fp8_e4m3": [202, 257, 296, 301, 332, 340, 341], "fp8_e4m3fn": [249, 341], "fp8_e5m2": [202, 249, 257, 296, 340, 341], "fp8_per_tensor_spec": [332, 340], "fp8e4m3perchannelspec": 199, "fp8e4m3pergroupspec": 199, "fp8e4m3pertensorspec": 199, "fp8e5m2perchannelspec": 199, "fp8e5m2pergroupspec": 199, "fp8e5m2pertensorspec": 199, "fp8\u2460": 315, "fp_model": 128, "fpquantiz": 7, "fpx": 335, "frac": [302, 306, 319, 336, 338], "framework": [32, 248, 251, 252, 278, 279, 290, 293, 317, 319], "frantar": 181, "free": [181, 335, 338], "freez": [150, 198, 303, 318, 319], "freeze_bn_stat": 235, "freeze_model": 226, "freezed_model": 318, "freezed_quantized_model": [303, 319], "freezedfakequant": 198, "frequenc": 241, "friendli": [302, 317, 339], "frobeniu": 26, "from": [1, 4, 5, 8, 9, 20, 22, 25, 26, 27, 39, 53, 54, 55, 57, 61, 63, 76, 82, 85, 95, 99, 105, 107, 108, 114, 140, 150, 151, 181, 198, 199, 202, 205, 228, 235, 243, 249, 251, 252, 253, 254, 255, 257, 258, 259, 260, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 309, 310, 313, 315, 318, 321, 322, 324, 325, 332, 334, 335, 336, 337, 340, 341], "from_float": 241, "from_pretrain": [198, 301, 303, 315, 319, 332, 337], "from_subgraph": 39, "frozen": 336, "ft": 316, "full": [55, 118, 251, 316, 322], "fulli": 257, "func": [99, 140, 159], "funcnam": 118, "function": [7, 31, 33, 50, 118, 186, 193, 213, 215, 217, 243, 251, 252, 257, 258, 278, 279, 283, 290, 293, 304, 315, 318, 322, 324, 325, 334, 341], "function_nam": [138, 140], "functool": 205, "fundament": [33, 50, 183, 338], "further": [226, 251, 257, 277, 278, 279, 301, 318, 338], "furthermor": 295, "fuse": [53, 235, 257, 290, 300, 336, 341], "fuse_gelu": 53, "fuse_instance_norm": 53, "fuse_l2_norm": 53, "fuse_layer_norm": 53, "fusegelu": 257, "fuseinstancenorm": 257, "fusel2norm": 257, "fuselayernorm": 257, "fusion": [257, 290], "futur": [61, 228, 257, 307], "fx": [198, 202, 211, 212, 213, 215, 217, 218, 219, 220, 221, 225, 226, 228, 242, 249, 303, 306, 329, 341], "fx_graph": [150, 198, 334], "fx_graph_mod": [150, 198, 199, 202, 318, 334], "g": [33, 95, 118, 150, 183, 199, 205, 217, 226, 230, 243, 244, 257, 271, 277, 278, 279, 281, 282, 302, 322, 325, 335], "g128": [310, 312, 313, 316], "gain": [249, 306], "gap": 341, "gate": [301, 315], "gate_proj": [306, 340], "gather": 300, "gb": 290, "gelu": [53, 257, 341], "gemm": [9, 95, 102, 254, 257, 277, 292, 297, 300], "gemma2": 341, "genai": 323, "genai_config": 309, "gener": [1, 32, 33, 50, 55, 61, 78, 82, 85, 86, 93, 114, 140, 141, 154, 159, 181, 198, 199, 205, 254, 257, 259, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 289, 290, 297, 298, 301, 302, 304, 309, 311, 313, 318, 323, 325, 333, 334, 339, 341], "generate_initi": 114, "generate_input_initi": 32, "generation_config": [272, 273, 275, 280, 322], "genproto": 159, "get": [3, 8, 9, 22, 34, 55, 111, 141, 144, 150, 215, 242, 244, 257, 272, 273, 275, 280, 288, 301, 303, 319, 329, 337, 338], "get_annotate_tensor": 55, "get_available_provid": [293, 294, 295, 296], "get_bia": 8, "get_clip_min_max": 55, "get_config": [34, 150, 341], "get_datatype_shap": 55, "get_default_config": [252, 258, 259, 299], "get_exclude_nod": 55, "get_export_model": 150, "get_fix_posit": 242, "get_hadamard_matric": 141, "get_input": 264, "get_library_path": [284, 290, 293, 294, 295, 296, 300], "get_min_max_by_ms": 241, "get_min_max_by_percentil": 241, "get_model_typ": 315, "get_modules_optimized_bia": 9, "get_modules_optimized_weight": 9, "get_nested_attr_from_modul": 147, "get_next": [55, 111, 258, 259], "get_output": 264, "get_provid": 284, "get_qdq_to_remov": 55, "get_qmin_qmax_for_qtyp": 55, "get_qrange_for_qtyp": 55, "get_rotation_matrix": 144, "get_tensor_valu": 114, "get_token": 315, "get_weight": 8, "getmessag": 118, "gettempdir": 150, "ggml": 319, "ggml_common_aggr": 319, "ggml_half": 319, "ggml_half2": 319, "gguf": [150, 154, 249, 303, 315, 320, 341], "git": 308, "github": [266, 274, 283, 285, 286, 291, 308], "give": [1, 336], "given": [9, 39, 55, 56, 61, 95, 141, 144, 147, 159, 182, 193, 198, 205, 213, 215, 217, 238, 243, 244, 251, 254, 257, 292, 306], "global": [3, 57, 138, 140, 151, 199, 217, 230, 257, 299, 302, 340], "global_config": [182, 336], "global_quant_config": [57, 182, 183, 198, 199, 230, 252, 253, 254, 255, 257, 258, 259, 284, 293, 294, 295, 296, 297, 299, 301, 303, 318, 319, 332, 336, 337, 340], "globalaveragepool": [53, 217, 290, 300], "goal": [260, 278, 279, 290, 302, 305], "goe": 302, "gold": 309, "good": [286, 301, 335], "gpfa2q": 181, "gpfq": 181, "gpt": [315, 341], "gpt2": 150, "gptj": 150, "gptnext": 150, "gptq": [181, 199, 249, 257, 258, 288, 303, 315, 319, 335, 340, 341], "gptq_config": 315, "gptq_quantized_model": 273, "gptqconfig": [131, 199, 340], "gptqparam": 257, "gptqprocessor": [131, 340], "gpu": [199, 249, 251, 257, 265, 266, 278, 279, 290, 293, 294, 295, 296, 297, 300, 308, 309, 310, 312, 313, 314, 315, 340, 341], "gqa": 341, "gradient": [278, 318], "gradual": 325, "grain": [266, 295, 296, 301], "grant": 323, "granular": [183, 296, 297, 302, 335, 341], "graph": [3, 31, 33, 50, 55, 150, 182, 202, 249, 252, 257, 290, 303, 306, 324, 329, 336, 338, 339, 341], "graph_model": 318, "graph_modul": [213, 215, 217], "graphmodul": [198, 211, 212, 213, 215, 217, 218, 219, 220, 221, 225, 226, 228, 306, 318], "graphtransform": 206, "greater": [99, 252, 257, 279, 290], "greatli": 284, "greedi": 181, "green": [5, 31, 55, 162], "grid": [298, 335], "grok": [315, 341], "group": [150, 151, 199, 202, 218, 219, 220, 234, 235, 251, 257, 265, 285, 286, 291, 294, 295, 296, 301, 302, 303, 319, 323, 331, 335, 340, 341], "group_siz": [150, 198, 199, 301, 315, 316, 319, 321, 323, 331, 332, 335, 336, 337, 340], "groupsiz": 257, "grow": 297, "gs128": [310, 312, 313], "gsm8k": 311, "guarante": [181, 307, 319], "guess": 290, "guess_output_rank": 290, "guid": [251, 278, 279, 284, 288, 298, 301, 304, 319, 329, 335, 337], "guidanc": [259, 283, 285, 286, 290, 291, 315, 335], "guidelin": 309, "guidenc": 308, "gz": [265, 266, 267, 268, 270, 274, 276], "h": [217, 319], "h1": 141, "h2": 141, "ha": [6, 7, 9, 22, 34, 55, 56, 82, 85, 99, 183, 193, 198, 199, 250, 252, 257, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 279, 280, 282, 285, 286, 291, 293, 296, 301, 303, 309, 319, 331, 336, 337, 338], "had": [199, 338], "hadamard": [140, 199, 254, 257, 277, 335, 338, 339], "hadamard_multipli": 140, "hadamard_transform": 140, "half": [9, 257, 297, 302], "half_even": [198, 199, 202, 303, 318, 319, 332, 336, 337, 340], "half_to_even": 337, "halt": [278, 279], "halv": 251, "hand": [251, 296], "handi": 319, "handl": [4, 33, 56, 63, 193, 198, 199, 205, 251, 261, 331], "handler": 118, "har": [312, 313, 341], "hard": [55, 257, 336], "hardmard_transform": 141, "hardsigmoid": 300, "hardwar": [140, 217, 248, 249, 251, 262, 294, 295, 297, 309, 311, 319, 332, 335, 338, 339], "harmon": [254, 277], "hat": 319, "have": [5, 31, 32, 33, 55, 150, 162, 181, 186, 199, 205, 228, 241, 251, 257, 265, 266, 267, 268, 270, 274, 276, 278, 279, 290, 292, 295, 296, 297, 300, 301, 302, 317, 319, 336, 338, 341], "head": 289, "heavi": 183, "hello": [198, 301, 303, 319, 332], "help": [251, 252, 260, 290, 297, 299, 305, 335, 336, 339, 340], "helper": [32, 55, 121, 122, 127, 128, 131, 135, 136, 143, 156, 178, 186, 194, 199, 206, 209, 222, 241], "henc": [254, 277, 302, 336], "here": [1, 180, 182, 251, 252, 253, 254, 255, 258, 266, 284, 288, 293, 294, 295, 296, 297, 302, 304, 306, 310, 311, 315, 316, 329, 332, 336, 338, 339, 340], "hessian": 257, "hf": [254, 271, 277, 281, 282, 310, 312, 313, 314, 315, 320, 323], "hf_format": [310, 312, 313, 315, 316], "hf_format_export": 150, "hf_token": 301, "hidden": [144, 302], "hidden_s": [144, 277], "hidden_st": 144, "hierarch": [57, 183, 194, 199], "hierarchi": 118, "high": [202, 252, 265, 278, 279, 285, 286, 291, 297, 298, 302, 303, 319, 325], "higher": [199, 251, 252, 290, 297, 299, 300, 315, 316, 318], "highli": 339, "highlight": 186, "hip": [295, 296], "histdataread": 111, "histogram": [3, 205, 241, 257, 260, 305, 306], "histogramobserv": 241, "histori": 298, "hold": [307, 336], "hook": 205, "how": [5, 20, 31, 33, 50, 55, 57, 118, 162, 183, 198, 199, 251, 253, 254, 255, 257, 259, 264, 273, 275, 277, 278, 279, 280, 281, 284, 286, 298, 303, 304, 310, 311, 312, 313, 317, 318, 325, 332, 335, 340], "howev": [140, 251, 295, 301, 309, 319, 336, 338, 339], "hpcai": [315, 341], "hqq": [257, 341], "hspace": 302, "html": [61, 205], "http": [61, 138, 202, 205, 235, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276, 277, 279, 280, 283, 285, 286, 291, 308], "hubara": 27, "hug": [301, 304, 315, 316], "huggingfac": [150, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, 281, 282, 310, 314, 315, 319, 320, 323, 337, 341], "human": 118, "hw_emul": 302, "hw_emulation_interfac": 302, "hyper": [278, 279, 340], "hyperparamet": [202, 336, 339], "i": [1, 3, 4, 7, 9, 22, 26, 27, 31, 32, 33, 39, 50, 53, 55, 56, 57, 61, 63, 71, 76, 82, 85, 93, 95, 99, 105, 118, 140, 150, 151, 154, 180, 181, 182, 183, 186, 193, 194, 198, 199, 202, 203, 205, 212, 213, 215, 217, 235, 238, 241, 244, 249, 250, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 274, 276, 277, 278, 279, 283, 284, 285, 286, 288, 289, 290, 291, 292, 293, 294, 298, 299, 300, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 313, 315, 316, 317, 318, 322, 323, 324, 325, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341], "id": [118, 315], "idea": [254, 277, 298, 336, 337, 338], "ideal": [140, 294], "ident": [257, 306, 336, 341], "identifi": [199, 244, 269, 275, 277, 293, 294, 295, 296, 297, 298, 315, 335, 339], "idx": 264, "ieee": 251, "ignor": [159, 341], "ignore_warn": 57, "igpu": 297, "illustr": [251, 297], "ilsvrc": [265, 266, 267, 268, 270, 274, 276], "ilsvrc2012_val_00000236": [265, 266, 267, 268, 270, 274, 276], "ilsvrc2012_val_00000262": [265, 266, 267, 268, 270, 274, 276], "ilsvrc2012_val_00000293": [265, 266, 267, 268, 270, 274, 276], "ilsvrc2012_val_00001079": [265, 266, 267, 268, 270, 274, 276], "ilsvrc2012_val_00002138": [265, 266, 267, 268, 270, 274, 276], "ilsvrc2012_val_00002663": [265, 266, 267, 268, 270, 274, 276], "imag": [98, 217, 257, 258, 259, 265, 266, 267, 268, 270, 274, 276, 283, 285, 286, 290, 291, 297, 308], "image_classif": [283, 285, 286, 288, 291, 329], "image_classification_example_quark_onnx_ryzen_ai_best_practic": 285, "image_fold": 259, "image_folder_1_path": [98, 292], "image_folder_2_path": [98, 292], "imagedataread": 259, "imagenet": [249, 265, 266, 267, 268, 269, 270, 274, 276, 318, 341], "immedi": [39, 251], "impact": [252, 257, 297], "implement": [7, 32, 51, 55, 140, 213, 215, 217, 235, 243, 257, 258, 292, 295, 296, 300, 302, 307, 318, 319, 336, 341], "impli": 257, "implicit": 337, "implicitli": 319, "import": [32, 82, 85, 95, 99, 150, 178, 193, 198, 250, 252, 253, 254, 255, 257, 258, 259, 264, 278, 279, 284, 290, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 306, 318, 319, 321, 324, 332, 334, 336, 337, 340, 341], "import_file_format": [310, 311, 312, 313], "import_hf_model": 322, "import_model": 150, "import_model_dir": [150, 310, 311, 312, 313, 316, 322, 325], "import_model_info": [150, 325], "improv": [18, 27, 61, 183, 186, 193, 198, 252, 253, 254, 255, 257, 266, 284, 285, 286, 290, 291, 297, 298, 318, 335, 338, 341], "in_channel": [234, 235], "in_feat": 336, "in_featur": [237, 336], "in_place_replace_lay": 230, "inc": [269, 275, 277, 293, 294, 295, 296, 297, 298], "includ": [1, 7, 32, 53, 55, 57, 95, 150, 198, 242, 244, 249, 250, 252, 253, 257, 269, 278, 279, 281, 284, 290, 293, 295, 296, 297, 298, 299, 300, 307, 308, 315, 318, 320, 321, 323, 334, 335, 337, 340, 341], "include_auto_mp": [57, 257, 297, 298], "include_cl": [55, 57, 253, 257, 270, 278, 279, 298, 299], "include_fast_ft": [55, 57, 252, 257, 278, 279, 283, 284, 294, 295, 296, 298], "include_rot": [55, 254, 257, 277], "include_sq": [55, 57, 255, 257, 277, 280, 298, 299], "incom": [260, 305], "incorpor": [308, 338], "increas": [292, 302, 336], "incur": [301, 338], "indent": 203, "independ": [266, 331], "index": [257, 297, 322], "indic": [39, 53, 150, 151, 154, 199, 257, 284, 290, 297, 302, 306], "individu": [32, 266, 294, 295, 296, 297, 309], "industri": 337, "inf": [4, 63, 99, 110, 244], "infer": [20, 55, 99, 150, 193, 198, 202, 249, 251, 257, 262, 278, 279, 283, 285, 289, 290, 291, 293, 294, 295, 296, 297, 298, 300, 301, 308, 309, 319, 323, 332, 336, 338, 341], "infer_pack_shap": 341, "infer_shap": [55, 99, 341], "inferdevic": [257, 278, 279, 283, 284, 293, 294, 295, 296], "inferenc": [295, 296], "inference_model": 20, "inferencesess": [20, 55, 264, 284, 290, 293, 294, 295, 296, 300], "infin": [55, 93], "influenc": 199, "info": [57, 72, 78, 86, 118, 150, 169, 194, 199, 257, 303, 319, 322, 330], "inform": [9, 39, 55, 82, 85, 99, 118, 150, 251, 252, 253, 254, 255, 258, 260, 265, 266, 275, 281, 290, 303, 305, 307, 314, 315, 316, 317, 318, 319, 322, 323, 325, 340], "inherit": [4, 39, 54, 63, 121, 122, 127, 128, 131, 135, 136, 143, 156, 178, 194, 199, 206, 209, 222, 241], "init_list": 55, "initi": [3, 4, 7, 32, 39, 55, 63, 92, 95, 105, 114, 118, 202, 235, 251, 257, 278, 279, 296, 298, 315, 318, 319, 337], "initial_lr": 27, "initialize_alpha": 7, "initializers_to_convert": 92, "inp": [264, 336, 340], "inp_data_float": [22, 25], "inp_data_qu": [22, 25], "input": [1, 7, 8, 13, 14, 15, 16, 17, 22, 25, 32, 33, 39, 50, 55, 56, 57, 61, 71, 77, 80, 81, 82, 83, 84, 85, 87, 90, 95, 99, 104, 105, 107, 108, 110, 140, 141, 144, 150, 178, 182, 183, 193, 198, 199, 205, 212, 218, 219, 220, 221, 225, 228, 243, 244, 251, 257, 258, 261, 265, 266, 267, 268, 269, 270, 274, 276, 278, 279, 281, 282, 283, 284, 286, 289, 290, 297, 298, 300, 303, 304, 306, 308, 311, 313, 319, 331, 337, 338, 341], "input1": 264, "input1_nam": [259, 264], "input2": 264, "input2_nam": [259, 264], "input_1": 257, "input_2": 257, "input_arg": [150, 303, 324], "input_data": [22, 264, 284], "input_data_rang": 55, "input_dict": 304, "input_fe": 264, "input_folder_path": 264, "input_height": 259, "input_histogram": 306, "input_id": [198, 301, 303, 304, 319, 332], "input_idx": 1, "input_imag": [286, 308], "input_layernorm": 340, "input_list": 304, "input_model": [71, 76, 93, 110, 111, 112, 292], "input_model_path": [93, 104, 110, 111, 112, 252, 253, 254, 255, 259, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 290, 291, 292, 293, 295, 296, 297, 299], "input_nam": [32, 39, 150, 258, 259, 264, 290], "input_nod": [33, 55, 57, 257, 299], "input_node_map": 55, "input_onnx_model_path": 90, "input_path": [77, 87, 292], "input_qdq_histogram": 306, "input_ref_histogram": 306, "input_ref_histogram_absmean_ch0": 306, "input_ref_histogram_absmean_ch1": 306, "input_scal": 39, "input_shap": [55, 111, 257, 259], "input_tensor": [183, 199, 304, 318, 332, 336, 340], "input_tensor_devic": 243, "input_tensor_nam": 259, "input_width": 259, "inputdataset": 264, "insal": [278, 279], "insensit": 297, "insert": [101, 140, 205, 225, 251, 254, 257, 277, 292, 293, 297, 306, 318, 319, 338], "insert_clip_bfloat16_qdq": 100, "insert_stats_hook": 205, "insid": 33, "inside_layer_modul": [199, 315, 340], "insight": 306, "inspect": 186, "inspir": 335, "instabl": 293, "instad": 199, "instal": [251, 257, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 288, 291, 292, 294, 295, 296, 300, 308, 309, 316, 317, 318, 329], "instanc": [9, 22, 32, 39, 53, 55, 118, 213, 217, 251, 257, 258, 269, 278, 279, 283, 290, 299, 301, 304, 340], "instancenorm": [53, 257, 300], "instanti": [32, 198, 258, 307, 334], "instead": [3, 199, 251, 257, 269, 292, 294, 295, 296, 301, 302, 306, 318, 322], "instruct": [264, 284, 306, 308, 309, 310, 311, 314, 315, 316, 335, 339, 341], "int": [1, 3, 5, 7, 13, 15, 20, 22, 26, 27, 32, 39, 54, 55, 57, 66, 72, 93, 111, 119, 141, 144, 150, 156, 159, 178, 181, 194, 199, 202, 203, 205, 219, 234, 235, 237, 240, 241, 242, 249, 252, 253, 254, 257, 259, 290, 340, 341], "int16": [55, 249, 257, 258, 276, 285, 286, 291, 297, 299, 341], "int16_cnn_accur": 299, "int16_cnn_default": 299, "int16_transformer_accur": 299, "int16_transformer_default": 299, "int16method": 55, "int16scal": 257, "int2": 340, "int32": [54, 150, 202, 249, 257, 258, 290, 297, 341], "int32_bia": 54, "int32bia": 257, "int4": [150, 199, 202, 249, 251, 257, 300, 303, 310, 312, 313, 315, 316, 319, 321, 323, 324, 335, 340, 341], "int4_matmul_nbit": 281, "int4perchannelspec": 199, "int4pergroupspec": 199, "int4pertensorspec": 199, "int8": [55, 150, 199, 202, 249, 251, 257, 258, 265, 266, 267, 268, 270, 273, 274, 275, 276, 277, 280, 282, 285, 286, 291, 293, 296, 297, 299, 300, 301, 302, 303, 308, 315, 317, 318, 324, 332, 335, 336, 337, 338, 339, 340, 341], "int8_cnn_accur": 299, "int8_cnn_default": 299, "int8_per_tenser_dynamic_spec": 332, "int8_per_tensor_spec": [318, 340], "int8_qdq": 275, "int8_transformer_accur": 299, "int8_transformer_default": [273, 275, 277, 280, 282, 299], "int8perchannelspec": 199, "int8pergroupspec": 199, "int8pertensorspec": [199, 340], "int_max": 290, "int_quant": 183, "integ": [7, 27, 55, 92, 183, 202, 241, 257, 263, 290, 297, 300, 301, 306, 333], "integr": [249, 251, 252, 319, 326, 329, 341], "intellig": [295, 298], "intend": [278, 279, 301], "intens": 297, "interact": 182, "intercept": 251, "interest": 302, "interfac": [213, 217, 235, 307, 317, 341], "intermedi": [55, 338], "intern": [31, 33, 55, 199, 317, 341], "interpret": 118, "interv": 55, "inth": 205, "intk": 317, "intquant": 7, "intric": 251, "introduc": [33, 50, 266, 295, 296, 297, 302, 319, 335, 336, 337, 341], "introduct": [248, 302], "introductori": 258, "intuit": 302, "int\u2461": 315, "invalid": [1, 56, 298], "invers": [71, 338], "invoc": 341, "invok": [250, 311, 317], "involv": [199, 251, 258, 260, 263, 297, 303, 305, 307, 318, 319, 333], "ipu": [199, 202, 217], "ir_vers": 105, "irretriev": 251, "is_activation_symmetr": 54, "is_approximately_equ": 55, "is_batchnorm2d_nod": 228, "is_big_endian": 156, "is_cat_nod": 228, "is_clip_with_min_max": 55, "is_conv1d_nod": 228, "is_conv2d_nod": 228, "is_conv3d_nod": 228, "is_convtranspose2d_nod": 228, "is_dropout_nod": 228, "is_dynam": [198, 199, 301, 303, 318, 319, 332, 336, 337, 340], "is_kv_cach": 180, "is_larg": [71, 76], "is_layerwis": 181, "is_leaky_relu_with_alpha": 55, "is_node_needs_annot": 55, "is_ort_version_below": 55, "is_symmetr": 66, "is_weight_symmetr": 54, "isquanttyp": 257, "issu": [99, 118, 252, 278, 290, 293, 295, 316, 318, 335, 341], "itai": 27, "item": [1, 278, 279, 283, 315], "item_forward": 1, "iter": [1, 5, 26, 31, 55, 162, 205, 252, 257, 258, 259, 298, 318, 324], "iter_x_": 1, "iteration_limit": 298, "itisquanttyp": 257, "its": [18, 25, 39, 141, 181, 193, 198, 205, 241, 243, 251, 252, 257, 258, 298, 300, 301, 302, 306, 307, 335, 336, 338], "itself": [32, 251, 325], "j": [315, 341], "jit": 22, "job": [257, 290], "join": [264, 318], "journei": 249, "jpeg": [265, 266, 267, 268, 270, 274, 276], "jpg": [283, 285, 286, 291, 292], "json": [150, 151, 154, 198, 199, 249, 254, 272, 273, 275, 277, 280, 281, 282, 303, 306, 309, 311, 315, 320, 322, 325, 334, 335, 339, 341], "json_export_config": [150, 151, 318, 319, 321, 322, 324, 325], "json_path": [154, 156, 198, 334], "jsonexporterconfig": [150, 151, 318, 319, 321, 322, 324, 325], "just": [118, 293], "k": [140, 141, 249, 302, 315, 338, 341], "k_1": 302, "k_2": 302, "k_proj": [150, 315, 336, 340], "keep": [39, 95, 251, 252, 278, 279, 319, 335], "keep_float_weight": 39, "keep_io_typ": 99, "kei": [61, 150, 251, 252, 253, 254, 255, 257, 258, 297, 298, 301, 303, 307, 315, 319, 323, 335, 336], "kept": [297, 319], "kera": 31, "kernel": [53, 140, 250, 257, 302, 330, 335, 338, 341], "kernel_s": [234, 235], "know": [5, 31, 55, 118, 162, 251], "knowledg": 118, "known": [251, 262, 265, 266, 267, 268, 269, 270, 274, 276, 290, 332], "kv": [199, 249, 303, 323, 335, 338, 341], "kv_cach": [150, 151, 180, 199, 315, 341], "kv_cache_cfg": 340, "kv_cache_dtyp": [150, 315], "kv_cache_fp8": 340, "kv_cache_group": 151, "kv_cache_qu": 199, "kv_cache_quant_config": 199, "kv_group": 150, "kv_layer": 150, "kv_layer_nam": 150, "kv_layers_nam": 180, "kv_scale": 150, "kwarg": [13, 14, 15, 16, 17, 150, 237, 243], "kwd": [5, 31, 55, 162, 183, 202], "l1": [1, 205, 278, 279, 283, 298, 336], "l1_metric": 1, "l2": [1, 20, 98, 257, 278, 279, 283, 292, 297, 298, 341], "l2_metric": 1, "l2norm": [53, 257], "l2target": [257, 297], "label": [205, 264, 269], "lamb_in1k": [252, 265, 267, 268], "lamb_in1k_adaquant_quant": [265, 267], "lamb_in1k_adaround_quant": 268, "lamb_in1k_fix": 265, "lamb_in1k_quant": [265, 267, 268], "languag": [150, 181, 249, 304, 314, 319, 323, 325, 329, 335, 336, 339, 340, 341], "language_model": [288, 304, 314, 315, 316, 329, 335, 336, 338], "larg": [3, 53, 61, 150, 181, 249, 254, 255, 257, 290, 294, 298, 304, 306, 316, 323, 325, 335, 336, 339, 341], "larger": [71, 76, 251, 252, 257, 301, 338], "last": [1, 257, 306, 319], "latenc": [278, 279, 293], "latent": [61, 257], "later": [250, 251, 257, 290, 336], "latest": [150, 293], "latter": 297, "layer": [5, 9, 13, 14, 15, 16, 17, 27, 53, 54, 57, 140, 144, 147, 183, 198, 199, 205, 215, 217, 230, 244, 251, 252, 253, 257, 285, 286, 288, 291, 297, 298, 306, 315, 323, 325, 336, 338, 339, 340, 341], "layer_metadata": 34, "layer_norm": 336, "layer_param": 9, "layer_qinfo": 9, "layer_quant_config": [199, 230, 340], "layer_type_quant_config": [199, 230, 340], "layernorm": [53, 257, 300, 317, 336], "layernormtyp": 162, "layerwis": [181, 183], "lead": [33, 193, 198, 251, 252, 265, 266, 267, 268, 292, 294, 297, 302, 335], "leaderboard": 341, "leakag": 307, "leaki": [55, 257], "leakyrelu": [257, 300], "learn": [26, 56, 57, 150, 151, 182, 183, 193, 194, 198, 199, 249, 251, 252, 257, 285, 286, 291, 293, 295, 296, 301, 318, 319, 337, 338, 341], "learning_r": [285, 286, 291], "learningr": [252, 257, 283, 284, 293, 294, 295, 296, 298], "left": [302, 306, 315], "len": [5, 31, 55, 162], "length": [32, 261, 313, 331], "less": [55, 99, 238, 251, 297, 301, 319, 336], "let": [217, 242, 301, 319, 336, 338], "leve11": 1, "level": [1, 39, 57, 118, 183, 199, 202, 244, 249, 251, 252, 257, 266, 290, 295, 297, 298, 303, 309, 338], "level1": 1, "level2": 1, "level3": 1, "levelnam": 118, "levelno": 118, "leverag": [249, 297, 298, 302, 307, 317], "lfloor": 319, "li": [251, 302], "librari": [20, 119, 150, 250, 251, 257, 290, 300, 301, 307, 317, 322, 325, 336, 337], "light": [249, 326, 329, 341], "lightweight": 317, "like": [39, 55, 61, 140, 199, 249, 251, 254, 257, 283, 286, 290, 293, 298, 299, 301, 306, 307, 319, 330, 335, 341], "likelihood": 298, "limit": [55, 93, 228, 257, 290, 294, 297, 298, 310, 311, 314, 315, 341], "lin1": 336, "line": [118, 203, 250, 283, 285, 286, 291, 332, 338], "linear": [55, 140, 199, 221, 230, 237, 251, 303, 315, 317, 325, 336, 339, 340, 341], "linear1": 336, "linear2": 336, "linear_lay": 317, "linear_mlp_fc1": 315, "linear_mlp_fc2": 315, "linear_o": 315, "linear_qkv": 315, "lineno": 118, "link": [250, 252, 265, 267, 268, 270, 276, 294, 295, 296, 300, 304, 309, 324], "linux": [249, 258, 293, 294, 295, 296, 300, 303, 340, 341], "list": [1, 3, 4, 5, 9, 20, 22, 25, 31, 32, 33, 34, 39, 50, 53, 54, 55, 57, 61, 63, 66, 74, 76, 95, 99, 107, 108, 111, 136, 150, 151, 162, 180, 193, 198, 199, 205, 213, 226, 283, 297, 298, 300, 309, 310, 322, 325, 336, 338, 340], "littl": [255, 302, 319], "ll": [186, 338], "llama": [150, 254, 257, 277, 282, 288, 301, 306, 309, 310, 312, 313, 314, 317, 321, 323, 335, 336, 338, 339, 341], "llama2": [150, 271, 275, 277, 281, 282, 310, 312, 313, 315, 317, 319, 321, 322, 323, 340, 341], "llama2_checkpoint_fold": 314, "llama3": [150, 321, 341], "llamamodelwrit": 156, "llayernorm": 317, "llinear": 317, "lllyasviel": 308, "llm": [249, 254, 257, 309, 310, 312, 313, 319, 323, 335, 336, 338, 339, 341], "llm_eval": [309, 310, 311, 312, 313, 316], "llm_prune": 314, "llm_ptq": [315, 335, 336, 338], "llm_qat": 316, "llm_util": [322, 325], "lm": [312, 313], "lm_head": [315, 323, 335, 339, 340], "lmhead": [310, 312, 313], "load": [9, 55, 118, 150, 180, 199, 258, 259, 278, 279, 297, 303, 310, 312, 313, 318, 319, 325, 341], "load_model": [82, 85, 99], "load_param": [198, 334], "load_pre_optimization_config_from_fil": 199, "load_quant_algo_config_from_fil": 199, "load_state_dict": [318, 341], "load_weight_and_bia": 9, "loader": [259, 303], "local": [250, 310, 312, 313, 319], "locat": [290, 300], "log": [150, 257, 290, 316, 318], "log2": 301, "log2t": 242, "log_dir": 205, "log_period": 27, "log_severity_level": [57, 194, 199, 257], "logger": 118, "logic": 307, "logit": 323, "logperiod": 257, "logrecord": 118, "long": [32, 302, 319, 330, 336], "longer": [3, 252, 257], "longmessag": 32, "look": [32, 39, 107, 338], "lookup": [5, 31, 55, 162], "loop": [257, 298], "lose": 255, "loss": [3, 22, 26, 98, 199, 251, 252, 257, 265, 267, 268, 278, 279, 285, 286, 291, 292, 294, 297, 301, 318, 335], "lossi": 251, "lost": 251, "lot": 290, "low": [36, 178, 202, 251, 266, 285, 286, 291, 295, 296, 302, 335], "lower": [183, 251, 290, 294, 297, 302, 315, 336], "lower_op": 107, "lowercas": 315, "lpnormal": [53, 300], "lr_adjust": 27, "lradjust": 257, "lsoftmax": 317, "lsq": [249, 318, 341], "lsqobserv": 240, "lstm": 86, "lybrand": 181, "m": [55, 77, 87, 93, 110, 111, 112, 212, 215, 217, 218, 219, 220, 221, 257, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 285, 286, 291, 292, 300, 302, 319, 323, 331, 338], "m_": 302, "machin": [57, 61, 151, 183, 194, 199, 267, 268, 285, 286, 291], "mae": 199, "magnitud": 294, "mai": [32, 39, 61, 82, 85, 118, 183, 199, 251, 257, 278, 279, 290, 306, 316, 318, 319, 322, 325, 335, 336, 339], "main": [31, 228, 254, 265, 266, 267, 268, 269, 270, 272, 273, 274, 275, 276, 277, 279, 280, 283, 285, 286, 291, 307, 314, 316, 319, 337], "main_import": 150, "mainli": [277, 319], "maintain": [257, 293, 294, 295, 297, 307], "mainten": 199, "major": 319, "make": [3, 118, 181, 251, 252, 253, 254, 257, 259, 277, 290, 293, 294, 297, 298, 306, 311, 340], "makedir": 264, "manag": [251, 260, 305, 323], "mani": [5, 20, 31, 32, 55, 162, 205, 228, 252, 257, 283, 297, 301, 302, 336], "manipul": [31, 251], "manner": [199, 298], "mantissa": [183, 202, 257, 293, 295, 296, 302, 337], "mantissa_bit_width": 183, "manual": [250, 278, 279, 297, 300, 307], "manulli": 339, "map": [3, 31, 39, 54, 55, 99, 150, 183, 184, 199, 251, 278, 279, 318, 322], "margin": [278, 279], "mark": [119, 230], "mark_exclude_nod": 226, "marku": [26, 27], "mask": 289, "match": [31, 33, 50, 61, 107, 108, 140, 243, 252, 264, 301, 319], "match_nod": [33, 50], "matmul": [9, 39, 61, 66, 254, 257, 277, 281, 282, 292, 297, 300, 341], "matmul10": 257, "matmul_4bit": 281, "matmul_4bits_hqq_quantized_model": 281, "matmul_4bits_quantized_model": 281, "matmul_nbit": [273, 281], "matmulconstbonli": [61, 257], "matmulnbit": [257, 273, 288, 341], "matmulnbitsparam": 257, "matmulnbitsquant": 66, "matmulqdqtoqoptransform": 50, "matric": [199, 338], "matrix": [71, 140, 141, 144, 254, 257, 293, 295, 302, 338, 339], "max": [3, 5, 39, 54, 55, 199, 238, 241, 257, 260, 300, 302, 305, 306, 313, 319, 336, 337], "max_attempt": 119, "max_finite_v": 99, "max_loop_num": 72, "max_new_tok": 313, "max_q": 7, "max_seq_len": 315, "max_val": [238, 241], "max_valu": [33, 55], "maxdiff": 32, "maximum": [3, 32, 55, 150, 183, 238, 241, 251, 254, 257, 260, 278, 279, 289, 290, 298, 301, 302, 305, 318], "maxloopnum": 257, "maxpool": [61, 257, 300], "mb": [265, 266, 267, 268, 270, 272, 273, 274, 275, 276, 280], "md": 250, "mean": [3, 55, 144, 183, 217, 238, 257, 260, 261, 285, 286, 291, 293, 295, 296, 301, 302, 305, 306, 311, 315, 319, 331, 335, 336, 339, 341], "meaning": 337, "measur": [98, 257, 278, 279, 297, 298], "mechan": 307, "meet": [53, 72, 74, 262, 283, 285, 286, 291, 298, 300, 332], "member": [5, 31, 55, 156, 162, 252, 257], "memori": [251, 281, 282, 285, 286, 291, 293, 294, 295, 296, 297, 299, 314, 315, 319, 341], "mention": [118, 319], "merg": [150, 151, 272, 273, 275, 280, 290, 335], "merge_batch_norm": 181, "messag": [32, 118, 257, 289], "met": 298, "meta": [271, 277, 281, 282, 301, 306, 310, 312, 313, 314, 315, 323, 335, 338, 339], "metadata": [31, 33, 225, 226, 319], "meteor": [309, 310, 312, 341], "meth": 159, "method": [3, 5, 31, 32, 39, 54, 55, 57, 138, 150, 162, 178, 183, 199, 202, 249, 250, 251, 252, 254, 257, 258, 261, 262, 278, 279, 283, 285, 286, 291, 294, 298, 303, 304, 306, 307, 317, 318, 319, 322, 331, 332, 335, 337, 338, 339, 340, 341], "method_nam": 138, "methodnam": 32, "methodologi": 335, "metric": [1, 205, 269, 278, 279, 283, 292, 297, 298, 306, 309, 311, 318, 341], "metrics_output_dir": [309, 312, 313], "mi210": 308, "micro": [249, 257, 269, 275, 277, 293, 294, 295, 296, 297, 298], "microexpon": [266, 297, 341], "microsc": [249, 295, 297, 315, 341], "microsoft": [283, 285, 286, 291, 314, 315, 316], "middl": 183, "might": [199, 250, 251, 252, 257, 259, 284, 289, 290, 292, 300, 301, 319, 336, 338], "migrat": [255, 257, 325], "millisecond": 118, "min": [3, 39, 54, 55, 199, 238, 241, 257, 260, 300, 305, 306, 319], "min_max": 340, "min_positive_v": 99, "min_q": 7, "min_val": [238, 241, 319], "min_valu": 55, "mind": [159, 251, 335], "mini": [257, 311, 314, 315, 316, 341], "minim": [3, 241, 251, 252, 257, 260, 278, 279, 285, 286, 291, 294, 297, 302, 305, 319, 335, 336], "minimum": [3, 55, 183, 199, 238, 241, 250, 251, 254, 257, 260, 300, 305, 318, 319], "minmax": [3, 57, 249, 254, 257, 258, 278, 279, 283, 285, 286, 291, 293, 294, 295, 296, 297, 298, 300, 303, 305, 317, 341], "minmaxcalibrat": 3, "minms": [3, 249, 252, 253, 255, 257, 258, 278, 279, 284, 285, 286, 289, 291, 298, 299, 300, 341], "minmse_mod": 3, "minmsemod": 257, "minor": [265, 266, 267, 268, 301], "minut": [250, 330], "miopen_batch_norm": 228, "misalign": [193, 198], "mislead": 341, "mismatch": 341, "mistral": [315, 341], "mistralai": [314, 315, 341], "mit": [269, 275, 277, 293, 294, 295, 296, 297, 298], "mitig": [254, 265, 267, 268, 277], "mix": [39, 57, 150, 249, 257, 288, 294, 299, 324, 341], "mixed_precis": 276, "mixedprecisiontensor": [257, 297], "mixtral": [314, 315, 341], "mkdir": [265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 285, 286, 291, 316], "mlcommon": 308, "mlp": [199, 306, 335, 336, 340], "mmlu_manag": 310, "mobil": 252, "mobilenet": [253, 318], "mobilenetv2": 318, "mobilenetv2_050": [252, 265, 267, 268], "mode": [3, 4, 7, 25, 39, 54, 55, 57, 61, 63, 150, 198, 199, 202, 228, 249, 257, 297, 303, 306, 307, 309, 311, 329, 330, 340, 341], "model": [1, 3, 4, 5, 8, 18, 20, 22, 31, 32, 33, 34, 39, 50, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 72, 74, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 95, 99, 104, 106, 107, 108, 110, 111, 112, 114, 121, 122, 127, 128, 131, 135, 136, 143, 147, 150, 151, 154, 180, 181, 182, 183, 193, 194, 198, 199, 202, 205, 211, 215, 217, 225, 226, 228, 230, 248, 252, 253, 254, 255, 256, 257, 258, 260, 262, 293, 297, 299, 302, 303, 304, 305, 306, 307, 317, 321, 322, 324, 325, 329, 332, 335, 336, 338, 341], "model_arg": [310, 312, 313], "model_config": 150, "model_copi": 336, "model_decoder_lay": [199, 315, 336, 338, 340], "model_dir": [150, 314, 315, 319, 322, 323, 335, 338], "model_export": [315, 316, 318, 323], "model_file_path": [198, 318, 334], "model_id": 308, "model_info_dir": [150, 322, 325], "model_input": [55, 56, 61, 257, 300], "model_input_nam": 258, "model_max_length": 315, "model_nam": [154, 156, 265, 267, 268, 269, 270, 276, 318], "model_name_exclude_layers_map": 315, "model_name_kv_layers_map": 315, "model_name_or_path": [271, 272, 273, 275, 277, 280, 281, 282], "model_name_pattern_map": 315, "model_output": [55, 56, 61, 257, 300], "model_output_path": 290, "model_path": [3, 22, 55, 99, 111, 277, 317], "model_reload": [310, 312, 313, 316], "model_state_dict": [150, 180], "model_transform": 30, "model_transformer_test": 30, "model_trust_remote_cod": 316, "model_typ": [150, 315, 319, 321, 334], "model_util": 113, "modelexport": [150, 182, 303, 318, 319, 321, 324, 325], "modelimport": [150, 325], "modeloptim": 25, "modelproto": [4, 5, 8, 9, 20, 22, 31, 34, 39, 51, 53, 54, 55, 61, 63, 66, 71, 72, 74, 76, 78, 80, 81, 82, 83, 84, 85, 86, 91, 92, 95, 99, 106, 107, 108, 114, 289], "modelprun": 193, "modelquant": [56, 182, 198, 252, 253, 254, 255, 258, 259, 284, 293, 294, 295, 296, 297, 299, 301, 303, 306, 318, 319, 332, 336, 337, 341], "modeltransform": 31, "modeltransformertest": 32, "modelwrit": 156, "modern": [294, 295], "modif": [335, 341], "modifi": [55, 107, 108, 110, 118, 140, 198, 205, 213, 252, 257, 300, 315, 318], "modified_annotate_input": 55, "modul": [257, 258, 260, 290, 303, 305, 306, 315, 318, 319, 332, 336, 341], "module2inspect": 340, "module_config": 230, "module_nam": 205, "modulelist": [244, 336], "moe": [180, 315, 341], "momentum": [228, 235], "more": [61, 150, 183, 193, 198, 241, 242, 251, 252, 254, 257, 258, 262, 266, 277, 278, 279, 283, 292, 294, 295, 296, 297, 298, 301, 302, 303, 309, 318, 319, 323, 332, 340, 341], "moreov": [205, 290, 322], "most": [182, 257, 265, 266, 267, 268, 270, 274, 276, 290, 293, 294, 306, 309, 315, 319, 339], "mostcommon": 257, "motiv": 306, "move": [3, 254, 257, 269], "moving_averag": 3, "mqa": 341, "msbuild": [250, 300], "mse": [183, 199, 241, 249, 257, 303, 305, 335, 341], "msec": 118, "much": [140, 250, 252, 255, 257, 319, 338], "mul": [108, 257, 286, 300, 336], "mulqdqtoqoptransform": 50, "multi": [31, 202, 249, 278, 279, 310, 311, 312, 313, 336, 341], "multi_gpu": [310, 312, 313, 314, 315], "multilay": 199, "multimod": 310, "multipl": [22, 33, 53, 119, 140, 249, 257, 294, 295, 313, 314, 315, 319, 341], "multipli": [140, 257], "multiprocess": 205, "must": [32, 54, 150, 156, 254, 257, 290, 292, 300, 301, 304, 311, 321, 336], "mx": [54, 199, 202, 249, 251, 288, 295, 335, 340, 341], "mx4": [266, 295, 297, 341], "mx6": [202, 249, 266, 295, 297, 340, 341], "mx6spec": 199, "mx9": [202, 249, 266, 297, 340, 341], "mx9spec": 199, "mx_element_dtyp": [199, 301], "mxattribut": [257, 296], "mxfixneuron": [6, 257, 296], "mxfp4": [296, 297, 315], "mxfp4e2m1": 266, "mxfp6": 296, "mxfp6_e2m3": 297, "mxfp6_e3m2": 297, "mxfp6e2m3": [266, 315], "mxfp6e3m2": [266, 315], "mxfp8": 296, "mxfp8_e4m3": 297, "mxfp8_e5m2": [297, 341], "mxfp8e4m3": [266, 315], "mxfp8e5m2": [266, 315], "mxint8": [266, 296, 297, 315], "mxquantiz": 6, "mxspec": 199, "mx\u2462": 315, "my_collate_fn": 304, "mymodel": 336, "mysubmodul": 336, "n": [55, 140, 141, 217, 228, 257, 302, 308, 319, 338], "n01440764": [265, 266, 267, 268, 270, 274, 276], "n01443537": [265, 266, 267, 268, 270, 274, 276], "n15075141": [265, 266, 267, 268, 270, 274, 276], "n_bin": 205, "na": [318, 341], "nagel": [26, 27, 181], "naiv": 306, "name": [1, 3, 5, 31, 32, 39, 53, 54, 55, 57, 61, 102, 107, 108, 114, 118, 138, 150, 151, 154, 162, 199, 205, 243, 244, 250, 251, 252, 257, 258, 259, 264, 265, 266, 267, 268, 269, 270, 271, 276, 290, 293, 295, 296, 297, 300, 302, 303, 306, 308, 314, 315, 316, 319, 322, 330, 334, 335, 336, 338, 340, 341], "named_modul": 230, "nan": [99, 244, 341], "narrow": 336, "nativ": [228, 257, 290, 293, 300, 319, 341], "native_batch_norm": 228, "native_funct": 228, "natur": 295, "navig": [244, 338], "nbit": 257, "nchw": [57, 87, 257], "nchw_onnx_model_path": 292, "ndarrai": [1, 8, 9, 13, 20, 22, 25, 55, 71, 99, 111, 114, 205], "nearest": [202, 301], "necessari": [32, 55, 56, 193, 198, 225, 250, 254, 257, 258, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 285, 286, 290, 291, 292, 301, 307, 316, 338], "necessarili": 302, "necessit": [251, 262, 332], "need": [1, 9, 22, 32, 33, 55, 71, 82, 85, 95, 99, 118, 140, 150, 154, 180, 183, 186, 213, 215, 217, 226, 249, 251, 252, 257, 258, 262, 264, 265, 266, 267, 268, 269, 270, 274, 276, 278, 279, 283, 292, 293, 294, 295, 296, 297, 298, 300, 301, 303, 308, 315, 317, 318, 319, 325, 332, 336, 338, 339], "neg": [99, 110, 183], "nest": [147, 169, 244], "network": [150, 181, 198, 202, 244, 249, 251, 252, 285, 286, 289, 291, 293, 294, 297, 334, 336, 337, 338], "neural": [27, 54, 181, 198, 202, 244, 249, 251, 252, 285, 286, 291, 293, 294, 297, 337], "neuron": 54, "new": [3, 33, 34, 39, 50, 243, 244, 250, 251, 257, 266, 274, 283, 291, 297, 300, 301, 307, 313, 315, 319, 338, 339], "new_model": [82, 85, 99], "new_modul": 244, "new_onnx_model": [82, 85, 95, 99], "new_tensor": 99, "new_valu": 243, "newer": 250, "next": [1, 55, 111, 258, 259, 318, 319, 324], "nhwc": [57, 87, 257], "nhwc_onnx_model_path": 292, "nibbl": 319, "night": 308, "nn": [9, 22, 25, 121, 122, 127, 128, 131, 135, 136, 138, 140, 143, 144, 147, 150, 182, 193, 198, 199, 205, 217, 228, 230, 244, 303, 315, 317, 318, 336, 341], "nnapi": 61, "nndct": 242, "nndct_fix_kernel": 242, "nndctfixneron": 242, "no_cuda": [271, 272, 273, 275, 280, 281, 282], "no_grad": 336, "no_merge_realq_config": [150, 322, 325], "node": [3, 5, 9, 13, 31, 33, 39, 50, 53, 54, 55, 57, 61, 82, 85, 92, 95, 107, 108, 150, 217, 225, 226, 228, 251, 257, 285, 286, 289, 290, 291, 292, 297, 300, 324, 336], "node_block_list": 99, "node_metadata": [31, 51], "nodedef": 33, "nodeproto": [9, 33, 39, 55, 107, 108], "nodes_list": 55, "nodes_to_exclud": [4, 5, 39, 53, 54, 55, 57, 61, 63, 66, 74, 257, 299], "nodes_to_quant": [4, 5, 39, 53, 54, 55, 57, 61, 63, 74, 257, 299], "nodes_to_remov": 54, "nodetre": [33, 50], "nodetyp": 31, "noinputqdqshar": 257, "non": [61, 205, 289, 306, 336], "none": [1, 3, 4, 7, 8, 9, 13, 20, 22, 25, 27, 31, 33, 34, 39, 51, 53, 54, 55, 56, 61, 63, 66, 95, 99, 107, 108, 110, 111, 114, 118, 119, 138, 140, 141, 144, 150, 151, 154, 169, 178, 180, 182, 193, 194, 198, 199, 205, 213, 215, 217, 218, 219, 220, 221, 230, 235, 240, 241, 242, 243, 244, 252, 253, 254, 255, 257, 258, 259, 283, 284, 290, 298, 299, 304, 315, 336, 340], "nonoverflow": [3, 55, 249, 257, 258, 298, 300, 341], "nonscaledfakequant": 243, "nonscaledrealquant": 178, "norm": [9, 26, 53, 257, 297, 298], "normal": [53, 144, 252, 257, 259, 290, 304, 319, 336], "normalization_lay": 317, "notat": 244, "note": [1, 39, 53, 56, 150, 198, 217, 228, 250, 257, 265, 267, 286, 290, 295, 306, 307, 308, 309, 311, 318, 319, 325, 336, 337], "noteworthi": 310, "notic": [300, 338], "novelti": 302, "novocab": 159, "now": [193, 198, 250, 278, 279, 301, 311, 315, 319, 324, 334, 337, 339, 340, 341], "np": [1, 71, 264], "np_arrai": 99, "npu": [53, 54, 57, 257, 293, 297, 299, 300, 309, 311], "npu_cnn": [249, 257, 290, 300], "npu_transform": [249, 257, 300], "npulimitationcheck": 257, "npy": [259, 269, 292], "num": [278, 308], "num1": 215, "num2": 215, "num3": 215, "num4": 215, "num_batch": 27, "num_bin": 3, "num_calib_data": [277, 314, 315, 323], "num_channel": 144, "num_eval_data": 313, "num_fewshot": [310, 311], "num_it": [285, 286, 291], "num_iter": 27, "num_quantized_bin": 3, "num_sampl": 264, "num_work": 264, "numbatch": 257, "number": [3, 33, 50, 118, 144, 183, 202, 205, 251, 252, 254, 255, 257, 259, 265, 278, 279, 285, 286, 291, 293, 294, 295, 296, 298, 301, 302, 308, 310, 313, 337, 338], "numer": [118, 251, 257, 266, 293, 295, 296, 297], "numiter": [252, 257, 283, 284, 293, 294, 295, 296, 298], "numpi": [1, 8, 9, 13, 20, 22, 25, 55, 71, 99, 111, 114, 205, 257, 264, 278], "numtarget": 257, "nvidia": 284, "o": [199, 264, 283, 285, 286, 291, 318, 323, 338], "o_proj": [199, 338, 340], "obj": 147, "object": [3, 32, 55, 56, 82, 85, 95, 99, 147, 150, 193, 198, 213, 257, 265, 266, 267, 268, 270, 274, 276, 286, 288, 295, 296], "observ": [183, 198, 199, 243, 251, 301, 303, 318, 319, 332, 336, 337, 340], "observer_cl": [198, 199, 301, 303, 318, 319, 332, 336, 337, 340], "observer_method": 340, "observerbas": [199, 241, 340], "obtain": [3, 22, 55, 93, 257, 260, 271, 277, 281, 282, 301, 314, 315, 318, 319], "occupi": 251, "occur": [251, 290, 294], "ocp": [296, 341], "ocp_fp8_e4m3": 303, "ocp_fp8e4m3": 341, "ocp_mxfp4": 303, "ocp_mxfp6": 303, "ocp_mxfp8_e4m3": 303, "ocp_mxint8": 303, "off": [249, 250, 278, 279, 290], "offer": [251, 252, 257, 262, 284, 291, 323, 332, 339], "offici": [228, 304, 319], "offlin": [309, 338, 341], "offset": 251, "often": [251, 260, 286, 292, 304, 305, 335, 338], "oga": [277, 309, 341], "oga_fp32_model": 277, "oga_refer": 311, "oga_valid": 277, "ok": [250, 300], "onc": [257, 307, 323], "one": [1, 22, 39, 95, 150, 217, 241, 244, 250, 251, 257, 261, 266, 278, 279, 283, 290, 292, 297, 302, 306, 307, 319, 322, 331, 332, 336, 337, 338, 339], "ones": [182, 251, 298, 310], "onli": [1, 9, 39, 54, 61, 82, 85, 99, 118, 138, 140, 150, 159, 183, 198, 199, 205, 215, 228, 241, 249, 250, 252, 257, 258, 259, 262, 269, 277, 282, 290, 292, 293, 294, 300, 302, 303, 306, 309, 311, 316, 318, 319, 320, 321, 322, 324, 325, 332, 334, 335, 336, 337, 338, 340, 341], "onlin": [199, 335, 338], "onnx": [150, 151, 182, 217, 226, 248, 250, 251, 253, 254, 255, 256, 257, 259, 260, 261, 262, 265, 266, 271, 272, 273, 274, 275, 276, 280, 281, 284, 298, 300, 303, 318, 319, 320, 329, 341], "onnx_aten": 150, "onnx_aten_fallback": 150, "onnx_evalu": 286, "onnx_export_config": [150, 151, 322, 325], "onnx_fallthrough": 150, "onnx_format": [310, 311, 312, 313], "onnx_ml_pb2": [11, 55], "onnx_model": [8, 9, 20, 22, 82, 85, 95, 99, 108, 273, 275, 280], "onnx_model_path": [71, 76, 264, 293, 294, 295, 296, 300], "onnx_model_path_with_init_shar": 95, "onnx_model_path_without_init_shar": 95, "onnx_path": [1, 278, 279, 283, 308], "onnx_pb": [13, 66, 99], "onnx_valid": [265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 280, 281, 282], "onnx_validate_with_custom_op": 265, "onnxexporterconfig": [150, 151, 322, 325], "onnxmltool": 99, "onnxquant": 39, "onnxquantizedmodel": 72, "onnxruntim": [3, 4, 20, 22, 39, 54, 55, 56, 61, 63, 250, 254, 257, 258, 259, 264, 278, 279, 283, 284, 285, 286, 289, 290, 291, 293, 294, 295, 296, 298, 300, 323, 341], "onnxruntime_extens": 278, "onnxruntime_genai": 323, "onnxruntime_pybind11_st": 289, "onnxruntimeerror": 289, "onnxsim": [257, 292, 300], "onnxtxt": [88, 89], "onto": 297, "oom": 341, "op": [3, 53, 54, 55, 78, 86, 93, 104, 107, 108, 109, 211, 217, 218, 219, 220, 221, 228, 257, 293, 294, 295, 303, 336, 341], "op_block_list": 99, "op_level_per_channel": 39, "op_typ": [33, 95, 257], "op_types_to_calibr": 3, "op_types_to_exclude_output_quant": 54, "op_types_to_quant": [4, 5, 39, 53, 54, 55, 57, 61, 63, 257, 299], "open": 341, "openllm": 316, "oper": [3, 5, 9, 39, 53, 54, 55, 57, 61, 82, 85, 106, 107, 108, 118, 140, 150, 151, 199, 202, 205, 228, 249, 250, 251, 257, 258, 290, 292, 293, 294, 295, 296, 297, 300, 302, 303, 315, 317, 319, 322, 324, 336, 338, 341], "operand": 118, "operaton": 257, "operator_export_typ": 150, "operatorexporttyp": 150, "opset": [90, 150, 257, 290, 300, 341], "opset_vers": [150, 290], "opt": [198, 255, 273, 280, 288, 303, 314, 315, 317, 332, 336, 337, 341], "optim": [5, 7, 8, 9, 22, 25, 26, 39, 54, 57, 120, 140, 150, 181, 183, 193, 198, 199, 202, 226, 249, 251, 252, 253, 254, 255, 257, 258, 262, 277, 278, 279, 284, 285, 286, 287, 290, 291, 294, 295, 297, 298, 299, 303, 316, 317, 318, 332, 335, 336, 338, 339, 341], "optim_algo": 27, "optim_devic": 27, "optimalgorithm": [252, 257, 283, 284, 293, 294, 295, 296, 298], "optimdevic": [252, 257, 278, 279, 283, 284, 293, 294, 295, 296], "optimize_model": [55, 57, 257, 289, 299, 300], "optimize_modul": 22, "optimized_rotation_path": 199, "optimum": [271, 272, 273, 275, 280, 281, 282, 309, 341], "option": [3, 22, 25, 39, 53, 54, 56, 57, 61, 118, 150, 151, 182, 183, 194, 199, 213, 218, 219, 220, 221, 249, 250, 252, 253, 254, 255, 257, 259, 281, 293, 294, 295, 296, 297, 299, 300, 303, 306, 307, 309, 315, 316, 318, 322, 335, 339, 341], "optpassbas": 213, "optpassmanag": 213, "optypepattern": [33, 50], "optypestoexcludeoutputquant": 257, "order": [22, 32, 140, 150, 199, 257, 278, 279, 335, 336], "ordereddict": 33, "org": [202, 205, 235, 277], "organ": [259, 265, 266, 267, 268, 269, 270, 274, 276], "origin": [8, 9, 22, 25, 26, 33, 34, 39, 50, 55, 71, 150, 151, 198, 202, 251, 252, 257, 258, 262, 278, 279, 290, 302, 303, 311, 322, 332, 336, 339], "ort": [55, 257, 264, 284, 290, 293, 294, 295, 296, 300], "orthogon": [71, 338], "ortonnxquant": 39, "ortqdqquant": 54, "os_cpu": 55, "osscar": [194, 314], "osscarconfig": [135, 194], "osscarprocessor": 135, "other": [33, 53, 55, 138, 140, 150, 251, 252, 257, 263, 290, 293, 294, 295, 296, 297, 301, 302, 303, 304, 306, 307, 315, 319, 322, 323, 324, 325, 330, 333, 334, 335, 336, 341], "other_type_lay": 215, "otherwis": [33, 39, 55, 118, 199, 244, 251, 252, 257, 269, 293, 295, 296, 297, 298, 302, 303, 306, 330, 334, 335, 336], "our": [319, 320], "out": [118, 264, 283, 336], "out_channel": [234, 235], "out_data_float": [22, 25], "out_feat": 336, "out_featur": [237, 336], "out_proj": 336, "outlier": [254, 255, 260, 277, 305, 338], "outlin": [283, 285, 286, 291, 307, 335], "output": [1, 3, 8, 20, 25, 26, 39, 54, 55, 56, 57, 61, 71, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 90, 95, 104, 107, 108, 110, 112, 138, 140, 144, 150, 178, 183, 199, 205, 225, 251, 252, 257, 260, 264, 269, 278, 279, 281, 282, 283, 284, 286, 290, 292, 297, 298, 305, 306, 308, 309, 311, 323, 336], "output_dir": [55, 150, 290, 315, 316, 319, 321, 322, 323, 324, 325], "output_file_path": 154, "output_imag": 286, "output_index": 20, "output_model": [93, 110, 292], "output_model_path": [93, 104, 110, 252, 253, 254, 255, 259, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 284, 285, 286, 290, 291, 292, 293, 295, 296, 297, 299], "output_nam": [107, 108, 150, 264, 290], "output_nod": [55, 57, 257, 299], "output_onnx_model_path": 90, "output_pad": [220, 234], "output_path": [77, 87, 111, 112, 277, 292], "output_tensor": [183, 199, 318, 340], "outputindex": [257, 297], "outputqdq": 257, "outputs_path": 311, "outsid": [33, 205], "over": [5, 31, 55, 57, 151, 162, 183, 194, 199, 205, 217, 295, 296, 302, 306, 330, 336], "overal": [252, 257, 296, 297, 336], "overfit": 257, "overflow": [181, 251, 257, 260, 293, 341], "overhead": [281, 282, 338], "overrid": [32, 323], "overridden": [57, 199, 257], "own": [5, 31, 32, 55, 162, 301, 304, 320], "p": [265, 266, 267, 268, 272, 273, 274, 275, 280, 283, 285, 286, 291, 316, 323], "pack": [55, 150, 178, 341], "pack_method": [150, 151, 322, 325], "packag": [250, 252, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 285, 286, 291, 309, 316, 323], "pad": [9, 218, 219, 220, 234, 235, 257, 300, 301], "padding_mod": [234, 235], "padding_sid": 315, "page": [271, 277, 281, 282, 314, 315, 323], "pair": [5, 7, 31, 51, 54, 55, 61, 107, 162, 252, 253, 254, 255, 257, 277, 293, 297, 319, 324], "paper": [26, 27, 181, 202, 252, 295, 336, 337], "para": 55, "param": [7, 9, 20, 22, 25, 26, 55, 78, 82, 85, 86, 90, 91, 93, 95, 99, 104, 106, 178, 194, 199, 212], "param_is_symmetr": 9, "param_typ": 183, "paramet": [3, 7, 9, 22, 25, 26, 27, 39, 53, 55, 56, 57, 99, 107, 108, 110, 147, 150, 151, 183, 185, 186, 193, 194, 198, 199, 202, 241, 243, 244, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 265, 278, 279, 283, 284, 285, 286, 290, 291, 295, 298, 300, 301, 305, 307, 318, 325, 331, 332, 334, 335, 341], "params_dict": 169, "paramt": 7, "paramtyp": 183, "parent": [39, 178, 336, 341], "pars": [228, 307], "parse_options_to_param": 22, "part": [3, 32, 180, 257, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 276, 277, 280, 282, 297, 315, 319, 325, 335, 341], "partial": [205, 318, 341], "particular": [199, 249, 311], "particularli": [260, 293, 294, 295, 297, 305], "pass": [32, 118, 140, 159, 205, 213, 215, 217, 241, 251, 266], "passmanag": [213, 217], "passresu": 213, "past": [250, 281, 300], "path": [1, 3, 20, 22, 55, 56, 61, 71, 76, 90, 99, 104, 110, 114, 119, 147, 150, 154, 156, 159, 181, 198, 199, 205, 250, 254, 257, 258, 264, 265, 266, 267, 268, 270, 274, 276, 278, 290, 297, 300, 304, 306, 308, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 330, 341], "pathdataread": 55, "pathlib": [3, 55, 61, 150, 154, 156, 159, 205], "pathnam": 118, "pattern": [31, 33, 50, 55, 226, 252, 339, 341], "pdf": [202, 235], "peopl": 308, "pep": 159, "per": [39, 54, 57, 61, 199, 249, 251, 253, 257, 258, 261, 267, 268, 290, 298, 301, 302, 303, 306, 318, 319, 331, 335, 338, 339, 340, 341], "per_channel": [4, 39, 54, 55, 57, 61, 63, 199, 202, 257, 299, 340], "per_gpu_eval_batch_s": [271, 272, 273, 275, 280, 281, 282], "per_group": [150, 198, 199, 202, 301, 319, 321, 332, 337, 340], "per_tensor": [199, 202, 303, 318, 332, 336, 340], "perblockbfpobserv": [241, 337], "perblockmxobserv": [241, 251, 301, 340], "percdamp": 257, "percent": [257, 311], "percentag": [199, 257], "percentage_of_processed_input": 181, "percentil": [3, 241, 249, 257, 258, 278, 279, 283, 289, 298, 300, 303, 305, 341], "percentilecalibrat": 3, "perceptron": [199, 336], "perchannel": [112, 257, 315], "perchannelminmaxobserv": [241, 251, 319, 332, 340], "perfectli": 330, "perform": [34, 39, 54, 61, 66, 118, 140, 178, 193, 198, 199, 249, 251, 252, 253, 257, 259, 260, 264, 265, 267, 268, 278, 279, 289, 290, 294, 297, 298, 299, 301, 302, 303, 305, 306, 316, 317, 318, 319, 335, 337, 338, 341], "pergroup": 315, "pergroupminmaxobserv": [198, 241, 340], "perhap": [9, 306], "permiss": [271, 277, 281, 282, 314, 315, 323], "perplex": [309, 310, 313, 319, 338, 339], "persist": 335, "pertensorhistogramobserv": 241, "pertensorhistogramobserverpro": 241, "pertensorminmaxobserv": [241, 251, 303, 318, 319, 332, 336, 340], "pertensormseobserv": [241, 340], "pertensorpercentileobserv": [241, 340], "peun": 193, "phase": [284, 330], "phi": [314, 315, 316, 341], "phi3": 311, "pick": 251, "pickl": 306, "pictur": 269, "pilev": 304, "pileval_for_awq_benchmark": [315, 323, 335], "pin_memori": 264, "pip": [250, 257, 292, 300, 309, 316, 323], "pipelin": [34, 51, 323, 341], "place": [32, 118, 202, 249, 258, 259, 290, 303, 319, 332, 341], "placeholderobserv": [241, 340], "plan": 257, "platform": [248, 249, 250, 257, 278, 279, 290, 294, 300, 339], "pleas": [33, 61, 198, 250, 251, 252, 257, 265, 266, 267, 268, 270, 274, 276, 293, 294, 295, 296, 297, 302, 303, 304, 306, 309, 311, 316, 319, 330, 332, 334, 335, 336, 340], "plot": [205, 306], "plu": [315, 341], "png": [283, 285, 286, 291, 292, 306], "po": [55, 93], "pof2": [72, 93, 202, 340], "point": [3, 7, 31, 39, 54, 55, 93, 118, 183, 202, 249, 251, 252, 257, 258, 260, 263, 264, 266, 278, 279, 285, 286, 288, 290, 291, 293, 295, 296, 297, 302, 303, 305, 306, 315, 318, 322, 325, 333, 338, 341], "pointwis": 336, "pool": [53, 217, 257], "poorli": [265, 267, 268], "popular": [307, 319, 320], "porint": 1, "portion": 118, "pos2scal": [55, 93], "pos_rang": 55, "posit": [55, 93, 99, 110, 257], "possess": 340, "possibl": [1, 150, 186, 228, 252, 255, 260, 283, 297, 298, 302, 305, 319, 341], "post": [26, 27, 181, 199, 215, 249, 251, 252, 257, 262, 278, 279, 283, 284, 285, 286, 291, 318, 319, 327, 332, 336, 339, 341], "post_attention_layernorm": 340, "postcondit": 213, "postprocess": 286, "potenti": [193, 198, 251, 252, 278, 279, 298], "power": [3, 183, 199, 202, 249, 257, 260, 274, 285, 286, 291, 295, 297, 301, 302, 307, 318, 341], "poweroftwomethod": [3, 55, 57, 252, 253, 254, 255, 257, 278, 279, 284, 289, 298, 299, 300], "powoftwocalibrat": 3, "powoftwocollector": 3, "ppl": [271, 272, 273, 275, 277, 280, 281, 282, 314], "pprint": 203, "practic": [249, 252, 288, 297, 336, 337, 341], "practition": 297, "pre": [7, 118, 120, 181, 183, 185, 199, 249, 251, 252, 257, 258, 259, 278, 279, 298, 303, 318, 336, 339, 341], "pre_optimization_config_file_path": 150, "pre_quant_opt_config": [143, 183, 199, 336], "pre_quant_optim": 336, "pre_quantization_optim": [150, 335, 338], "prec": [270, 274, 276], "preced": [53, 336], "precis": [36, 39, 57, 178, 183, 249, 251, 255, 257, 258, 262, 265, 266, 285, 286, 288, 291, 293, 294, 295, 296, 298, 299, 302, 318, 323, 325, 332, 338, 341], "precondit": [213, 215, 217], "predefin": [278, 279, 298], "predict": [260, 264, 302, 305, 306, 309, 311], "prefer": 283, "prefix": [1, 251, 252, 293, 295, 296, 297, 302, 303, 306, 330, 334, 335, 336], "prelu": [257, 300], "prepar": [150, 198, 226, 290, 317, 318, 341], "preparatori": 118, "prepare_calib_dataset": 318, "prepare_data": [265, 266, 267, 268, 270, 274, 276], "prepare_data_load": 318, "prepared_model": 318, "preprocess": [180, 181, 258, 259, 269, 283, 285, 286, 291, 341], "preprocess_import_info": 180, "prequantoptconfig": 199, "presenc": 159, "present": [217, 284, 318], "preserv": [252, 265, 285, 286, 291, 294, 295, 297, 299, 302, 306], "pretrain": [309, 310, 311, 312, 313, 318], "pretrained_config": 150, "pretrainedconfig": 150, "pretrainedmodel": 150, "pretti": 203, "prev_op": [336, 340], "prevent": [199, 257], "previou": [252, 257, 262, 293, 297, 299, 332, 336], "primari": 307, "primarili": [159, 257, 263, 317, 333, 335], "print": [32, 55, 57, 102, 203, 257, 264, 284, 315, 336], "print_a16w8_a8w8_nod": [100, 292], "print_quantize_dynamic_info": 55, "print_quantize_info": 55, "print_summari": 57, "prior": 340, "priorit": [278, 279], "prioriti": [1, 297, 298], "problem": 293, "proceduc": 217, "process": [1, 7, 39, 54, 56, 118, 182, 183, 193, 194, 198, 199, 217, 250, 251, 252, 257, 259, 262, 265, 283, 284, 292, 295, 296, 297, 298, 300, 307, 315, 318, 319, 323, 332, 336, 341], "process_model_transform": 230, "processed_data": 258, "produc": [33, 50, 107, 108, 193, 198, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 297, 298, 318], "product": [251, 252, 293, 295, 296, 297, 302, 303, 306, 330, 334, 335, 336], "profil": 297, "program": [27, 250, 300], "project": [150, 199, 254, 277, 336, 338, 341], "prompt": [250, 300], "proper": 335, "properli": [278, 279], "properti": 33, "proportion": [285, 286, 291], "propos": [26, 254, 277], "proprietari": 325, "protect": 257, "proto": [20, 159, 257], "protobuf": [289, 290], "protocol": 159, "provabl": 181, "provid": [3, 53, 55, 56, 57, 76, 121, 122, 127, 128, 131, 135, 136, 143, 150, 156, 178, 182, 193, 194, 198, 199, 206, 209, 222, 228, 230, 241, 243, 244, 248, 249, 251, 252, 256, 257, 258, 264, 265, 266, 267, 268, 270, 274, 276, 283, 284, 285, 286, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 304, 306, 307, 308, 309, 311, 315, 316, 317, 318, 319, 322, 323, 324, 335, 339, 340, 341], "prune": [128, 135, 327, 328, 329, 341], "pruner": 328, "pruning_algo": 314, "pruning_algo_config": 135, "pruning_model": 193, "psnr": [1, 98, 278, 279, 283, 292, 341], "psnr_metric": 1, "pt": [198, 286, 301, 303, 306, 319, 332], "pth": [150, 198, 318, 320, 325, 334], "pth_path": [198, 334], "ptq": [182, 226, 249, 251, 252, 253, 254, 255, 256, 278, 279, 283, 284, 285, 286, 291, 319, 327, 329, 341], "pure": [211, 257, 300], "purpos": [199, 298, 300, 337], "push": 297, "put": [315, 336], "pwd": 308, "py": [80, 81, 82, 83, 84, 85, 90, 95, 98, 104, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 291, 292, 302, 304, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 323, 335, 338, 341], "py39": 330, "python": [77, 80, 81, 82, 83, 84, 85, 87, 90, 93, 95, 98, 104, 110, 111, 112, 250, 257, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 290, 291, 292, 299, 300, 308, 310, 311, 312, 313, 314, 315, 316, 317, 338, 340], "python3": [314, 315, 318, 323, 335], "pythonpath": 308, "pytorch": [9, 13, 119, 120, 150, 151, 182, 193, 194, 198, 199, 202, 205, 228, 241, 248, 250, 251, 252, 257, 284, 286, 304, 305, 306, 307, 308, 314, 315, 316, 318, 319, 324, 331, 332, 335, 341], "pytorch_exampl": 318, "pytorch_model": [272, 273, 275, 280], "pytorchlight": 317, "pytroch_light": 317, "q": [7, 36, 55, 107, 108, 140, 251, 257, 293, 300, 319, 341], "q4_0": 319, "q4_1": [303, 319], "q8_0": 319, "q8_1": 319, "q_fold": 7, "q_proj": [336, 340], "qat": [202, 219, 226, 249, 251, 327, 329, 341], "qbfloat16": [257, 293, 297, 300], "qbfp": [294, 295, 297], "qbloat16": 297, "qconfig": 317, "qconv1d": 14, "qconv2d": 14, "qconv3d": 14, "qconvtranspose1d": 14, "qconvtranspose2d": 14, "qconvtranspose3d": 14, "qdq": [22, 51, 54, 55, 57, 78, 86, 91, 106, 108, 109, 150, 251, 252, 253, 254, 255, 257, 258, 273, 284, 287, 290, 293, 297, 299, 300, 306, 341], "qdq_op_type_per_channel_support_to_axi": 54, "qdqnputransformerquant": 54, "qdqoptypeperchannelsupporttoaxi": 257, "qdqquantiz": 54, "qfloat16": [257, 300], "qgemm": [9, 15], "qinstancenorm1d": 17, "qinstancenorm2d": [9, 17], "qinstancenorm3d": 17, "qint16": [257, 278, 279, 283, 297, 298, 300], "qint32": [257, 300], "qint4": 257, "qint8": [3, 54, 57, 61, 252, 253, 254, 255, 257, 278, 279, 283, 284, 297, 298, 299, 300], "qk4_1": 319, "qkrotat": 140, "qkv": 336, "qlayernorm": [9, 17], "qlinearop": [4, 39, 54, 63], "qmatmul": [9, 16], "qmax": 55, "qmin": 55, "qmodel_path": 21, "qmx": 296, "qoper": [51, 91, 257, 258], "qparamslinear": 150, "qscale_typ": 317, "qscheme": [183, 198, 199, 301, 303, 318, 319, 332, 336, 337, 340], "qschemetyp": [198, 199, 202, 301, 303, 318, 319, 332, 336, 337, 340], "qserver": 335, "qspec": [178, 240, 241, 242], "qtype": [39, 55], "qualiti": [290, 302, 306], "quant": [20, 120, 199, 215, 217, 251, 255, 257, 258, 273, 288, 303, 308, 318, 332], "quant_algo": [150, 315, 323, 335], "quant_algo_config": [121, 122, 127, 131, 136], "quant_algo_config_file_path": 150, "quant_base_op": 9, "quant_config": [150, 198, 234, 235, 237, 252, 253, 254, 255, 257, 259, 284, 293, 294, 295, 296, 297, 299, 301, 303, 306, 315, 318, 319, 322, 325, 332, 336, 337, 340], "quant_config_file_path": 308, "quant_format": [54, 55, 57, 252, 253, 254, 255, 257, 284, 293, 294, 295, 296, 297, 299, 300], "quant_gemm_op": 9, "quant_info": 13, "quant_matmul_op": 9, "quant_mod": [150, 198, 199, 318, 334], "quant_model": [22, 182, 198, 292, 301, 303, 306, 319, 332, 337], "quant_model_nonsmooth": 336, "quant_model_path": [18, 36], "quant_model_smooth": 336, "quant_modul": [22, 25], "quant_norm_op": 9, "quant_output": 26, "quant_pre_process": 290, "quant_result": 20, "quant_schem": [150, 315, 316, 323, 324, 335, 338], "quant_spec": [243, 336], "quant_typ": [7, 183, 257, 300], "quant_util": [3, 4, 20, 39, 54, 61, 63, 72, 298], "quantconv2d": [218, 234, 251], "quantconvtranspose2d": [220, 234], "quantformat": [57, 252, 253, 254, 255, 257, 258, 284, 290, 299, 300], "quantformatqdq": 258, "quantiat": [55, 111], "quantif": 76, "quantiti": 102, "quantiz": [1, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 17, 18, 20, 22, 25, 26, 27, 34, 36, 39, 51, 53, 54, 55, 72, 92, 95, 98, 104, 107, 108, 127, 131, 140, 143, 150, 151, 178, 181, 182, 183, 185, 186, 248, 250, 254, 255, 256, 258, 260, 264, 284, 287, 303, 304, 305, 307, 309, 310, 311, 312, 313, 316, 317, 321, 322, 324, 325, 327, 328, 329, 330, 336, 341], "quantization_annot": 225, "quantization_candid": 3, "quantization_config": [258, 298, 322, 325], "quantizationconfig": [57, 182, 183, 198, 199, 230, 234, 235, 237, 252, 253, 254, 255, 257, 284, 293, 294, 295, 296, 297, 299, 301, 303, 318, 319, 332, 336, 337], "quantizationmod": [4, 39, 54, 63, 150, 198, 199, 202, 318, 334], "quantizationmodul": [9, 13], "quantizationparam": 39, "quantizationspec": [178, 182, 183, 198, 199, 240, 241, 242, 243, 301, 303, 318, 319, 332, 336, 337], "quantize_bia": 54, "quantize_bias_stat": 39, "quantize_data_pof2": 55, "quantize_dequant": 7, "quantize_diffus": 308, "quantize_dynam": 61, "quantize_fp16": 55, "quantize_initi": 39, "quantize_model": [56, 182, 198, 252, 253, 254, 255, 258, 259, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 284, 293, 295, 296, 297, 299, 301, 303, 306, 318, 319, 332, 336, 337, 341], "quantize_quark": [283, 285, 286, 291, 315, 317, 323, 335, 338], "quantize_stat": 300, "quantize_weight": 39, "quantize_weight_per_channel": 39, "quantizealloptyp": 257, "quantizebia": 257, "quantized_configur": 55, "quantized_model": [258, 273, 275, 277, 280, 282, 284, 290, 316, 318], "quantized_model_path": 258, "quantized_onnx_model_path": 292, "quantized_tensor_typ": [3, 4, 39, 54, 63], "quantized_value_map": 39, "quantizedconvbatchnorm2d": [219, 235], "quantizefp16": 257, "quantizelinear": [7, 22, 107, 257, 300, 341], "quantizerlinear": 324, "quantizewrapp": [9, 13], "quantlinear": [150, 221, 230, 237, 251], "quantschem": 315, "quanttyp": [3, 54, 57, 61, 183, 252, 253, 254, 255, 257, 278, 279, 283, 284, 294, 297, 298, 299, 300], "quark": [248, 250, 253, 254, 255, 256, 257, 260, 261, 262, 264, 265, 266, 271, 272, 273, 274, 275, 276, 280, 281, 284, 290, 292, 298, 299, 300, 302, 304, 305, 307, 310, 312, 313, 314, 317, 320, 321, 322, 324, 327, 331, 332, 334, 335, 338, 339, 340, 341], "quark_0": 250, "quark_debug": 306, "quark_debug_act_hist": 306, "quark_debug_input_pickl": 306, "quark_exported_model": 319, "quark_quant": [278, 279], "quark_safetensor": 323, "quarot": [199, 249, 257, 258, 288, 327, 335, 341], "quarotconfig": 199, "question": 319, "quick": [257, 283, 285, 286, 290, 291], "quicker": 298, "quickli": [299, 315], "quint16": [257, 300], "quint32": [257, 300], "quint4": 257, "quint8": [252, 253, 254, 255, 257, 278, 279, 284, 300], "quit": 319, "qunat": 39, "qweight": 322, "qwen": [314, 315, 338, 341], "qwen1": [315, 341], "qwen2": [314, 315, 338, 341], "qx": 242, "qzero": 322, "r": [55, 199, 251, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 285, 286, 289, 291, 309, 314, 315, 316, 341], "r1": [199, 277, 338, 339, 341], "r2": [199, 277, 338], "r3": [140, 199, 277, 338], "r4": [199, 277, 338], "r4wrapper": 140, "r_config_path": 277, "r_matrix": 71, "ra_in1k": 276, "ra_in1k_mixed_precision_quant": 276, "ra_in1k_quant": 276, "rais": [3, 32, 39, 56, 241, 257, 301], "rand": [264, 304, 318], "randint": [264, 337], "random": [22, 55, 111, 141, 144, 182, 199, 254, 257, 264, 290, 298, 337], "random2": 199, "random_data_reader_input_shap": [55, 290], "random_hadamard_matrix": 141, "random_quant": 292, "randomdataread": [55, 257], "randomdatareaderinputdatarang": 257, "randomdatareaderinputshap": [257, 290], "randomli": [199, 298], "rang": [1, 3, 39, 54, 55, 57, 93, 151, 183, 202, 241, 249, 251, 257, 264, 278, 279, 285, 286, 291, 293, 294, 295, 296, 297, 298, 302, 306, 336], "rank": 290, "rapid": [55, 111], "rate": [252, 257, 314], "rather": [32, 260, 305], "ratio": 257, "raw": [1, 266, 271, 272, 273, 274, 275, 277, 280, 281, 282, 283, 285, 286, 291, 316, 319], "rceil": 319, "rconfigpath": [254, 257], "re": [257, 293], "reach": [278, 279, 297, 301], "read": [257, 258, 319], "readabl": 341, "reader": [1, 20, 22, 55, 56, 257, 258, 290, 297], "readi": [193, 198, 278, 279, 301], "readm": 250, "real": [55, 150, 151, 178, 182, 251, 259, 260, 289, 298, 305, 325], "real_quant": [150, 178, 322, 325], "realli": 302, "realquantizerbas": 178, "reap": 297, "reason": [150, 183, 319], "rec": [265, 267, 268], "receiv": [33, 50, 54], "recipi": 309, "recogn": 159, "recommend": [82, 85, 95, 150, 250, 257, 259, 290, 292, 300, 336], "reconstruct": [26, 285, 286, 291], "reconvert": 290, "record": [118, 241, 260, 305, 318], "recov": 150, "recreat": 55, "recres": 1, "recurs": 39, "red": [5, 31, 55, 162], "reduc": [39, 54, 55, 57, 193, 194, 198, 217, 251, 257, 260, 265, 281, 282, 285, 286, 290, 291, 293, 294, 295, 296, 297, 298, 300, 301, 305, 306, 335, 336, 338], "reduce_mean": 217, "reduce_rang": [4, 39, 54, 55, 57, 61, 63, 241, 257, 299], "reducemean": [53, 257, 300], "reduct": 251, "redund": [82, 85, 290, 300], "ref_input": [1, 306], "refactor": 341, "refer": [1, 55, 61, 228, 251, 252, 253, 254, 255, 262, 264, 265, 266, 275, 281, 284, 288, 290, 293, 294, 295, 296, 297, 302, 303, 304, 305, 306, 307, 314, 315, 316, 317, 318, 319, 323, 329, 330, 332, 334, 335, 336], "referenc": [26, 27], "refin": [260, 298, 305, 341], "reflect": [251, 264], "reg_param": 27, "regard": 226, "regardless": [295, 296], "regist": [20, 243, 265, 266, 267, 268, 270, 274, 276, 290, 293, 294, 295, 296, 300, 307], "register_custom_ops_librari": [284, 290, 293, 294, 295, 296, 300], "register_forward_hook": 205, "regress": [260, 305], "rel": [118, 252, 257, 302, 306, 336], "relat": [55, 252, 253, 254, 255, 316, 319], "relativecr": 118, "releas": [250, 257, 286, 307], "release_vers": 250, "relev": [199, 252, 253, 254, 255, 265, 266, 275, 281, 284, 302, 306, 307, 314, 315, 316, 317, 318], "reli": [298, 308, 309, 315], "reliev": [254, 277], "reload": [198, 234, 322, 341], "relu": [33, 39, 53, 54, 55, 215, 253, 257, 300], "relu6": 33, "relu9": 257, "relu_input": 55, "relu_output_0": 257, "remain": [251, 292, 297], "remov": [1, 51, 54, 55, 82, 85, 104, 105, 106, 107, 108, 251, 257, 298, 337, 338, 341], "remove_bf16_cast": 100, "remove_initi": 55, "remove_initializer_from_input": 100, "remove_nod": 55, "remove_qdq": 100, "remove_qdq_between_op": 100, "remove_qdq_mul_add": 100, "removedropoutnod": 222, "removeinputinit": 257, "removeqdqbetweenop": 257, "removeqdqconvclip": 257, "removeqdqconvgelu": 257, "removeqdqconvleakyrelu": 257, "removeqdqconvprelu": 257, "removeqdqconvrelu": 257, "removeqdqinstancenorm": 257, "removeqdqmuladd": 257, "removeqdqtransform": 50, "removeqdqtransformspipelin": 51, "renam": 250, "reorder": [150, 151, 178, 198, 322, 325], "repeat": 298, "replac": [33, 50, 109, 110, 140, 150, 198, 202, 211, 218, 219, 220, 221, 230, 244, 249, 251, 253, 257, 258, 290, 301, 303, 317, 319, 332, 340, 341], "replace_bfloat16_qdq_cast": 100, "replace_conv2d_qtconv2d": 218, "replace_conv2dbn_quantizedconv_modul": 219, "replace_convtranspose2d_qtconvtranspose2d": 220, "replace_inf_in_onnx_weight": 110, "replace_inf_valu": [110, 292], "replace_inf_weight": [100, 292], "replace_linear_qtlinear": 221, "replaceclip6relu": [253, 257], "repo": [266, 274, 283, 285, 291], "report": [250, 309], "repr": 32, "repres": [33, 39, 147, 182, 241, 251, 257, 262, 263, 278, 279, 290, 294, 295, 296, 298, 301, 302, 304, 315, 332, 333], "represent": [55, 183, 202, 251, 257, 294, 295, 302], "reproduc": 257, "request": [271, 277, 281, 282, 301, 314, 315, 317, 320, 323], "requir": [32, 53, 119, 178, 182, 183, 199, 213, 215, 217, 218, 219, 220, 221, 241, 250, 251, 252, 257, 262, 290, 292, 294, 297, 300, 307, 309, 315, 316, 323, 332, 335], "require_acceler": 119, "require_torch_gpu": 119, "res_orig": 336, "res_quant_nonsmooth": 336, "res_quant_smooth": 336, "rescal": 336, "reserv": [269, 275, 277, 293, 294, 295, 296, 297, 298], "reset": 241, "reset_it": 55, "reset_min_max_v": 241, "reset_model": 150, "reshap": [140, 212, 257, 289, 300, 301], "resiz": [93, 300], "resnet": [266, 274, 283, 291, 318, 341], "resnet152": [253, 269, 270], "resnet152_cle_quant": 270, "resnet152_quant": 270, "resnet18": 318, "resnet50": [259, 266, 283, 285, 288, 291], "resnet50_quant": 259, "resnetv17_conv0_fwd": [285, 291], "resnetv17_stage1_conv0_fwd": [285, 291], "resolv": [272, 273, 275, 280, 316], "resourc": [252, 278, 279, 294, 298], "respect": [32, 257, 302, 313, 319, 338], "respons": [118, 298, 307, 311], "rest": 337, "restor": [150, 251], "result": [1, 20, 33, 39, 55, 82, 85, 118, 249, 251, 252, 257, 259, 262, 266, 277, 283, 286, 298, 316, 332, 335, 336, 338, 341], "retain": [252, 266, 297], "retent": [285, 286, 291], "retrain": [249, 251, 252], "retri": 119, "retriev": [39, 147, 230, 301, 302, 337, 341], "retrieve_dataset": 311, "retry_flaky_test": 119, "return": [1, 3, 9, 20, 22, 26, 31, 33, 34, 39, 50, 51, 53, 55, 56, 78, 82, 85, 86, 91, 93, 95, 99, 106, 107, 108, 110, 111, 118, 147, 150, 159, 193, 198, 199, 213, 228, 241, 244, 258, 259, 264, 278, 279, 283, 289, 298, 304, 315, 336], "return_tensor": [198, 301, 303, 319, 332], "reus": [95, 264], "rewind": 259, "rewrit": 290, "right": [1, 251, 269, 275, 277, 278, 279, 293, 294, 295, 296, 297, 298, 302, 306, 337], "rigid": 252, "rigor": [299, 340], "rm": [271, 272, 273, 275, 280], "rmatrixdim": [254, 257], "rmax": [39, 55], "rmax_overrid": 55, "rmin": [39, 55], "rmin_overrid": 55, "rmin_real_rang": 55, "rmove": 104, "rmsnorm": 144, "robust": [249, 260, 285, 286, 291, 305], "rocm": [250, 258, 265, 284, 293, 294, 295, 296, 303, 341], "rocmexecutionprovid": [293, 294, 295, 296], "roi": 289, "root": [57, 144], "rope": 140, "rotat": [71, 140, 199, 254, 257, 277, 315, 327, 335, 341], "rotate_in_channel": [71, 144], "rotate_in_channels2": 140, "rotate_out_channel": [71, 144], "rotate_out_channels2": 140, "rotated_quantized_model": 277, "rotated_smoothed_quantized_model": 277, "rotation_config": [254, 277], "rotation_config_info": 71, "rotation_matrix": 71, "rotationconfig": [143, 199], "rotationprocessor": 143, "roug": [309, 310, 312, 341], "round": [7, 25, 26, 27, 199, 202, 242, 251, 252, 257, 285, 286, 291, 293, 298, 301, 302, 337, 340], "round_impl": 7, "round_method": [198, 199, 303, 318, 319, 332, 336, 337, 340], "rounding_mod": [257, 295, 296], "roundtyp": [198, 199, 202, 301, 303, 318, 319, 332, 336, 337, 340], "rsmnorm": 339, "rst": [285, 309], "rule": 304, "run": [1, 20, 25, 32, 55, 61, 213, 215, 217, 241, 250, 251, 257, 258, 260, 264, 265, 266, 269, 284, 289, 292, 303, 305, 309, 310, 311, 312, 313, 314, 316, 317, 321, 323, 330, 336, 338, 340, 341], "run_checks_after_each_pass": 213, "run_onnx_model": 55, "runnabl": 341, "running_mean": [219, 228], "running_var": [219, 228], "runtest": 32, "runtim": [3, 39, 55, 159, 249, 250, 257, 259, 262, 284, 290, 294, 298, 300, 319, 332, 341], "runtime_check": 159, "runtime_except": 289, "runtimeexcept": 289, "runwayml": 308, "ryzen": [258, 341], "s16s16_mixed_s8s8": [276, 299], "s16s16_mixed_s8s8_aaw": 276, "s16s8": 96, "s16s8_asw": 299, "s16s8_asws_adaqu": 299, "s16s8_asws_adaround": 299, "s2": 302, "s8s8": 94, "s8s8_aaw": [267, 268, 270, 276, 278, 279, 299], "s8s8_aaws_adaqu": [252, 267, 299], "s8s8_aaws_adaround": [252, 268, 299], "s_": 302, "s_2": 336, "s_b": 302, "sacrific": 293, "safe": 323, "safetensor": [150, 151, 154, 198, 249, 303, 322, 323, 334, 341], "safetensor_path": [154, 156], "safetensors_path": [198, 334], "sai": [306, 336, 338], "same": [7, 33, 50, 151, 182, 199, 202, 228, 243, 251, 257, 262, 269, 285, 286, 290, 291, 293, 295, 296, 297, 311, 315, 319, 322, 331, 332], "sampl": [20, 253, 254, 255, 269, 298, 310, 311, 313, 323, 332], "sample_1": 269, "sample_2": 269, "samsum": 313, "satisfactori": 298, "satisfi": [55, 95, 215, 269], "save": [1, 3, 22, 56, 61, 99, 110, 114, 150, 182, 198, 199, 205, 252, 257, 258, 269, 278, 279, 283, 285, 286, 290, 291, 297, 306, 311, 312, 313, 318, 319, 323, 325, 336, 341], "save_as_external_data": [281, 282, 290], "save_dir": [150, 314, 334], "save_distribution_histogram": 205, "save_metrics_to_csv": [312, 313], "save_model": [82, 85, 99, 114], "save_param": [150, 198, 334], "save_path": [1, 205], "save_prefix": 1, "save_pretrain": 322, "save_pruned_model": 314, "save_tensor_hist": 100, "save_torch_model": 22, "save_weights_hist": 100, "saved_path": 308, "savetensorhistfig": 257, "sb": 302, "scalar": [55, 261, 331, 336], "scale": [3, 7, 39, 55, 72, 93, 141, 150, 151, 178, 183, 199, 202, 240, 241, 242, 249, 251, 252, 253, 257, 258, 260, 261, 263, 266, 274, 278, 279, 285, 286, 291, 293, 294, 295, 296, 297, 303, 305, 318, 319, 322, 325, 331, 333, 334, 335, 336, 340, 341], "scale2po": [55, 93], "scale_clamp_min": [199, 336, 340], "scale_shap": 178, "scale_typ": [183, 198, 199, 303, 318, 319, 332, 336, 337, 340], "scaledfakequant": 243, "scaledrealquant": 178, "scaletyp": [198, 199, 202, 301, 303, 318, 319, 332, 336, 337, 340], "scaling_lay": [199, 336, 340], "scenario": 298, "schedul": 213, "scheme": [199, 202, 251, 252, 254, 257, 258, 302, 303, 306, 317, 318, 340, 341], "score": [308, 309, 312, 313], "scratch": 252, "screen": 257, "script": [266, 269, 290, 317, 319, 338, 339, 341], "sd": 308, "sd1": [308, 341], "sd3": 341, "sdxl": [249, 308, 341], "seamless": [249, 341], "seamlessli": 249, "sean": 202, "search": [1, 107, 108, 269, 288, 323, 335, 341], "search_algo": [278, 279, 283], "search_cache_dir": [278, 279, 283], "search_evalu": [278, 279, 283, 298], "search_forward": 1, "search_metr": [278, 279, 283], "search_metric_toler": [278, 279, 283], "search_model": 298, "search_spac": [278, 279, 283, 298], "search_space_advanc": 283, "search_space_with_gpu": [278, 279], "search_space_xint8": [278, 279, 283], "search_stop_condit": 298, "searched_candid": 298, "searchspac": 1, "second": [241, 257, 298, 311, 335], "section": [235, 248, 265, 266, 267, 268, 270, 274, 276, 303], "see": [5, 31, 55, 159, 162, 202, 242, 258, 259, 271, 277, 281, 282, 293, 303, 306, 309, 310, 311, 314, 315, 336, 338, 339], "seed": [22, 257], "seen": 336, "select": [61, 150, 199, 250, 251, 252, 254, 257, 295, 297, 298, 301, 313, 316, 335], "selective_upd": 27, "selectiveupd": 257, "self": [33, 39, 50, 159, 199, 241, 258, 259, 264, 336], "self_attn": [199, 336, 338, 340], "self_attn_layer_norm": 336, "sensit": [257, 306, 335, 336], "sentencepiecetokentyp": 156, "separ": [7, 39, 257, 290, 297, 311], "seper": 313, "seq_len": [313, 315, 323], "seqlen": 317, "sequenc": [3, 34, 313], "sequenti": 199, "seri": [269, 278, 279], "serial": 322, "serv": [264, 307], "sess_opt": [293, 294, 295, 296, 300], "session": [20, 264, 278, 279, 283, 284, 290, 293, 294, 295, 296, 300], "sessionopt": [284, 290, 293, 294, 295, 296, 300], "set": [8, 9, 22, 32, 33, 50, 55, 56, 61, 82, 85, 99, 150, 186, 193, 198, 199, 205, 252, 253, 254, 257, 258, 259, 262, 263, 277, 283, 284, 289, 290, 292, 293, 294, 295, 296, 297, 298, 299, 300, 303, 304, 307, 310, 311, 312, 313, 315, 319, 332, 333, 338, 339], "set_algo_config": 199, "set_bia": 8, "set_modules_original_bia": 9, "set_modules_original_weight": 9, "set_op_by_nam": 244, "set_weight": 8, "settl": 202, "setup": [32, 199, 310, 319], "setup_config_per_lay": 230, "setup_se": 22, "sever": [1, 251, 256, 257, 258, 266, 295, 296, 298, 302, 319, 335, 336, 339, 340], "sh": [269, 308], "shape": [7, 33, 50, 55, 79, 82, 85, 99, 182, 243, 257, 261, 265, 290, 292, 301, 302, 319, 331, 336], "shape_infer": 290, "share": [95, 151, 199, 202, 249, 251, 252, 257, 262, 265, 266, 285, 286, 291, 293, 294, 295, 296, 297, 301, 303, 306, 309, 330, 332, 334, 335, 336, 337], "shift": [7, 76, 257, 263, 302, 333, 335, 336, 337], "short": [252, 302, 337], "should": [1, 3, 32, 33, 50, 118, 119, 140, 150, 151, 193, 198, 199, 205, 226, 241, 243, 254, 257, 259, 278, 279, 290, 295, 298, 299, 300, 301, 304, 306, 307, 311, 314, 315, 319, 337, 338, 340], "should_quantize_nod": 74, "show": [111, 112, 150, 252, 253, 254, 255, 273, 277, 280, 286, 297, 303, 318, 330], "shown": [290, 302, 311, 319], "shuffl": [264, 304], "sigma": 336, "sigmoid": [55, 61, 257, 286, 300], "sigmoidqdqtoqoptransform": 50, "sign": [99, 202, 257, 295, 296, 302, 318, 335, 337], "signatur": [32, 159], "signific": [249, 252, 257, 297, 335], "significantli": [252, 262, 265, 267, 268, 286, 294, 297, 298, 332, 335, 338], "similar": [6, 98, 99, 235, 251, 278, 279, 292, 318], "similarli": 302, "simpl": [159, 193, 198, 252, 264, 293, 294, 295, 296, 297, 302, 304, 306, 319], "simpler": 251, "simpli": [251, 252, 262, 293, 295, 296, 297, 302, 303, 306, 330, 332, 334, 335, 336, 339], "simplifi": [82, 85, 249, 251, 257, 292, 300, 315, 336, 341], "simplifymodel": 257, "simul": [55, 75, 251, 257], "simulate_bf16": 292, "simulate_dpu_softmax": 35, "simulate_transform": 74, "simulatedpu": 257, "simultan": 297, "sinc": [22, 32, 140, 292, 338], "singl": [8, 22, 25, 32, 217, 257, 296, 301, 302, 309, 336, 337, 341], "single_gpu": 258, "situat": [1, 254, 277, 278, 279], "size": [3, 53, 61, 71, 76, 99, 140, 141, 193, 198, 199, 212, 249, 251, 252, 257, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 285, 286, 289, 290, 291, 300, 301, 302, 314, 315, 318, 319, 323, 335, 338, 340, 341], "skip": [257, 290, 335], "skip_evalu": 316, "skip_finetun": 316, "skip_onnx_shap": 290, "skip_optim": 290, "skip_prun": 314, "skip_quant": [226, 308, 315, 316], "skip_symbolic_shap": 290, "slice": [53, 257, 286, 290, 300], "slice_1": 286, "slightli": 341, "small": [199, 266, 284, 293, 296, 302, 338], "smaller": [53, 251, 296, 302, 331], "smooth": [3, 76, 199, 255, 257, 288, 335, 339, 340], "smooth_config": 335, "smooth_fc_fc": 339, "smooth_quant": [255, 280], "smoothalpha": [255, 257, 298], "smoothed_quantized_model": 280, "smoothli": [278, 279], "smoothquant": [76, 181, 199, 249, 257, 258, 303, 315, 319, 327, 335, 338, 340, 341], "smoothquant_config": 336, "smoothquantconfig": [127, 199, 336, 340], "smoothquantprocessor": [127, 340], "so": [1, 32, 55, 61, 95, 180, 183, 213, 217, 228, 251, 255, 257, 269, 284, 290, 293, 294, 300, 322, 337, 338], "softmax": [75, 257, 300, 317], "sole": 257, "solut": [249, 289, 298, 330], "some": [39, 55, 61, 150, 180, 183, 212, 215, 252, 253, 255, 257, 278, 279, 285, 286, 290, 291, 292, 297, 299, 306, 315, 316, 318, 322, 325, 336, 339, 340, 341], "sometim": [251, 252, 290, 293, 295, 296, 297, 302, 303, 306, 330, 334, 335, 336, 339], "sort": 297, "sourc": [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 50, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 72, 74, 76, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 111, 114, 118, 119, 121, 122, 127, 128, 131, 135, 136, 138, 140, 141, 143, 144, 147, 150, 151, 154, 156, 159, 162, 169, 178, 180, 181, 182, 183, 186, 193, 194, 198, 199, 202, 203, 205, 206, 209, 211, 212, 213, 215, 217, 218, 219, 220, 221, 222, 225, 226, 228, 230, 234, 235, 237, 238, 240, 241, 242, 243, 244], "space": [1, 269, 278, 279, 283, 290, 298, 299, 336, 341], "space1": [278, 279, 283], "space2": [278, 279, 283], "spacetodepth": 300, "span": [265, 266, 267, 268, 270, 274, 276], "spdx": [269, 275, 277, 293, 294, 295, 296, 297, 298], "spec": 319, "special": [150, 180, 228, 249, 336], "special_tokens_map": [272, 273, 275, 280, 322], "specif": [4, 39, 53, 54, 55, 57, 63, 183, 199, 202, 217, 244, 251, 252, 257, 278, 279, 283, 285, 286, 291, 293, 294, 295, 296, 297, 298, 301, 302, 303, 304, 306, 307, 308, 311, 322, 325, 330, 334, 335, 336, 337, 340], "specifi": [3, 31, 32, 39, 53, 54, 55, 56, 57, 61, 107, 108, 110, 118, 150, 151, 154, 182, 183, 193, 198, 199, 205, 226, 241, 244, 252, 253, 254, 257, 258, 259, 278, 279, 290, 292, 297, 298, 306, 309, 310, 312, 313, 318, 323, 336, 340, 341], "specific_tensor_precis": [55, 57, 257, 297], "sped": 338, "speed": [251, 284, 285, 286, 291, 294, 295, 296, 297, 298], "spinquant": [140, 277, 338, 339], "split": [1, 21, 53, 169, 180, 244, 257, 300, 302], "split_large_kernel_pool": 53, "split_model_info": 169, "split_params_for_dbrxexpert": 180, "splite": 1, "splitlargekernelpool": 257, "splitquantmodulecalledoveronc": 217, "sq": 254, "sq_alpha": 277, "sqrt": [140, 338], "squar": [3, 26, 57, 144, 183, 257, 260, 305, 335], "squeez": 300, "src": 228, "ssim": [1, 278, 279, 283], "ssim_metr": 1, "stabil": [295, 318], "stabilityai": 308, "stabl": [150, 205, 249, 252, 257, 308, 341], "stackoverflow": 138, "stage": 251, "stand": 302, "standalon": 300, "standard": [7, 78, 121, 122, 127, 128, 131, 135, 136, 143, 150, 156, 178, 194, 199, 202, 206, 209, 222, 241, 251, 252, 294, 309, 318, 338, 341], "start": [55, 57, 61, 107, 108, 147, 199, 257, 269, 278, 279, 283, 285, 286, 291, 299, 330], "start_bin": 241, "start_node1": 257, "start_node2": 257, "startup": 118, "stat": [183, 205], "state": [31, 144, 249, 251, 252, 293, 295, 296, 297, 302, 303, 306, 319, 330, 334, 335, 336], "state_dict": [150, 180, 319], "static": [4, 26, 39, 54, 63, 159, 198, 199, 249, 252, 257, 258, 259, 260, 262, 278, 279, 283, 301, 303, 305, 306, 322, 324, 332, 334, 335, 337, 338, 339, 340, 341], "static_group": [199, 340], "staticmethod": [278, 279, 283], "statist": [183, 205, 243, 260, 305, 306, 336, 341], "statu": 289, "std": 202, "stem": 257, "step": [39, 118, 249, 250, 251, 253, 257, 258, 278, 279, 283, 284, 290, 292, 297, 298, 301, 303, 315, 318, 332, 336, 337, 339, 341], "still": [7, 39, 301, 303, 319, 332], "stop": [252, 257, 269, 278, 279, 283, 298], "storag": [264, 265, 266, 267, 268, 269, 270, 274, 276, 285, 286, 290, 291, 294, 297, 301, 302, 319], "store": [3, 150, 205, 251, 257, 259, 290, 292, 302, 306, 319, 322, 334, 337], "str": [1, 3, 4, 5, 6, 13, 18, 20, 21, 22, 27, 31, 32, 33, 34, 36, 39, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 74, 76, 92, 99, 107, 108, 110, 111, 114, 118, 119, 136, 138, 140, 147, 150, 151, 154, 156, 169, 180, 182, 193, 198, 199, 203, 205, 226, 230, 234, 235, 243, 244, 257, 258, 259, 278, 279, 283, 290, 315, 340], "straightforward": 251, "strategi": [217, 249, 258, 283, 285, 286, 291, 294, 302, 303, 335, 338, 339, 341], "streamlin": [248, 307], "strength": 317, "strictli": 302, "stride": [218, 219, 220, 234, 235, 241], "strike": [251, 295, 306], "string": [1, 54, 118, 147, 150, 199, 252, 254, 257, 290], "strive": 306, "strongli": 250, "struct": 319, "structur": [71, 108, 159, 193, 198, 250, 252, 288, 298, 319, 329, 339], "stuck": 330, "studio": [250, 300], "stuff": 235, "style": 118, "sub": [33, 50, 71, 257, 259, 266, 295, 300, 302, 341], "sub_1": 286, "sub_block_s": [257, 295], "sub_block_shift_bit": [257, 295], "subblock": 302, "subclass": [32, 307], "subdivid": 301, "subgraph": [21, 39, 57, 61, 257, 286, 341], "subgraphs_to_exclud": [55, 57, 61, 257, 299], "subgroup": 296, "submit": [271, 277, 281, 282, 314, 315], "submodul": 244, "subscal": 302, "subscript": 302, "subsequ": [250, 264], "subset": [265, 266, 267, 268, 270, 274, 276, 302], "substanti": 251, "substitut": 251, "subtract": 301, "subtyp": 159, "success": [250, 251, 290], "successfulli": [55, 290, 303, 319, 321], "suffici": [293, 294, 325], "suit": 290, "suitabl": [31, 257, 297, 341], "sum": 257, "summar": 309, "summari": [57, 205, 306, 310, 312, 313], "summarize_activ": 205, "summarize_weight": 205, "summary_io_quantization_error": 306, "summary_ref_input_error": 306, "summary_ref_output_error": 306, "summary_weight_error": 306, "super": [264, 278, 336], "superior": [249, 318, 335], "suppli": [118, 318], "support": [3, 8, 9, 22, 39, 55, 61, 72, 150, 198, 199, 217, 249, 251, 257, 259, 260, 261, 264, 265, 266, 277, 278, 279, 283, 284, 286, 290, 292, 293, 294, 295, 297, 298, 301, 302, 305, 306, 308, 310, 312, 313, 317, 318, 319, 320, 321, 322, 324, 325, 330, 331, 334, 335, 336, 337, 338, 339, 340, 341], "suppress": 57, "sure": [251, 257, 311, 340], "swish": 257, "sym_shape_infer_temp": 341, "symbol": 290, "symint": [218, 220], "symmetr": [3, 9, 54, 55, 61, 183, 198, 199, 241, 249, 251, 254, 257, 258, 261, 263, 267, 268, 270, 274, 276, 285, 286, 291, 299, 300, 303, 318, 319, 331, 332, 333, 335, 336, 337, 339, 340, 341], "system": [118, 249, 258, 303, 307, 341], "systemat": 298, "t": [39, 140, 159, 183, 186, 241, 244, 315, 319, 336], "t_expon": 244, "tabl": [202, 257, 290, 295, 318, 319], "tag": 319, "tail": 336, "tailor": [262, 294, 297, 332, 336], "take": [3, 213, 250, 252, 257, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 285, 286, 298, 319, 322, 336, 338], "taken": [290, 336], "tanh": 300, "tar": [265, 266, 267, 268, 270, 274, 276], "target": [25, 53, 55, 90, 150, 183, 193, 198, 199, 202, 243, 249, 251, 257, 283, 285, 286, 291, 297, 298, 300, 322], "target_devic": 199, "target_opset": 90, "target_opset_vers": 90, "target_vers": 55, "targetindic": 257, "targetoptyp": [257, 297], "targetquanttyp": [257, 297], "targettensor": 257, "task": [249, 271, 272, 273, 275, 278, 279, 280, 281, 282, 298, 309, 310, 312, 313, 341], "teardown": 32, "tech": [315, 341], "technic": 182, "techniqu": [53, 251, 252, 253, 254, 255, 256, 266, 285, 286, 291, 294, 295, 297, 302, 317, 335], "technologi": [251, 252, 293, 295, 296, 297, 302, 303, 306, 330, 334, 335, 336], "tee": 316, "tell": 1, "tempfil": 150, "templat": [278, 279], "temporari": 341, "ten": 330, "tensor": [3, 7, 8, 22, 26, 33, 39, 50, 54, 55, 57, 78, 80, 81, 82, 83, 84, 85, 86, 99, 121, 122, 127, 128, 131, 135, 136, 140, 141, 144, 150, 151, 169, 178, 180, 182, 183, 193, 198, 199, 202, 205, 218, 219, 220, 221, 228, 238, 241, 243, 244, 249, 251, 253, 257, 258, 260, 261, 264, 267, 268, 290, 293, 294, 295, 296, 297, 302, 303, 305, 306, 318, 319, 322, 323, 331, 335, 336, 337, 338, 339, 341], "tensor_arrai": 114, "tensor_dtyp": 32, "tensor_float": 99, "tensor_float16": 99, "tensor_hist": 257, "tensor_quant": [178, 205], "tensor_shap": 32, "tensor_stat": 205, "tensor_sync": 7, "tensor_to_its_receiving_nod": 54, "tensorproto": [13, 32, 33, 39, 55, 99, 114], "tensors_rang": [4, 39, 54, 63], "tensors_to_quant": 54, "term": [193, 198, 251, 252, 293, 295, 296, 297, 301, 302, 303, 306, 330, 334, 335, 336], "termin": [55, 298, 303, 330], "test": [32, 119, 257, 259, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 290, 298, 318, 319, 341], "test_bf16": 316, "test_cas": 119, "test_imag": [283, 285, 286, 291], "test_prompt": 308, "test_siz": 308, "test_w_uint4_asym_finetun": 316, "testcas": 32, "text": [118, 198, 271, 272, 273, 275, 280, 281, 282, 301, 302, 303, 306, 319, 332], "text_encod": [308, 341], "text_encoder_2": [308, 341], "text_to_imag": 308, "textual": 118, "tf": 31, "than": [9, 22, 32, 55, 71, 76, 99, 140, 238, 241, 250, 251, 252, 257, 259, 260, 263, 279, 283, 290, 294, 297, 301, 302, 305, 319, 323, 333, 335, 336, 338], "thank": 319, "thankfulli": 319, "thei": [5, 9, 31, 55, 118, 159, 162, 186, 193, 198, 199, 202, 217, 238, 243, 251, 257, 302, 304, 319, 338, 339, 340], "them": [5, 31, 55, 140, 162, 178, 213, 217, 251, 259, 283, 285, 286, 290, 291, 292, 294, 301, 302, 307, 315, 319, 323, 335, 340], "themselv": 257, "theoret": 318, "theori": 140, "therebi": [251, 254, 277, 291], "therefor": [290, 292, 297, 309], "thi": [3, 9, 26, 31, 32, 33, 39, 50, 53, 55, 56, 61, 72, 99, 138, 140, 150, 154, 182, 186, 193, 194, 198, 199, 205, 213, 215, 217, 228, 243, 250, 252, 253, 254, 255, 257, 258, 259, 260, 262, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 307, 308, 309, 311, 314, 315, 316, 317, 318, 319, 322, 323, 324, 325, 330, 332, 334, 335, 336, 337, 338, 339, 340], "think": [251, 319], "thorough": 340, "thoroughli": 301, "those": [33, 50, 294], "though": 336, "thousand": 269, "thread": [31, 118], "threadnam": 118, "three": [7, 257, 262, 266, 290, 292, 298, 304, 317, 319, 332, 337], "three_level_spac": 1, "threshold": [202, 249, 257, 278, 279, 290, 298, 318, 341], "through": [150, 181, 205, 213, 244, 249, 251, 257, 262, 264, 295, 296, 297, 302, 307, 311, 315, 317, 332, 335, 336], "throughput": 202, "thu": [257, 302, 306, 338], "thudm": [314, 315, 316], "tied_paramet": 180, "tile": 301, "time": [3, 55, 118, 119, 250, 251, 252, 257, 262, 269, 278, 279, 294, 297, 298, 299, 302, 307, 318, 319, 330, 332, 336], "time_limit": 298, "timm": [265, 267, 268, 270, 276], "tinygsm8k": 311, "tisquanttyp": 257, "titl": 205, "to_bfp": [257, 295, 300], "to_bfp_prim": [257, 295], "to_real_quantize_param": 178, "todo": 1, "togeth": [301, 338], "token": [150, 154, 198, 277, 281, 282, 301, 303, 309, 313, 315, 316, 319, 322, 332, 335, 337, 339], "tokenization_auto": 315, "tokenized_output": [198, 301, 303, 319, 332], "tokenizer_config": [150, 154, 272, 273, 275, 280, 322], "tokenizer_dir": [154, 156], "tokenizer_path": [150, 321], "toler": [55, 269, 278, 279, 283, 297, 298], "too": [199, 252], "tool": [249, 250, 251, 252, 257, 265, 281, 282, 290, 306, 308, 320, 323, 335, 341], "tool_vers": 55, "toolkit": [249, 319, 341], "top": [244, 297], "top1": [257, 266, 297], "top1acctarget": 257, "top5": 266, "topic": [264, 283, 285, 286, 291, 308, 314, 317, 335, 340], "torch": [7, 8, 9, 13, 14, 15, 16, 17, 22, 25, 26, 27, 76, 250, 264, 265, 267, 268, 270, 271, 272, 273, 275, 276, 277, 278, 280, 281, 282, 288, 290, 301, 302, 303, 306, 307, 308, 309, 314, 315, 316, 317, 318, 319, 320, 321, 322, 324, 325, 329, 330, 332, 334, 335, 337, 338, 339, 341], "torch_compil": 198, "torch_extens": 330, "torch_model": 22, "torchmodel": 8, "torchscript": 290, "torchvis": [308, 318], "total": 302, "touch": 140, "tow": 98, "toward": 202, "tqt": [202, 242, 249, 341], "tqtobserv": 242, "tqtspec": 199, "tqtthresholdinitmeth": 202, "trace": [22, 212], "trace_model": 181, "track": 205, "trade": [249, 278, 279], "tradit": [252, 297], "tradition": 319, "trail": 337, "train": [22, 26, 27, 181, 193, 198, 199, 219, 226, 228, 235, 249, 251, 252, 257, 262, 265, 278, 279, 283, 284, 285, 286, 291, 293, 294, 295, 296, 316, 318, 319, 327, 332, 336, 339, 341], "train_batch_s": 318, "train_load": 318, "train_model_param": [22, 25, 26], "train_torch": 22, "train_torch_module_api": 22, "trainloss": 26, "trainparamet": [22, 25, 26, 27], "trans_opsfunc_2_quant_modul": 211, "transa": 15, "transb": 15, "transfer": [215, 217], "transform": [5, 30, 31, 32, 34, 50, 51, 53, 54, 55, 57, 71, 74, 140, 150, 181, 198, 199, 205, 206, 251, 254, 257, 258, 260, 273, 275, 277, 280, 290, 299, 301, 303, 305, 315, 316, 318, 319, 332, 335, 336, 337, 338], "transform_for_annot": 226, "transforms_pipelin": 30, "transformspipelin": 34, "transpos": [61, 71, 178, 257, 300, 341], "trasnform": 55, "travers": [107, 108, 251], "treat": [244, 257], "treatment": 180, "tree": [33, 265, 267, 268, 269, 270, 274, 276, 279], "trick": 140, "trigger": [278, 279], "true": [3, 5, 27, 39, 53, 55, 57, 61, 71, 72, 74, 76, 82, 85, 99, 118, 144, 150, 181, 183, 198, 199, 234, 235, 252, 253, 254, 255, 257, 259, 260, 264, 278, 279, 283, 284, 285, 286, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 305, 315, 318, 324, 332, 337, 340], "true_sequenti": [199, 340], "truli": 199, "trust_remote_cod": [312, 313, 315], "try": [257, 318, 339, 341], "tsv": 308, "tune": [57, 249, 257, 260, 283, 285, 286, 291, 298, 301, 305, 316, 335, 336, 341], "tupl": [9, 13, 27, 31, 39, 51, 55, 57, 61, 74, 107, 141, 150, 178, 180, 182, 205, 241, 257], "turbo": [308, 341], "turn": [252, 257, 290], "tutori": [293, 295, 296, 297, 301, 319, 337], "tw": 336, "two": [3, 98, 107, 183, 199, 202, 228, 241, 249, 251, 252, 257, 260, 264, 266, 278, 279, 283, 285, 286, 291, 293, 295, 297, 298, 300, 309, 318, 319, 325, 336, 339, 341], "txt": [265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 283, 285, 286, 290, 291, 309, 311, 316], "type": [3, 5, 8, 9, 13, 22, 32, 39, 53, 54, 55, 57, 61, 80, 81, 82, 83, 84, 85, 99, 107, 111, 150, 159, 183, 193, 198, 199, 217, 243, 249, 252, 257, 258, 260, 262, 266, 271, 277, 290, 295, 296, 297, 298, 301, 303, 304, 309, 310, 312, 313, 315, 317, 318, 319, 323, 332, 335, 336, 337, 340, 341], "typedef": 319, "typic": [118, 199, 249, 251, 252, 260, 261, 264, 278, 279, 285, 286, 291, 297, 302, 304, 305, 306, 318, 331, 335, 336, 339], "typo": [299, 340], "u": [301, 306, 338], "u16s8": 96, "u16s8_aaw": 299, "u16s8_aaws_adaqu": 299, "u16s8_aaws_adaround": 299, "u16u8": 97, "u4w": 316, "u8s8": 94, "u8s8_aaw": 299, "u8s8_aaws_adaqu": 299, "u8s8_aaws_adaround": 299, "u8u8": 97, "u8u8_aawa": 341, "uint16": [55, 249, 257, 258, 292, 297, 299, 300, 341], "uint32": [249, 258, 297, 300, 341], "uint4": [150, 198, 202, 249, 257, 300, 303, 310, 312, 313, 315, 319, 324, 332, 340, 341], "uint4_int4_flag": [150, 324], "uint4_per_group_asym_spec": 340, "uint4perchannelspec": 199, "uint4pergroupspec": 199, "uint4pertensorspec": 199, "uint8": [55, 150, 202, 249, 257, 258, 271, 272, 292, 297, 299, 300, 315, 319, 324, 340, 341], "uint8_dynamic_qu": [271, 272, 299], "uint8_t": 319, "uint8perchannelspec": 199, "uint8pergroupspec": 199, "uint8pertensorspec": 199, "ultim": 140, "ultra": 202, "unchang": [99, 252, 338], "uncom": 332, "under": [215, 265, 266, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 307, 315, 318, 319, 321, 341], "undergo": 251, "understand": [301, 311, 340], "undesir": 341, "unet": 341, "unfortun": 290, "unifi": [228, 252], "uniform": [240, 241, 242, 252, 297, 318], "uniformli": 202, "uniformscalingobserv": 241, "uninstal": [278, 279], "union": [56, 57, 150, 154, 193, 198, 319], "uniqu": [95, 199], "unit": [54, 341], "unknown": [33, 55, 290], "unless": [57, 199, 251, 252, 293, 295, 296, 297, 302, 303, 306, 330, 334, 335, 336], "unlik": [252, 293, 318], "unnecessari": [82, 85], "unquant": 251, "unset": 257, "unsign": [202, 257, 335], "unsqueez": 300, "unsupport": [250, 257], "until": [1, 297, 298, 301, 322, 325], "unutbu": 138, "unzip": [250, 288, 318, 329], "up": [26, 27, 32, 39, 252, 257, 258, 277, 285, 286, 291, 304, 338], "up_proj": 340, "updat": [31, 39, 243, 252, 257, 290, 316, 336, 341], "update_bia": 27, "update_buff": 243, "updatebia": 257, "upgrdt": 105, "upon": [271, 277, 281, 282, 314, 315], "upper_op": 107, "upsample_r": 241, "upscal": 341, "upward": [107, 108, 257], "url": [269, 279], "us": [1, 3, 7, 9, 20, 26, 31, 32, 33, 39, 50, 53, 54, 55, 56, 57, 61, 76, 80, 81, 82, 83, 84, 85, 90, 92, 95, 98, 99, 104, 110, 111, 118, 121, 122, 127, 128, 131, 135, 136, 140, 143, 150, 151, 154, 156, 159, 178, 181, 182, 183, 193, 194, 198, 199, 202, 205, 206, 209, 217, 222, 230, 235, 241, 244, 249, 250, 251, 254, 255, 257, 258, 260, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 282, 283, 284, 285, 286, 288, 289, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 302, 303, 304, 305, 306, 307, 309, 310, 311, 313, 314, 317, 322, 324, 325, 327, 329, 332, 333, 334, 335, 338, 339, 340, 341], "usabl": [322, 325], "usag": [182, 251, 290, 293, 294, 295, 296, 297, 319, 341], "use_compiler_version_cpu_kernel": 257, "use_dynamic_qu": [57, 257, 299], "use_external_data_format": [3, 55, 57, 61, 257, 289, 290, 299], "use_fast": 315, "use_gptq": 273, "use_moving_averag": 277, "use_pof2": 55, "use_sc": 55, "use_temp_fil": 156, "use_unsigned_relu": 39, "used_scale_zp_map": 39, "usefp32scal": 257, "usegptq": 257, "usematmulnbit": 257, "useqdqvitiscustomop": 257, "user": [1, 55, 56, 118, 186, 193, 198, 226, 228, 251, 257, 259, 264, 287, 288, 297, 307, 308, 314, 315, 317, 319, 328, 332, 337, 339, 340, 341], "userandomdata": [257, 259], "userandomhad": [254, 257], "usestim": 118, "useunsignedrelu": 257, "usual": [3, 118, 257, 278, 279], "util": [27, 76, 99, 121, 122, 127, 128, 131, 135, 136, 182, 186, 193, 198, 205, 250, 251, 257, 259, 264, 265, 266, 267, 268, 269, 270, 276, 284, 287, 293, 298, 301, 303, 304, 307, 309, 312, 319, 332, 336, 337], "v": [199, 257, 315, 335, 338], "v0": [286, 314, 315, 341], "v01": [315, 341], "v1": [266, 283, 285, 288, 291, 308, 316, 319, 341], "v2": 318, "v_proj": [150, 199, 315, 336, 338, 340], "vae": [308, 341], "vai_lib_path": [284, 294, 300], "vaiml": 257, "val": [265, 267, 268, 269, 270, 274, 276, 278], "val_data": [265, 266, 267, 268, 269, 270, 274, 276], "val_imag": [265, 266, 267, 268, 270, 274, 276], "val_load": 318, "valid": [1, 39, 118, 186, 238, 257, 265, 266, 267, 268, 269, 270, 274, 276, 318, 319, 341], "valu": [1, 3, 5, 9, 11, 31, 39, 54, 55, 61, 78, 86, 99, 110, 118, 147, 150, 162, 183, 199, 202, 205, 238, 241, 243, 252, 253, 254, 255, 257, 258, 260, 261, 263, 278, 279, 284, 290, 294, 295, 296, 297, 301, 302, 303, 305, 306, 315, 318, 319, 323, 331, 333, 335, 336, 338, 341], "valueerror": [3, 39, 56, 241, 289], "values_idx": 1, "vari": [262, 297, 332], "variabl": [250, 300, 306, 315], "variat": [265, 266, 267, 268], "varieti": [249, 251, 317, 319], "variou": [33, 53, 61, 213, 248, 249, 251, 252, 253, 254, 255, 257, 262, 294, 297, 298, 307, 332, 335, 339], "vector": 338, "verbos": [150, 290], "veri": [150, 257, 290, 302, 306, 336, 339], "verif": 184, "verifi": [249, 250, 336], "verifici": 186, "versatil": [252, 319], "version": [39, 55, 90, 150, 193, 198, 228, 234, 237, 241, 251, 257, 258, 290, 300, 308, 315, 316], "vi": 257, "via": [178, 284, 309, 317, 323, 341], "vision": [266, 274, 283, 291, 309, 310, 315, 329, 341], "visit": 323, "visual": [205, 250, 300], "vit": 257, "viti": [39, 54, 257], "vitisbfpquant": 54, "vitisdequantizelinear": [257, 300, 341], "vitisextendedquant": 54, "vitisinstancenorm": 341, "vitislstm": 341, "vitisonnxquant": [39, 54], "vitisqdq": [101, 292, 297], "vitisqdqcpuquant": [4, 63], "vitisqdqnpucnnquant": 54, "vitisqdqquant": [4, 54, 63], "vitisquantformat": [55, 57, 257, 258, 293, 294, 295, 296, 297, 300], "vitisquantizelinear": [257, 300, 341], "vitisquanttyp": [3, 55, 57, 257, 293, 294, 295, 296, 297, 300], "vllm": [150, 320, 322, 325, 341], "vlm": [310, 312, 313], "vmaf": [98, 292, 341], "vnni": 61, "vocab": [159, 272, 273, 275, 280], "w": [217, 302, 308, 336, 341], "w3": 335, "w4": 335, "w8": 341, "w8a8": [338, 339, 341], "w_alpha": 13, "w_bfp16": 315, "w_bfp16_a_bfp16": 315, "w_fp8_a_fp8": [315, 335], "w_fp8_a_fp8_per_tensor": 340, "w_fp8_a_fp8_per_tensor_config": 340, "w_int4_per_channel_sym": 324, "w_int4_per_group_sym": [315, 324], "w_int8_a_int8_per_tensor": [338, 340], "w_int8_a_int8_per_tensor_config": 340, "w_int8_a_int8_per_tensor_sym": [315, 335, 338], "w_int8_per_channel_a_int8_per_tensor": 339, "w_int8_per_channel_a_int8_per_tensor_kv_cache_int8_per_tensor": 339, "w_int8_per_channel_a_int8_per_token": 339, "w_int8_per_channel_a_int8_per_token_kv_cache_int8_per_token": 339, "w_int8_per_tensor": 338, "w_int8_per_tensor_a_int8_per_tensor": 339, "w_int8_per_tensor_a_int8_per_tensor_kv_cache_int8_per_tensor": 339, "w_mx6": 315, "w_mx6_a_mx6": 315, "w_mx_fp8": 315, "w_mx_fp8_a_mx_fp8": 315, "w_uint4_a_bfloat16_per_group_asym": 324, "w_uint4_asym": 316, "w_uint4_per_group": 340, "w_uint4_per_group_asym": [315, 323, 324, 335], "w_uint4_per_group_config": 340, "wa": [118, 205, 257, 265, 267, 268, 270, 273, 274, 275, 276, 277, 280, 300], "wai": [121, 122, 127, 128, 131, 135, 136, 140, 143, 156, 178, 182, 194, 199, 206, 209, 222, 241, 278, 279, 283, 300, 302], "walk": 308, "want": [33, 39, 183, 198, 255, 257, 265, 266, 294, 295, 296, 303, 309, 315, 319, 338, 339, 340], "warm_start": 27, "warn": [57, 118, 186, 194, 199, 257, 290, 341], "we": [1, 9, 33, 39, 55, 140, 150, 180, 183, 199, 212, 213, 215, 217, 226, 235, 242, 250, 255, 257, 264, 266, 269, 277, 278, 279, 283, 290, 292, 293, 294, 295, 297, 299, 300, 304, 306, 307, 309, 310, 311, 315, 317, 318, 319, 325, 335, 336, 337, 338, 339, 340], "websit": [265, 266, 267, 268, 270, 274, 276], "weight": [7, 8, 9, 13, 14, 15, 16, 17, 22, 25, 26, 33, 39, 54, 55, 57, 61, 66, 71, 76, 110, 112, 140, 144, 150, 151, 169, 178, 180, 181, 182, 183, 198, 199, 202, 205, 218, 219, 220, 221, 228, 235, 249, 251, 252, 253, 255, 257, 258, 262, 278, 279, 282, 284, 285, 286, 290, 291, 293, 296, 297, 299, 300, 302, 303, 306, 308, 316, 318, 319, 321, 322, 324, 325, 332, 334, 335, 337, 338, 339, 340, 341], "weight_fli": 150, "weight_format": [150, 151, 322, 325], "weight_group": 150, "weight_merge_group": 151, "weight_nam": 39, "weight_qtyp": [4, 39, 54, 63], "weight_scal": 39, "weight_spec": 182, "weight_stat": 306, "weight_stats_hook": 205, "weight_tensor_nam": 297, "weight_tensor_name1": 297, "weight_tensor_name2": 297, "weight_typ": [36, 55, 57, 61, 252, 253, 254, 255, 257, 278, 279, 283, 284, 293, 294, 295, 296, 297, 298, 299, 300], "weight_zero_point": 322, "weightonlyquantconfig": 66, "weights_onli": 54, "weights_only_quant": [275, 281], "weightscal": [257, 293], "weightsonli": 257, "weightsymmetr": [61, 257, 278, 279, 283, 298], "weighttargetquanttyp": [257, 297], "well": [180, 205, 251, 257, 262, 292, 294, 300, 307, 315, 332, 336], "were": 252, "wether": 283, "wget": [266, 272, 273, 274, 275, 280, 283, 285, 286, 291], "what": [33, 50, 228, 301], "wheel": 250, "when": [3, 32, 55, 61, 118, 150, 180, 183, 199, 250, 252, 253, 254, 257, 259, 262, 269, 278, 279, 283, 284, 290, 292, 293, 294, 295, 296, 297, 298, 300, 302, 303, 306, 308, 314, 315, 316, 319, 330, 332, 334, 335, 336, 338, 341], "where": [55, 56, 107, 118, 156, 186, 199, 212, 251, 257, 258, 260, 278, 279, 290, 294, 295, 296, 297, 300, 302, 305, 313, 336, 338, 341], "whether": [3, 32, 33, 39, 53, 54, 55, 150, 151, 181, 183, 199, 228, 252, 253, 254, 255, 257, 290, 298, 301, 302, 306, 318, 337, 340], "which": [1, 3, 7, 20, 22, 25, 32, 33, 34, 39, 50, 71, 76, 118, 150, 154, 198, 199, 213, 226, 228, 241, 244, 250, 251, 252, 257, 262, 265, 266, 267, 268, 270, 273, 274, 275, 276, 277, 278, 279, 280, 283, 290, 293, 294, 296, 297, 298, 299, 300, 301, 302, 304, 306, 309, 315, 318, 319, 325, 332, 334, 335, 336, 338, 341], "while": [243, 251, 252, 257, 262, 266, 289, 293, 294, 295, 296, 297, 298, 300, 301, 302, 307, 319, 325, 332, 335], "whl": [250, 288, 329], "who": 257, "whole": [183, 269, 302, 319], "whose": [32, 119, 252, 257, 290, 297], "why": [9, 301, 302], "wide": [249, 285, 286, 291, 297, 306, 319, 335, 337], "wider": 257, "width": [183, 251, 257, 278, 279, 297, 298, 302, 307, 335], "wiki": 319, "wiki2": 314, "wikitext": [304, 316, 319, 338], "wikitext2": [271, 272, 273, 275, 277, 280, 281, 282, 312, 319], "wikitext_for_gptq_benchmark": 335, "wildcard": 151, "window": [249, 250, 258, 293, 294, 295, 296, 300, 303, 330, 341], "wise": [27, 252, 285, 286, 291, 297, 298, 340, 341], "wish": [257, 290], "with_cast": 292, "with_mixed_precis": 276, "within": [55, 119, 183, 199, 202, 205, 241, 252, 254, 257, 266, 269, 277, 278, 279, 283, 294, 295, 296, 297, 298, 302, 307, 315, 341], "without": [22, 95, 99, 140, 249, 251, 252, 257, 265, 269, 292, 293, 295, 296, 297, 301, 302, 303, 306, 307, 318, 319, 330, 334, 335, 336, 338, 341], "won": 186, "word": [150, 322, 324], "work": [7, 82, 85, 99, 251, 257, 290, 297, 301, 323], "worker": 308, "workflow": [284, 318, 341], "world": [298, 309], "wors": [259, 336], "would": [140, 311, 322, 336], "wrap": 241, "wrapper": [13, 14, 15, 16, 17, 22, 25, 26, 34, 138, 140], "wrapper_fn": 138, "write": [205, 257, 319], "written": 154, "x": [55, 140, 141, 159, 215, 217, 241, 309, 310, 312, 313, 319, 336, 338], "x_orig": 241, "x_q": 302, "x_r": 302, "xilinx": 182, "xint8": [258, 259, 274, 285, 286, 291, 299], "xint8_adaqu": 299, "xint8_adaround": 299, "xint8_config": 258, "xl": [308, 341], "xsum": 313, "xw": 336, "xzf": [265, 266, 267, 268, 270, 274, 276], "y": 336, "yaml": 228, "ye": 290, "yet": 315, "yield": [118, 252, 259, 335], "yolo": [278, 286, 318, 341], "yolo_na": 278, "yolo_nas_inf": 278, "yolov": 279, "yolov3": [269, 279, 288], "yolov3_float_onnx_path": 269, "yolov7": 341, "yolov8": [286, 341], "yolov8n": 286, "you": [32, 33, 198, 250, 251, 252, 257, 261, 262, 265, 266, 267, 268, 269, 270, 274, 276, 278, 279, 283, 284, 285, 286, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 309, 313, 315, 316, 318, 319, 325, 329, 330, 332, 334, 335, 336], "your": [249, 250, 251, 252, 257, 258, 259, 264, 265, 266, 267, 268, 270, 274, 276, 278, 279, 283, 285, 286, 291, 292, 300, 304, 308, 309, 315, 317, 335, 341], "your_model": 315, "z": [55, 251, 319], "zero": [39, 55, 182, 183, 199, 202, 234, 235, 244, 251, 252, 257, 263, 278, 279, 289, 293, 301, 318, 322, 325, 333], "zero_point": [7, 55, 150, 319, 334], "zero_point_shap": 178, "zeropint": 178, "zeropoint": 150, "zeropointtyp": 202, "zhang": 181, "zip": [288, 318, 319, 329], "zp": 7, "\u2463": 315, "\u2464": 315, "\u2465": 315}, "titles": ["<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.auto_search</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.bias_correction</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.calibrate</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.cpu_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.equalization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.base_fn_quantizers</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.base_qdq_quantizers</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model_test</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_base_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_conv_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_gemm_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_matmul_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_norm_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.fast_finetune</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.onnx_evaluate</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.onnx_subgraph</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.torch_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.torch_utils_test</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch.train_model</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch.train_model_loss</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch.train_model_param</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.gptq.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.model_transformer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.model_transformer_test</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.transforms</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.transforms_pipeline</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.mprecision.auto_mixprecision</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.mprecision</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.mprecision.mixed_bfp</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.onnx_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.custom_ops.build_vai_custom_op</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.custom_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.concat</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.hardsigmoid</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.layernorm</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.prelu</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.qdq_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.softmax</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimizations.convert_transforms</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimizations.convert_transforms_pipeline</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimizations</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.qdq_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quant_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.config.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.config.custom_config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.bfp_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.cpu_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.extended_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.matmul_nbits_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.npu_cnn_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.npu_transformer_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.onnx_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.qdq_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quarot</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.refine</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.registry</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.simulate_dpu</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.simulate_dpu_softmax</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.smooth_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_customqdq_to_qdq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_dynamic_to_fixed</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp16_to_bf16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp16_to_bfp16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp16_to_fp32</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp32_to_bf16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp32_to_bfp16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp32_to_fp16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_lstm_to_customlstm</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_nchw_to_nhwc</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_onnx_to_onnxtxt</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_onnxtxt_to_onnx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_opset_version</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_qdq_to_qop</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_quant_to_float</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_resize_fs_to_pof2s</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_s8s8_to_u8s8</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_shared_initializer_to_unique</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_u16s8_to_s16s8</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_u16u8_to_u8u8</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.evaluate</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.float16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.insert_clip_bfloat16_qdq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.print_a16w8_a8w8_nodes</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.random_quantize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_bf16_cast</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_initializer_from_input</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_qdq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_qdq_between_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_qdq_mul_add</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.replace_bfloat16_qdq_cast</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.replace_inf_weights</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.save_tensor_hist</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.save_weights_hist</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.utils.model_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils.import_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils.log</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils.testing_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.auto_smooth</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.awq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.modules.act</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.modules</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.scale</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.smooth</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.blockwise_tuning.blockwise_tuning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.blockwise_tuning.blockwise_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.blockwise_tuning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.gptq.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.osscar</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.osscar.osscar</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.processor</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot.monkeypatch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot.quarot</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation.hadamard</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation.rotation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation.rotation_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.auto_config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.module</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.prepare</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.config.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.constants</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.gguf_model_converter</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.gguf_model_writer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.tensor_convert</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder.llm_info</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder.llm_info_builder</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder.native_model_info_builder</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.converter</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.converter.llm_info_converter</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.utils.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_export.model_post_process</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_export.quant_config_parser</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_import</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_import.pretrained_config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn.modules</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn.modules.qparamslinear</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn.modules.realquantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.onnx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.algos</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.mapping</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.verification</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel.hw_emulation.extensions</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel.hw_emulation.hw_emulation_interface</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel.hw_emulation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.model_transformation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.config_verification</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.type</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.constants</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.debug</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.fx.base</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.fx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.activate_dropout</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.model_optimization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.modify_reshape_param</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.opt_pass_manager</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.post_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.remove_dropout_node</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor.insert_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor.processor</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor.processor_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.torch_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.model_transformation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.mixin</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_conv</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_conv_bn_fused</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_embed</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_linear</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer.lsq_observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer.observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer.tqt_observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.tensor_quantize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.utils.pack</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.version</span></code>", "Basic Usage", "Welcome to AMD Quark Documentation!", "Installation Guide", "Quantization with AMD Quark", "Quantization Using AdaQuant and AdaRound", "Quantizing Using CrossLayerEqualization (CLE)", "QuaRot", "SmoothQuant (SQ)", "Accuracy Improvement Algorithms", "Full List of Quantization Configuration Features", "AMD Quark for ONNX", "Adding Calibration Datasets", "Calibration Methods", "Quantization Schemes", "Quantization Strategies", "Quantization Symmetry", "Using ONNX Model Inference and Saving Input Data in NPY Format", "Block Floating Point (BFP) Example", "Microscaling (MX) Example", "Quark ONNX Quantization Example", "Quark ONNX Quantization Example", "Quark ONNX Quantization Example", "Quark ONNX Example for CrossLayerEqualization (CLE)", "Dynamic Quantization for Llama-2-7b", "Dynamic Quantization for OPT-125M", "Quantizating a model with GPTQ", "Quantizing a ResNet50-v1-12 Model", "Quantizing an OPT-125M Model", "Quantization using Mixed Precision", "Quark ONNX Quantization Example", "Auto Search for RyzenAI ONNX Model Quantization \u2013 yolo_nas_s", "Auto Search for Best Practice of RyzenAI ONNX Model Quantization", "Quantization using SmoothQuant", "Quantizating Llama-2-7b model using MatMulNBits quantizer", "Quark ONNX Quantization Example", "Best Practice for Ryzen AI in Quark ONNX", "Accelerate with GPUs", "Best Practice for Ryzen AI in Quark ONNX", "Best Practice for Ryzen AI in Quark ONNX", "AMD Quark APIs for ONNX", "Accessing ONNX Examples", "Frequently Asked Questions (FAQ)", "Optional Utilities", "Best Practice for Ryzen AI in AMD Quark ONNX", "Tools", "Introduction", "BFP16 (Block floating point) Quantization", "Introduction", "Microscaling (MX)", "Mixed Precision", "Automatic Search for Model Quantization", "Configuring ONNX Quantization", "Supported Data and Op Types", "Using MX (Microscaling)", "Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents)", "AMD Quark for PyTorch", "Adding Calibration Datasets", "Calibration Methods", "Debugging quantization degradation in AMD Quark", "Brevitas Integration", "Diffusion Model Quantization using Quark", "Language Model Evaluations in Quark", "LM-Evaluation Harness Evaluations", "LM-Evaluation Harness (Offline)", "Perplexity Evaluations", "Rouge &amp; Meteor Evaluations", "Pruning", "Language Model Post Training Quantization (PTQ) Using Quark", "Language Model QAT Using Quark", "Integration with AMD Pytorch-light (APL)", "Vision Model Quantization Using Quark FX Graph Mode", "Bridge from Quark to llama.cpp", "Exporting Quantized Models", "GGUF Exporting", "HuggingFace Format", "Exporting Using ONNX Runtime Gen AI Model Builder", "ONNX Exporting", "Quark Format", "Extensions for PyTorch", "Language Model Optimization", "Quark APIs for PyTorch", "Accessing PyTorch Examples", "Frequently Asked Questions (FAQ)", "Quantization Schemes", "Quantization Strategies", "Quantization Symmetry", "Save &amp; Load Quantized Models", "Best Practices for Post-Training Quantization (PTQ)", "Activation/weight smoothing (SmoothQuant)", "BFP16 (Block floating point) Quantization", "Rotation-based quantization with QuaRot", "Quantizing with Rotation and SmoothQuant", "Configuring PyTorch Quantization", "Release Notes"], "titleterms": {"0": 341, "1": [300, 301, 309, 311, 314, 315, 316, 317, 318, 319, 337, 340, 341], "12": 274, "125m": [272, 275], "2": [271, 281, 300, 301, 309, 311, 314, 315, 316, 317, 318, 319, 337, 340, 341], "3": [300, 301, 309, 311, 315, 316, 317, 319, 337, 340], "4": [300, 301, 309, 311, 315, 337, 340], "5": [301, 309, 311, 315, 341], "6": [315, 341], "7": [315, 341], "7b": [271, 281], "8": [315, 341], "For": [259, 264], "It": 319, "Not": 315, "Of": 311, "Their": 251, "These": 302, "With": [270, 273, 276, 280], "a16w8": 292, "a8w8": 292, "a_float16": 315, "acceler": 284, "access": [288, 329], "accuraci": [256, 288, 292, 293, 294, 295, 296], "act": 124, "activ": 336, "activate_dropout": 209, "actual": 251, "ad": [259, 304], "adaqu": [252, 265, 267], "adaround": [252, 268], "ai": [283, 285, 286, 288, 291, 323], "algo": 181, "algoconfig": 340, "algorithm": [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 256, 335], "amd": [249, 251, 252, 258, 259, 287, 288, 289, 291, 293, 294, 295, 296, 297, 301, 303, 306, 317, 330], "an": [275, 311], "analysi": 297, "api": [56, 120, 150, 154, 182, 193, 198, 287, 328], "apl": 317, "appli": [301, 335], "ar": [251, 301, 302], "argument": [252, 253, 254, 255, 310, 312, 313], "ask": [289, 330], "asymmetr": 315, "attribut": [41, 228], "auto": [278, 279, 283], "auto_config": 145, "auto_mixprecis": 36, "auto_search": [1, 269], "auto_smooth": 121, "automat": [297, 298, 315], "awq": [121, 122, 123, 124, 125, 126, 127, 315, 323], "base": [206, 297, 338], "base_fn_quant": 6, "base_qdq_quant": 7, "baselin": 311, "basic": [248, 258, 303], "benchmark": 308, "benefit": [252, 294, 297], "best": [279, 283, 285, 286, 291, 335], "between": 292, "bf16": 293, "bfloat16": [292, 300], "bfp": 265, "bfp16": [265, 284, 292, 294, 300, 315, 337], "bfp_quantiz": 62, "bias_correct": 2, "block": [265, 294, 301, 337], "blockwise_tun": [128, 129, 130], "blockwise_util": 129, "brevita": [181, 182, 183, 184, 185, 186, 307], "bridg": 319, "bug": 341, "build_vai_custom_op": 40, "builder": [161, 162, 163, 164, 323], "c": 330, "cach": 315, "calcul": 301, "calibr": [3, 259, 260, 283, 284, 285, 286, 291, 301, 304, 305, 308], "call": [278, 279], "class": [1, 3, 4, 5, 6, 7, 8, 13, 14, 15, 16, 17, 21, 22, 25, 26, 27, 31, 32, 33, 34, 39, 50, 51, 53, 54, 55, 56, 57, 59, 63, 66, 71, 76, 111, 118, 121, 122, 123, 127, 128, 130, 131, 132, 134, 135, 136, 139, 140, 143, 144, 150, 151, 156, 159, 160, 162, 178, 181, 182, 183, 186, 188, 193, 194, 195, 198, 199, 202, 206, 209, 213, 215, 217, 222, 229, 232, 234, 235, 236, 237, 240, 241, 242, 243, 259], "classif": [288, 318], "cle": [253, 270], "code": [259, 264, 318], "compar": 311, "compil": 330, "compon": 298, "concat": 43, "concept": 294, "conclus": [298, 301, 307], "config": [57, 58, 59, 151, 152, 183, 194, 199, 200, 201, 202, 203, 278, 279, 340], "config_verif": 200, "configur": [257, 299, 301, 315, 337, 340], "consider": 251, "constant": [153, 204], "content": [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 41, 50, 51, 53, 54, 55, 56, 57, 59, 61, 63, 66, 71, 72, 74, 76, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 111, 114, 118, 119, 120, 121, 122, 123, 127, 128, 130, 131, 132, 134, 135, 136, 138, 139, 140, 141, 143, 144, 147, 150, 151, 154, 156, 159, 160, 162, 169, 178, 180, 181, 182, 183, 186, 188, 190, 193, 194, 195, 198, 199, 202, 203, 205, 206, 209, 211, 212, 213, 215, 217, 218, 219, 220, 221, 222, 225, 226, 228, 229, 230, 232, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244], "context": 302, "controlnet": 308, "conv": 292, "convert": [165, 166, 251, 292], "convert_a8w8_npu_to_a8w8_cpu": 77, "convert_customqdq_to_qdq": 78, "convert_dynamic_to_fix": 79, "convert_fp16_to_bf16": 80, "convert_fp16_to_bfp16": 81, "convert_fp16_to_fp32": 82, "convert_fp32_to_bf16": 83, "convert_fp32_to_bfp16": 84, "convert_fp32_to_fp16": 85, "convert_lstm_to_customlstm": 86, "convert_nchw_to_nhwc": 87, "convert_onnx_to_onnxtxt": 88, "convert_onnxtxt_to_onnx": 89, "convert_opset_vers": 90, "convert_qdq_to_qop": 91, "convert_quant_to_float": 92, "convert_resize_fs_to_pof2": 93, "convert_s8s8_to_u8s8": 94, "convert_shared_initializer_to_uniqu": 95, "convert_transform": 50, "convert_transforms_pipelin": 51, "convert_u16s8_to_s16s8": 96, "convert_u16u8_to_u8u8": 97, "correspond": 319, "cpp": 319, "cpu": [292, 315], "cpu_quant": [4, 63], "create_model": 8, "create_model_op": 9, "create_model_test": 10, "create_model_util": 11, "create_torch": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "crosslayerequ": [253, 270], "custom": 299, "custom_config": 58, "custom_op": [40, 41], "data": [251, 259, 264, 265, 266, 267, 268, 269, 270, 274, 276, 283, 285, 286, 291, 300, 301, 302], "dataload": 304, "dataread": 259, "dataset": [259, 304, 308, 311], "debug": [205, 306], "default": 299, "degrad": 306, "depend": 308, "design": 307, "detail": [264, 309], "detect": 318, "diagram": 298, "dict": 304, "differ": 335, "diffus": 308, "do": [319, 337], "document": 249, "doe": [251, 319, 336], "dtype": 319, "dump": 290, "dynam": [271, 272, 288, 301], "eager": 334, "effici": 249, "enabl": [252, 294, 295, 296, 297], "enhanc": 341, "entir": 308, "entropi": 260, "environ": [284, 330], "equal": 5, "establish": 340, "eval": 311, "evalu": [98, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 280, 281, 282, 290, 292, 309, 310, 311, 312, 313, 314, 315, 316], "exampl": [252, 253, 254, 255, 258, 259, 265, 266, 267, 268, 269, 270, 277, 282, 288, 294, 295, 296, 303, 317, 318, 321, 322, 324, 325, 329, 334], "exclud": 335, "expand": 339, "experi": [318, 319], "experiment": 315, "explan": 340, "export": [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 290, 308, 309, 311, 315, 316, 319, 320, 321, 322, 323, 324, 325], "extended_quant": 64, "extens": [181, 182, 183, 184, 185, 186, 187, 189, 307, 326], "fake": 251, "faq": [289, 330], "fast": [252, 284], "fast_finetun": 18, "featur": [249, 257, 258, 303, 307, 309, 341], "file": 308, "fine": 318, "finetun": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 252, 284, 316], "fix": 341, "flexibl": 249, "float": [265, 290, 294, 337], "float16": [99, 292, 300, 314, 315], "float32": [292, 300], "flow": 298, "folder": 292, "format": [264, 266, 300, 302, 322, 325], "fp8": 315, "fp8_e4m3": 315, "frequent": [289, 330], "from": [250, 286, 311, 319], "full": 257, "function": [1, 3, 5, 9, 11, 18, 20, 22, 32, 36, 41, 53, 55, 59, 61, 72, 74, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 114, 119, 120, 138, 140, 141, 144, 147, 150, 154, 169, 180, 188, 190, 198, 199, 203, 205, 211, 212, 218, 219, 220, 221, 225, 226, 228, 229, 230, 235, 238, 244], "further": [293, 294, 295, 296], "fx": [206, 207, 318, 334], "gen": 323, "gener": 315, "get": [309, 311, 318], "gguf": [319, 321], "gguf_export": [154, 155, 156, 157, 158, 159, 315], "gguf_model_convert": 155, "gguf_model_writ": 156, "gptq": [28, 29, 131, 132, 273], "gpu": 284, "grain": 318, "graph": [206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 318, 334], "graph_transform": [30, 31, 32, 33, 34], "guid": [250, 309, 318], "guidelin": 277, "hadamard": 141, "happen": 251, "har": [309, 310, 311], "hardsigmoid": 44, "hardwar": 302, "hf": 322, "how": [252, 293, 294, 295, 296, 297, 301, 302, 319, 336, 337], "hqq": 281, "huggingfac": 322, "hw_emul": [189, 190, 191], "hw_emulation_interfac": 190, "i": [251, 295, 296, 297, 301, 319], "imag": [288, 292, 318], "import": [309, 322, 325], "import_util": 116, "improv": [256, 288, 293, 294, 295, 296], "inf": 292, "infer": [264, 284, 286], "input": [259, 264, 292], "insert_clip_bfloat16_qdq": 101, "insert_quant": 225, "instal": [250, 301, 315, 323, 337], "int": 315, "int16": 300, "int32": 300, "integ": [251, 302], "integr": [307, 317], "interest": 311, "intern": 251, "introduct": [277, 293, 295, 301, 317, 319, 337, 339], "issu": [289, 330], "json_export": [161, 162, 163, 164, 165, 166, 167, 168, 169], "json_safetensors_export": 315, "kei": [249, 294], "kernel": [189, 190, 191, 192], "kv": 315, "languag": [288, 309, 315, 316, 327], "layer": 335, "layernorm": 46, "level": 302, "licens": [269, 275, 277, 293, 294, 295, 296, 297, 298], "light": 317, "list": [257, 304, 315], "llama": [271, 281, 315, 319], "llama2": 314, "llm": 316, "llm_eval_haness_offlin": 309, "llm_eval_har": 309, "llm_info": 162, "llm_info_build": 163, "llm_info_convert": 166, "lm": [309, 310, 311], "load": [308, 334], "log": 118, "lsq_observ": 240, "main_export": [170, 171, 172], "main_import": [173, 174], "map": [185, 302], "matmul_nbits_quant": 66, "matmulnbit": 281, "mean": 251, "meteor": 313, "method": [260, 305], "microexpon": [295, 302], "microsc": [266, 296, 301], "minmax": 260, "mix": [276, 292, 297, 300], "mixed_bfp": 38, "mixin": 233, "mode": [318, 334], "model": [249, 251, 259, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 288, 289, 290, 291, 292, 294, 295, 296, 298, 300, 301, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 319, 320, 323, 327, 334, 337, 339, 340], "model_optim": 211, "model_post_process": 171, "model_transform": [31, 196, 230], "model_transformer_test": 32, "model_util": 114, "modelquant": 307, "modify_reshape_param": 212, "modul": [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 50, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 72, 74, 76, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 111, 114, 118, 119, 120, 121, 122, 124, 125, 127, 128, 131, 135, 136, 138, 139, 140, 141, 143, 144, 147, 150, 151, 154, 156, 159, 162, 169, 176, 177, 178, 180, 181, 182, 183, 186, 190, 193, 194, 198, 199, 202, 203, 205, 206, 209, 211, 212, 213, 215, 217, 218, 219, 220, 221, 222, 225, 226, 228, 230, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 340], "monkeypatch": 138, "more": [299, 339], "mprecis": [36, 37, 38], "mse": 260, "multi": [259, 264], "mx": [266, 296, 301, 315], "mx4": 302, "mx6": [302, 315], "mx9": [295, 302], "name": 292, "native_model_info_build": 164, "nchw": 292, "new": 341, "nhwc": 292, "nn": [175, 176, 177, 178, 231, 232, 233, 234, 235, 236, 237, 238, 340], "nonoverflow": 260, "note": [302, 341], "npu": 292, "npu_cnn_quant": 67, "npu_transformer_quant": 68, "npy": 264, "object": 318, "observ": [239, 240, 241, 242], "obtain": 302, "ocp": 315, "offlin": 311, "oga": [311, 323], "older": 250, "onli": [288, 301, 308, 315], "onnx": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 179, 249, 252, 258, 264, 267, 268, 269, 270, 277, 278, 279, 282, 283, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 299, 308, 309, 310, 311, 312, 313, 323, 324], "onnx_evalu": 20, "onnx_quant": [39, 69], "onnx_subgraph": 21, "op": 300, "oper": [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], "opt": [272, 275], "opt_pass_after_qu": 215, "opt_pass_before_qu": 217, "opt_pass_manag": 213, "optim": [50, 51, 52, 53, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 311, 327], "option": [286, 290, 340], "origin": 316, "osscar": [134, 135], "other": [300, 310, 312, 313], "outlier": 335, "overal": 340, "overview": [298, 307], "pack": 246, "packag": [41, 59, 123, 130, 132, 134, 160, 188, 195, 229, 232], "paramet": 340, "parti": 308, "path": 259, "per_group": 315, "percentil": 260, "perplex": 312, "pip": [265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 285, 286, 291], "pipelin": 308, "point": [265, 294, 337], "post": [315, 335], "post_quant": [214, 215], "ppl": [309, 312], "practic": [279, 283, 285, 286, 291, 335], "pre": [290, 311], "pre_quant": [216, 217, 218, 219, 220, 221], "precis": [276, 292, 297, 300], "prelu": 47, "prepar": [148, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 291, 314, 315, 316, 323], "prerequisit": 250, "pretrained_config": 174, "print": 292, "print_a16w8_a8w8_nod": 102, "process": [278, 279, 290, 311], "processor": [136, 224, 225, 226, 227], "processor_util": 227, "prompt": 308, "prune": [193, 194, 195, 196, 197, 314], "ptq": [315, 318, 335], "pytorch": [249, 290, 303, 317, 326, 328, 329, 330, 340], "qat": [316, 318, 335], "qdq_op": 48, "qdq_quantiz": [54, 70], "qparamslinear": 177, "quant": [280, 319], "quant_base_op": 13, "quant_config_pars": 172, "quant_conv_op": 14, "quant_gemm_op": 15, "quant_matmul_op": 16, "quant_norm_op": 17, "quant_util": 55, "quantiti": 292, "quantiz": [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 249, 251, 252, 253, 257, 259, 261, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 306, 308, 315, 318, 319, 320, 323, 331, 332, 333, 334, 335, 337, 338, 339, 340], "quantizationconfig": [307, 340], "quantizationspec": 340, "quantize_conv": 234, "quantize_conv_bn_fus": 235, "quantize_emb": 236, "quantize_linear": 237, "quark": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 252, 258, 259, 267, 268, 269, 270, 277, 282, 283, 285, 286, 287, 288, 289, 291, 293, 294, 295, 296, 297, 301, 303, 306, 308, 309, 315, 316, 318, 319, 323, 325, 328, 329, 330, 336, 337], "quarot": [71, 137, 138, 139, 140, 254, 277, 338], "question": [289, 330], "quick": 318, "random": [259, 292], "random_quant": 103, "realquant": 178, "recip": [310, 312, 313, 314, 315, 316], "refer": 311, "refin": 72, "registri": 73, "releas": [288, 329, 341], "reload": 316, "remove_bf16_cast": 104, "remove_dropout_nod": 222, "remove_initializer_from_input": 105, "remove_qdq": 106, "remove_qdq_between_op": 107, "remove_qdq_mul_add": 108, "replac": 292, "replace_bfloat16_qdq_cast": 109, "replace_conv2d_to_qtconv2d": 218, "replace_conv_bn_to_qt_model": 219, "replace_convtranspose2d_to_qtconvtranspose2d": 220, "replace_inf_weight": 110, "replace_linear_to_qtlinear": 221, "requir": [265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 280, 281, 282, 283, 285, 286, 291, 301], "resnet50": 274, "result": [265, 290, 309, 318, 319, 339], "retriev": 311, "rotat": [141, 142, 143, 144, 338, 339], "rotation_util": 144, "roug": 313, "run": [308, 315, 319], "runtim": 323, "ryzen": [283, 285, 286, 288, 291], "ryzenai": [278, 279, 283], "safetensor": [308, 314, 316], "save": [264, 314, 334], "save_tensor_hist": 111, "save_weights_hist": 112, "scale": [126, 301, 302], "scheme": [261, 319, 331, 335], "score": 311, "script": [308, 314, 315, 316, 318], "search": [278, 279, 283, 298], "sensit": 297, "set": [278, 279, 301, 337, 340], "setup": 284, "share": [115, 116, 117, 118, 119, 302], "simul": 290, "simulate_dpu": 74, "simulate_dpu_softmax": 75, "singl": [259, 264], "smooth": [127, 280, 336], "smooth_quant": 76, "smoothquant": [255, 277, 280, 336, 339], "softmax": 49, "some": 319, "someth": 251, "sq": 255, "start": [309, 318], "static": 315, "step": [307, 311, 319, 340], "str": 304, "strategi": [262, 332], "streamlin": 249, "submodul": [30, 35, 52, 59, 60, 100, 113, 133, 152, 160, 184, 195, 201, 229], "summari": 300, "support": [258, 300, 303, 309, 311, 314, 315, 316], "symmetri": [263, 333], "tabl": [300, 309], "task": [311, 318], "tensor": [301, 304, 340], "tensor_convert": 158, "tensor_quant": 243, "test": 308, "testing_util": 119, "thi": [251, 288, 301, 329], "third": 308, "tip": 290, "tool": [77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 292], "torch": [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 286, 304, 310, 312, 313, 336, 340], "torch_util": [22, 228], "torch_utils_test": 23, "tqt": 318, "tqt_observ": 242, "train": [311, 315, 335], "train_model": 25, "train_model_loss": 26, "train_model_param": 27, "train_torch": [24, 25, 26, 27], "transform": 33, "transforms_pipelin": 34, "try": 335, "turn": 301, "tutori": 315, "two": [292, 302], "type": [202, 251, 300, 302], "u16u8": 292, "u8u8": 292, "uint4": 323, "unet": 308, "uniform": 302, "up": [301, 340], "upgrad": 252, "us": [252, 253, 259, 264, 276, 280, 281, 292, 301, 308, 315, 316, 318, 319, 323, 336, 337], "usag": [248, 298, 315], "user": [309, 318], "util": [113, 114, 116, 117, 118, 119, 140, 145, 146, 147, 148, 149, 159, 168, 169, 180, 197, 203, 223, 238, 244, 245, 246, 290], "v": 309, "v1": 274, "vai_op": [43, 44, 45, 46, 47, 48, 49], "valu": [251, 292], "verif": [186, 250], "version": [247, 250, 341], "vision": 318, "vlm": 309, "w_uint4": 315, "we": 251, "weight": [288, 292, 301, 315, 336], "welcom": 249, "well": 301, "what": [251, 295, 296, 297, 319], "when": 251, "without": [267, 268, 270, 273, 276, 277, 280, 308, 314, 315], "work": [319, 336, 337], "yolo_nas_": 278, "your": 319, "zip": 250}})